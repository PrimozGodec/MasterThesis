@article{DBLP:journals/corr/KingmaB14,
author = {Kingma, Diederik P and Ba, Jimmy},
journal = {CoRR},
mendeley-groups = {deep colorization},
title = {{Adam: {\{}A{\}} Method for Stochastic Optimization}},
url = {http://arxiv.org/abs/1412.6980},
volume = {abs/1412.6},
year = {2014}
}
@article{Lotter2016,
abstract = {While great strides have been made in using deep learning algorithms to solve supervised learning tasks, the problem of unsupervised learning - leveraging unlabeled examples to learn about the structure of a domain - remains a difficult unsolved challenge. Here, we explore prediction of future frames in a video sequence as an unsupervised learning rule for learning about the structure of the visual world. We describe a predictive neural network ("PredNet") architecture that is inspired by the concept of "predictive coding" from the neuroscience literature. These networks learn to predict future frames in a video sequence, with each layer in the network making local predictions and only forwarding deviations from those predictions to subsequent network layers. We show that these networks are able to robustly learn to predict the movement of synthetic (rendered) objects, and that in doing so, the networks learn internal representations that are useful for decoding latent object parameters (e.g. pose) that support object recognition with fewer training views. We also show that these networks can scale to complex natural image streams (car-mounted camera videos), capturing key aspects of both egocentric movement and the movement of objects in the visual scene, and the representation learned in this setting is useful for estimating the steering angle. Altogether, these results suggest that prediction represents a powerful framework for unsupervised learning, allowing for implicit learning of object and scene structure.},
archivePrefix = {arXiv},
arxivId = {1605.08104},
author = {Lotter, William and Kreiman, Gabriel and Cox, David},
eprint = {1605.08104},
mendeley-groups = {deep colorization},
month = {may},
title = {{Deep Predictive Coding Networks for Video Prediction and Unsupervised Learning}},
url = {http://arxiv.org/abs/1605.08104},
year = {2016}
}
@book{welstead1999fractal,
author = {Welstead, Stephen T},
mendeley-groups = {deep colorization},
publisher = {Spie Press},
title = {{Fractal and wavelet image compression techniques}},
volume = {40},
year = {1999}
}
@article{Hinton2015,
abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
archivePrefix = {arXiv},
arxivId = {1503.02531},
author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
eprint = {1503.02531},
mendeley-groups = {deep colorization},
month = {mar},
title = {{Distilling the Knowledge in a Neural Network}},
url = {http://arxiv.org/abs/1503.02531},
year = {2015}
}
@article{mackay1992practical,
author = {MacKay, David J C},
journal = {Neural computation},
mendeley-groups = {deep colorization},
number = {3},
pages = {448--472},
publisher = {MIT Press},
title = {{A practical Bayesian framework for backpropagation networks}},
volume = {4},
year = {1992}
}
@inproceedings{Ramaswamy,
abstract = {In this paper, we propose a novel formulation for distance-based outliers that is based on the distance of a point from its kth nearest neighbor. We rank each point on the basis of its distance to its kth nearest neighbor and declare the top n points in this ranking to be outliers. In addition to developing relatively straightforward solutions to finding such outliers based on the classical nested-loop join and index join algorithms, we develop a highly efficient partition-based algorithm for mining outliers. This algorithm first partitions the input data set into disjoint subsets, and then prunes entire partitions as soon as it is determined that they cannot contain outliers. This results in substantial savings in computation. We present the results of an extensive experimental study on real-life and synthetic data sets. The results from a real-life NBA database highlight and reveal several expected and unexpected aspects of the database. The results from a study on synthetic data sets demonstrate that the partition-based algorithm scales well with respect to both data set size and data set dimensionality.},
author = {Ramaswamy, Sridhar and Rastogi, Rajeev and Shim, Kyuseok},
booktitle = {Proceedings of the 2000 ACM SIGMOD international conference on Management of data - SIGMOD '00},
doi = {10.1145/342009.335437},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ramaswamy, Rastogi, Shim KAIST - Unknown - Efficient Algorithms for Mining Outliers from Large Data Sets.pdf:pdf},
isbn = {1581132174},
issn = {01635808},
mendeley-groups = {deep colorization},
pages = {427--438},
title = {{Efficient algorithms for mining outliers from large data sets}},
url = {ftp://ftp10.us.freebsd.org/users/azhang/disc/disc01/cd1/out/papers/sigmod/efficientalgorisrrak.pdf http://portal.acm.org/citation.cfm?doid=342009.335437},
year = {2000}
}
@misc{Karpathy2016a,
author = {Karpathy, Andrej},
mendeley-groups = {deep colorization},
title = {{Neural Networks Part 1: Setting up the Architecture}},
year = {2016}
}
@article{iizuka2016let,
author = {Iizuka, Satoshi and Simo-Serra, Edgar and Ishikawa, Hiroshi},
journal = {ACM Transactions on Graphics (TOG)},
mendeley-groups = {deep colorization},
number = {4},
pages = {110},
publisher = {ACM},
title = {{Let there be color!: joint end-to-end learning of global and local image priors for automatic image colorization with simultaneous classification}},
volume = {35},
year = {2016}
}
@misc{Krizhevsky2012,
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Krizhevsky, Sutskever, Hinton - 2012 - ImageNet Classification with Deep Convolutional Neural Networks.pdf:pdf},
mendeley-groups = {deep colorization},
pages = {1097--1105},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks},
year = {2012}
}
@article{Ioffe2015,
abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9{\%} top-5 validation error (and 4.8{\%} test error), exceeding the accuracy of human raters.},
archivePrefix = {arXiv},
arxivId = {1502.03167},
author = {Ioffe, Sergey and Szegedy, Christian},
eprint = {1502.03167},
mendeley-groups = {deep colorization},
month = {feb},
title = {{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}},
url = {http://arxiv.org/abs/1502.03167},
year = {2015}
}
@book{Saupe2006,
abstract = {Fractal image compression is a new technique for encoding images compactly.It builds on local self-similarities within images. Image blocks are seenas rescaled and intensity transformed approximate copies of blocks found elsewherein the image. This yields a self-referential description of image data,which --- when decoded --- shows a typical fractal structure. This paper providesan elementary introduction to this compression technique. We have chosenthe similarity to a particular variant of vector quantization as the mostdirect approach to fractal image compression. We discuss the hierarchicalquadtree scheme and vital complexity reduction methods. Furthermore, wesurvey some of the advanced concepts such as fast decoding, hybrid methods,and adaptive partitionings. We conclude with a list of relevant WEB resourcesincluding complete public domain C implementations of the method and acomprehensive list of up-to-date references.1 IntroductionAbout ten to fifteen years ago fra...},
author = {Saupe, Dietmar and Hamzaoui, Raouf and Hartenstein, Hannes},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Saupe, Hamzaoui, Hartenstein - Unknown - Fractal Image Compression An Introductory Overview.pdf:pdf},
keywords = {{\&}accelerate {\&}accelerated {\&}access {\&}acoustic {\&}adapti},
mendeley-groups = {deep colorization},
pages = {66},
publisher = {Univ., Inst. f{\{}$\backslash$"u{\}}r Informatik},
title = {{Fractal Image Compression An Introductory Overview}},
url = {https://karczmarczuk.users.greyc.fr/matrs/Dess/RADI/Refs/SaHaHa96a.pdf},
year = {2006}
}
@inproceedings{dalal2005histograms,
author = {Dalal, Navneet and Triggs, Bill},
booktitle = {Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on},
mendeley-groups = {deep colorization},
organization = {IEEE},
pages = {886--893},
title = {{Histograms of oriented gradients for human detection}},
volume = {1},
year = {2005}
}
@article{LeCun2015,
author = {LeCun, Y and Bengio, Y and Hinton, G},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/LeCun, Bengio, Hinton - 2015 - Deep learning.pdf:pdf},
journal = {Nature},
mendeley-groups = {deep colorization},
title = {{Deep learning}},
url = {https://www.nature.com/nature/journal/v521/n7553/abs/nature14539.html},
year = {2015}
}
@article{Chen2016,
abstract = {In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled filters, or 'atrous convolution', as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed "DeepLab" system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7{\%} mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online.},
archivePrefix = {arXiv},
arxivId = {1606.00915},
author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
eprint = {1606.00915},
mendeley-groups = {deep colorization},
month = {jun},
title = {{DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs}},
url = {http://arxiv.org/abs/1606.00915},
year = {2016}
}
@misc{Dahl,
author = {Dahl, Ryan},
mendeley-groups = {deep colorization},
title = {{Automatic Colorization}},
url = {http://tinyclouds.org/colorize/}
}
@article{Larsson2016,
abstract = {We develop a fully automatic image colorization system. Our approach leverages recent advances in deep networks, exploiting both low-level and semantic representations during colorization. As many scene elements naturally appear according to multimodal color distributions, we train our model to predict per-pixel color histograms. This intermediate output can be used to automatically generate a color image, or further manipulated prior to image formation; our experiments consider both scenarios. On both fully and partially automatic colorization tasks, our system significantly outperforms all existing methods.},
archivePrefix = {arXiv},
arxivId = {1603.06668},
author = {Larsson, Gustav and Maire, Michael and Shakhnarovich, Gregory},
eprint = {1603.06668},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Larsson, Maire, Shakhnarovich - 2016 - Learning Representations for Automatic Colorization.pdf:pdf},
journal = {arXiv preprint},
keywords = {1,4 and 5,automatic colorization,convolutional neural networks,deep learning,fig,grayscale input,hy-,more examples in figures,our automatic colorization of,percolumns},
mendeley-groups = {deep colorization},
month = {mar},
title = {{Learning Representations for Automatic Colorization}},
url = {http://arxiv.org/abs/1603.06668},
year = {2016}
}
@book{Schwiegerling2004,
abstract = {"SPIE digital library." Visual optics requires an understanding of both biology and optical engineering. This Field Guide assembles the anatomy, physiology, and functioning of the eye, as well as the engineering and design of a wide assortment of tools for measuring, photographing, and characterizing properties of the surfaces and structures of the eye. Also covered are the diagnostic techniques, lenses, and surgical techniques used to correct and improve human vision. Glossary -- Ocular function -- Eyeball -- Cornea -- Retina -- Photoreceptors -- Retinal landmarks -- Properties of ocular components -- Accommodation -- Pupil size and dark adaptation -- Transmission and reflectance -- Axes of the eye -- Stiles-Crawford effect -- Photopic V[lambda] and scotopic V'[lambda] response -- Eye movements -- Vergence -- Paraxial schematic eye -- Arizona eye model -- Aberrations -- Visual acuity -- Visual acuity and eye charts -- Contrast sensitivity function (CSF) -- Emmetropia and ametropia -- Far and near points -- Presbyopia -- Correction of ocular errors -- Spectacles : single vision -- Spectacle lenses -- Lensmeter -- Spherical and cylindrical refractive error -- Prismatic error -- Astigmatic decomposition -- Special ophthalmic lenses -- Variable prisms and lenses -- Contact lenses -- Radiuscope -- Spectacle and contact lens materials -- Surgical correction of refractive error -- Cataract surgery -- Ophthalmic instrumentation and metrology -- Purkinje images -- Fluorescein imaging -- Indocyanine green imaging -- Keratometry -- Corneal topography -- Corneal topography : axial power -- Corneal topography : instantaneous power -- Anterior segment imaging -- Wavefront sensing : Shack-Hartmann sensing -- Wavefront sensing : Tscherning aberrometry -- Wavefront sensing : retinal raytracing -- Wavefront sensing : spatially resolved refractometry -- Wavefront sensing : reconstruction. Zernike polynomials : wavefront sensing standard -- Zernike polynomials : Cartesian coordinates -- Zernike polynomials : useful formulas -- Ophthalmoscopy -- Retinal imaging -- Field of view and perimetry -- Retinoscopy -- Autorefraction -- Badal optometer and Maxwellian view -- Common ophthalmic lasers -- Eye safety : laser sources -- Eye safety : non-laser sources -- Color -- Photometry -- Colorimetry : RBB and CIE XYZ systems -- Colorimetry : chromaticity diagram -- Colorimetry : primaries and gamut -- Colorimetry : CIELUV color space -- Colorimetry : CIELAB color space -- Chromatic adaptation -- L, M, and S cone fundamentals -- Appendices -- Aspheric and astigmatic surfaces -- Differential geometry -- Trigonometric identities -- CIE Photopic V[lambda] and scotopic V'[lambda] response -- 1931 CIE 2° color matching functions -- 1964 CIE 10° color matching functions -- Stockman {\&} Sharpe 2° cone fundamentals -- Incoherent retinal hazard functions -- Zernike polynomials : table in polar coordinates -- Zernike polynomials : table in Cartesian coordinates -- Equation summary -- Bibliography -- Index.},
author = {Schwiegerling, Jim. and {Society of Photo-optical Instrumentation Engineers.}},
isbn = {9780819456298},
mendeley-groups = {deep colorization},
pages = {71},
publisher = {SPIE},
title = {{Field guide to visual and ophthalmic optics}},
url = {https://spie.org/Publications/Book/592975},
year = {2004}
}
@misc{Everding,
author = {Everding, Holger},
mendeley-groups = {deep colorization},
title = {{CIELAB boundaries – the borders of the CIELAB color space}},
url = {https://www.freiefarbe.de/en/grenzen-des-cielab-farbraums/},
urldate = {2017-06-27}
}
@misc{Gibson,
author = {Gibson, Adam and Nicholson, Cris and Patterson, Josh},
mendeley-groups = {deep colorization},
title = {{Introduction to Deep Neural Networks}},
url = {https://deeplearning4j.org/neuralnet-overview}
}
@article{Zhang2016,
author = {Zhang, R and Isola, P and Efros, AA},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Isola, Efros - 2016 - Colorful image colorization.pdf:pdf},
journal = {European Conference on Computer Vision},
mendeley-groups = {deep colorization},
title = {{Colorful image colorization}},
url = {http://link.springer.com/chapter/10.1007/978-3-319-46487-9{\_}40},
year = {2016}
}
@article{shirley2001color,
author = {Shirley, Peter},
journal = {IEEE Corn},
mendeley-groups = {deep colorization},
pages = {34--41},
title = {{Color transfer between images}},
volume = {21},
year = {2001}
}
@inproceedings{huang2005adaptive,
author = {Huang, Yi-Chin and Tung, Yi-Shin and Chen, Jun-Cheng and Wang, Sung-Wen and Wu, Ja-Ling},
booktitle = {Proceedings of the 13th annual ACM international conference on Multimedia},
mendeley-groups = {deep colorization},
organization = {ACM},
pages = {351--354},
title = {{An adaptive edge detection based colorization algorithm and its applications}},
year = {2005}
}
@article{Bansal,
abstract = {— This paper proposes an approach for the segmentation of color images using CIELab color space and Ant based clustering. Image segmentation is the process of assigning a label to every pixel in an image such that pixels with the same label share certain visual characteristics. The objective of segmentation is to change the image into meaningful form that is easier to analyze. This paper elaborates the ant based clustering for image segmentation. CMC distance is used to calculate the distance between pixels as this color metric gives good results with CIELab color space. Results show the segmentation using ant based clustering and also verifies that number of clusters for the image with the CMC distance also varies. Clustering quality is evaluated using MSE measure.},
author = {Bansal, Seema and Aggarwal, Deepak},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bansal, Aggarwal - Unknown - Color Image Segmentation Using CIELab Color Space Using Ant Colony Optimization.pdf:pdf},
keywords = {CIELab color space,CMC distance,segmentation,— Ant Clust},
mendeley-groups = {deep colorization},
title = {{Color Image Segmentation Using CIELab Color Space Using Ant Colony Optimization}},
url = {http://www.ijcset.net/docs/Volumes/volume1issue7/ijcset2011010715.pdf}
}
@incollection{joyce2011kullback,
author = {Joyce, James M},
booktitle = {International Encyclopedia of Statistical Science},
mendeley-groups = {deep colorization},
pages = {720--722},
publisher = {Springer},
title = {{Kullback-leibler divergence}},
year = {2011}
}
@inproceedings{Iizuka2016,
abstract = {We present a novel technique to automatically colorize grayscale images that combines both global priors and local image features. Based on Convolutional Neural Networks, our deep network fea- tures a fusion layer that allows us to elegantly merge local information dependent on small image patches with global priors computed using the entire image. The entire framework, including the global and local priors as well as the colorization model, is trained in an end-to-end fashion. Furthermore, our architecture can process images of any resolution, unlike most existing approaches based on CNN. We leverage an existing large-scale scene classification data-base to train our model, exploiting the class labels of the dataset to more efficiently and discriminatively learn the global priors. We validate our approach with a user study and compare against the state of the art, where we show significant improvements. Further- more, we demonstrate our method extensively on many different types of images, including black-and-white photography from over a hundred years ago, and show realistic colorizations.},
author = {Iizuka, Satoshi and {Edgar Simo-Serra} and Ishikawa, Hiroshi},
booktitle = {SIGGRAPH '16},
doi = {10.1145/2897824.2925974},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Iizuka, Edgar Simo-Serra, Ishikawa - 2016 - Let there be Color! Joint End-to-end Learning of Global and Local Image Priors for Automatic.pdf:pdf},
isbn = {9781450342797},
issn = {07300301},
keywords = {colorization,computing methodologies,convolutional neural network concepts,image processing,neural net-},
mendeley-groups = {deep colorization},
number = {4},
pages = {110},
publisher = {ACM},
title = {{Let there be Color!: Joint End-to-end Learning of Global and Local Image Priors for Automatic Image Colorization with Simultaneous Classification}},
volume = {35},
year = {2016}
}
@book{Ohta2005,
author = {Ohta, N and Robertson, A R},
doi = {10.1002/0470094745},
isbn = {null},
issn = {1070664X},
mendeley-groups = {deep colorization},
pages = {92--98},
title = {{Colorimetry: Fundamentals and Applications}},
url = {https://books.google.com/books?hl=en{\&}lr={\&}id=U8jeh1uhSHgC{\&}oi=fnd{\&}pg=PR2{\&}dq=Colorimetry:+Fundamentals+and+Applications+{\&}ots=SUbuBOkPfU{\&}sig=FtBn43qgQvrKC0pXi7lAeVB0FGk},
volume = {null},
year = {2005}
}
@article{Pm2013,
abstract = {A color model is an abstract mathematical model describing the way colors can be represented as tuples of numbers, typically as three or four values or color components (e.g. RGB and CMYK are color models). However, a color model with no associated mapping function to an absolute color space is a more or less arbitrary color system with no connection to any globally understood system of color interpretation. This paper mainly discusses about various colour spaces and the how they organized and the colour conversion algorithms like CMYK to RGB, RGB to CMYK, HSL to RGB, RGB to HSL , HSV to RGB , RGB To HSV , YUV to RGB And RGB to YUV.},
author = {Pm, Nishad and Chezian, R Manicka},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pm, Chezian - 2013 - VARIOUS COLOUR SPACES AND COLOUR SPACE CONVERSION ALGORITHMS.pdf:pdf},
journal = {Journal of Global Research in Computer Science},
keywords = {CMYK,Colour spaces,HSL,HSV and YUV,RGB,colour conversion algorithms},
mendeley-groups = {deep colorization},
number = {1},
title = {{VARIOUS COLOUR SPACES AND COLOUR SPACE CONVERSION ALGORITHMS}},
url = {http://jgrcs.info/index.php/jgrcs/article/viewFile/587/430},
volume = {4},
year = {2013}
}
@inproceedings{Mannor2005,
address = {New York, New York, USA},
author = {Mannor, Shie and Peleg, Dori and Rubinstein, Reuven},
booktitle = {Proceedings of the 22nd international conference on Machine learning  - ICML '05},
doi = {10.1145/1102351.1102422},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mannor, Peleg, Rubinstein - 2005 - The cross entropy method for classification.pdf:pdf},
isbn = {1595931805},
mendeley-groups = {deep colorization},
pages = {561--568},
publisher = {ACM Press},
title = {{The cross entropy method for classification}},
url = {http://portal.acm.org/citation.cfm?doid=1102351.1102422},
year = {2005}
}
@inproceedings{levin2004colorization,
author = {Levin, Anat and Lischinski, Dani and Weiss, Yair},
booktitle = {ACM Transactions on Graphics (TOG)},
mendeley-groups = {deep colorization},
number = {3},
organization = {ACM},
pages = {689--694},
title = {{Colorization using optimization}},
volume = {23},
year = {2004}
}
@incollection{Jack2005,
abstract = {The chapter reviews the common color spaces, how they are mathematically related and when a specific color space is used. A color space is a mathematical representation of a set of colors. All of the color spaces can be derived from the red, green, and blue (RGB) information supplied by devices such as cameras and scanners. The three most popular color models are RGB (used in computer graphics); YIQ, YUV, or YCbCr (used in video systems); and CMYK (used in color printing). Considerations for converting from a non-RGB to a RGB color space and gamma correction are also discussed in the chapter. By “gamma correcting” the video signals before transmission, the intensity output of the display becomes roughly linear and the transmission- induced noise is reduced.},
author = {Jack, Keith and Jack, Keith},
booktitle = {Video Demystified},
doi = {10.1016/B978-075067822-3/50004-X},
isbn = {9780750678223},
mendeley-groups = {deep colorization},
pages = {15--34},
title = {{Chapter 3 – Color Spaces}},
year = {2005}
}
@article{Simonyan2014,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
eprint = {1409.1556},
mendeley-groups = {deep colorization},
month = {sep},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
url = {http://arxiv.org/abs/1409.1556},
year = {2014}
}
@article{Cheng2015,
abstract = {This paper investigates into the colorization problem which converts a grayscale image to a colorful version. This is a very difficult problem and normally requires manual adjustment to achieve artifact-free quality. For instance, it normally requires human-labelled color scribbles on the grayscale target image or a careful selection of colorful reference images (e.g., capturing the same scene in the grayscale target image). Unlike the previous methods, this paper aims at a high-quality fully-automatic colorization method. With the assumption of a perfect patch matching technique, the use of an extremely large-scale reference database (that contains sufficient color images) is the most reliable solution to the colorization problem. However, patch matching noise will increase with respect to the size of the reference database in practice. Inspired by the recent success in deep learning techniques which provide amazing modeling of large-scale data, this paper re-formulates the colorization problem so that deep learning techniques can be directly employed. To ensure artifact-free quality, a joint bilateral filtering based post-processing step is proposed. We further develop an adaptive image clustering technique to incorporate the global image information. Numerous experiments demonstrate that our method outperforms the state-of-art algorithms both in terms of quality and speed.},
archivePrefix = {arXiv},
arxivId = {1511.04587},
author = {Cheng, Zezhou},
doi = {10.1109/ICCV.2015.55},
eprint = {1511.04587},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cheng - 2015 - Deep Colorization.pdf:pdf},
isbn = {978-1-4673-8391-2},
issn = {0162-8828},
journal = {CVPR},
mendeley-groups = {deep colorization},
pages = {415--423},
title = {{Deep Colorization}},
volume = {1},
year = {2015}
}
@article{Donahue2016,
abstract = {The ability of the Generative Adversarial Networks (GANs) framework to learn generative models mapping from simple latent distributions to arbitrarily complex data distributions has been demonstrated empirically, with compelling results showing that the latent space of such generators captures semantic variation in the data distribution. Intuitively, models trained to predict these semantic latent representations given data may serve as useful feature representations for auxiliary problems where semantics are relevant. However, in their existing form, GANs have no means of learning the inverse mapping -- projecting data back into the latent space. We propose Bidirectional Generative Adversarial Networks (BiGANs) as a means of learning this inverse mapping, and demonstrate that the resulting learned feature representation is useful for auxiliary supervised discrimination tasks, competitive with contemporary approaches to unsupervised and self-supervised feature learning.},
archivePrefix = {arXiv},
arxivId = {1605.09782},
author = {Donahue, Jeff and Kr{\"{a}}henb{\"{u}}hl, Philipp and Darrell, Trevor},
eprint = {1605.09782},
mendeley-groups = {deep colorization},
month = {may},
title = {{Adversarial Feature Learning}},
url = {http://arxiv.org/abs/1605.09782},
year = {2016}
}
@inproceedings{collobert2008unified,
author = {Collobert, Ronan and Weston, Jason},
booktitle = {Proceedings of the 25th international conference on Machine learning},
mendeley-groups = {deep colorization},
organization = {ACM},
pages = {160--167},
title = {{A unified architecture for natural language processing: Deep neural networks with multitask learning}},
year = {2008}
}
@article{bay2006surf,
author = {Bay, Herbert and Tuytelaars, Tinne and {Van Gool}, Luc},
journal = {Computer vision--ECCV 2006},
mendeley-groups = {deep colorization},
pages = {404--417},
publisher = {Springer},
title = {{Surf: Speeded up robust features}},
year = {2006}
}
@misc{Karpathy2016,
author = {Karpathy, Andrej},
howpublished = {CS231n Convolutional Neural Networks for Visual Recognition},
institution = {Stanford University},
mendeley-groups = {deep colorization},
title = {{Understanding and Visualizing Convolutional Neural Networks}},
year = {2016}
}
@article{Limmer2016,
abstract = {This paper proposes a method for transferring the RGB color spectrum to near-infrared (NIR) images using deep multi-scale convolutional neural networks. A direct and integrated transfer between NIR and RGB pixels is trained. The trained model does not require any user guidance or a reference image database in the recall phase to produce images with a natural appearance. To preserve the rich details of the NIR image, its high frequency features are transferred to the estimated RGB image. The presented approach is trained and evaluated on a real-world dataset containing a large amount of road scene images in summer. The dataset was captured by a multi-CCD NIR/RGB camera, which ensures a perfect pixel to pixel registration.},
archivePrefix = {arXiv},
arxivId = {1604.02245},
author = {Limmer, Matthias and Lensch, Hendrik P. A.},
eprint = {1604.02245},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Limmer, Lensch - 2016 - Infrared Colorization Using Deep Convolutional Neural Networks.pdf:pdf},
mendeley-groups = {deep colorization},
month = {apr},
pages = {6},
title = {{Infrared Colorization Using Deep Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1604.02245},
year = {2016}
}
@article{hauke2011comparison,
author = {Hauke, Jan and Kossowski, Tomasz},
journal = {Quaestiones geographicae},
mendeley-groups = {deep colorization},
number = {2},
pages = {87},
publisher = {De Gruyter Open Sp. z oo},
title = {{Comparison of values of Pearson's and Spearman's correlation coefficients on the same sets of data}},
volume = {30},
year = {2011}
}
@inproceedings{ke2004pca,
author = {Ke, Yan and Sukthankar, Rahul},
booktitle = {Computer Vision and Pattern Recognition, 2004. CVPR 2004. Proceedings of the 2004 IEEE Computer Society Conference on},
mendeley-groups = {deep colorization},
organization = {IEEE},
pages = {II----II},
title = {{PCA-SIFT: A more distinctive representation for local image descriptors}},
volume = {2},
year = {2004}
}
@book{Jack2005,
abstract = {4th ed. Previous edition: Eagle Rock, Va.: LLH Technology, 2001. This international bestseller and essential reference is the "bible" for digital video engineers and programmers worldwide. This is by far the most informative analog and digital video reference available, includes the hottest new trends and cutting-edge developments in the field. Video Demystified, Fourth Edition is a "one stop" reference guide for the various digital video technologies. The fourth edition is completely updated with all new chapters on MPEG-4, H.264, SDTV/HDTV, ATSC/DVB, and Streaming Video (Video over DSL, Ethernet, etc.), as well as discussions of the latest standards throughout. The accompanying CD-ROM is updated to include a unique set of video test files in the newest formats. *This essential reference is the "bible" for digital video engineers and programmers worldwide *Contains all new chapters on MPEG-4, H.264, SDTV/HDTV, ATSC/DVB, and Streaming Video *Completely revised with all the latest and most up-to-date industry standards. Introduction; Introduction to Video; Color Spaces; Video Signals Overview; Analog Video Interfaces; Digital Video Interfaces; Video Processing; NTSC/PAL/SECAM Overview; H.261 and H.263 Video Compression; Consumer DV Video Compression; MPEG-1 Video Compression; MPEG-2 Video Compression; MPEG-4 Video Compression, including H.264; Digital Television (SDTV/HDTV, ATSC/DVB); Streaming Video (Video over DSL, Ethernet, etc.).},
author = {Jack, Keith},
isbn = {9780750678223},
mendeley-groups = {deep colorization},
publisher = {Elsevier},
title = {{Video demystified : a handbook for the digital engineer}},
url = {http://www.sciencedirect.com/science/book/9780750678223},
year = {2005}
}
@article{Connolly1997,
author = {Connolly, C. and Fleiss, T.},
doi = {10.1109/83.597279},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Connolly, Fleiss - 1997 - A study of efficiency and accuracy in the transformation from RGB to CIELAB color space.pdf:pdf},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
mendeley-groups = {deep colorization},
month = {jul},
number = {7},
pages = {1046--1048},
title = {{A study of efficiency and accuracy in the transformation from RGB to CIELAB color space}},
url = {http://ieeexplore.ieee.org/document/597279/},
volume = {6},
year = {1997}
}
@article{Weatherall1992,
author = {Weatherall, Ian L and Coombs, Bernard D},
doi = {10.1111/1523-1747.ep12616156},
issn = {0022202X},
journal = {Journal of Investigative Dermatology},
mendeley-groups = {deep colorization},
month = {oct},
number = {4},
pages = {468--473},
title = {{Skin Color Measurements in Terms of CIELAB Color Space Values}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0022202X92903477},
volume = {99},
year = {1992}
}
@article{Koo2016,
abstract = {We attempt to use DCGANs (deep convolutional genera-tive adversarial nets) to tackle the automatic colorization of black and white photos to combat the tendency for vanilla neural nets to " average out " the results. We construct a small feed-forward convolutional neural network as a base-line colorization system. We train the baseline model on the CIFAR-10 dataset with a per-pixel Euclidean loss func-tion on the chrominance values and achieve sensible but mediocre results. We propose using the adversarial frame-work as proposed by Goodfellow et al. [5] as an alternative to the loss function—we reformulate the baseline model as a generator model that maps grayscale images and random noise input to the color image space, and construct a dis-criminator model that is trained to predict the probability that a given colorization was sampled from data distribu-tion rather than generated by the generator model, condi-tioned on the grayscale image. We analyze the challenges that stand in the way of training adversarial networks, and suggest future steps to test the viability of the model.},
author = {Koo, Stephen},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Koo - 2016 - Automatic Colorization with Deep Convolutional Generative Adversarial Networks.pdf:pdf},
mendeley-groups = {deep colorization},
title = {{Automatic Colorization with Deep Convolutional Generative Adversarial Networks}},
url = {http://cs231n.stanford.edu/reports2016/224{\_}Report.pdf},
year = {2016}
}
@misc{ARCHAMBAULT,
author = {ARCHAMBAULT, MICHAEL},
mendeley-groups = {deep colorization},
title = {{A Brief History of Color Photography, From Dream to Reality}},
url = {https://petapixel.com/2015/10/11/a-brief-history-of-color-photography-from-dream-to-reality/},
urldate = {2017-06-28}
}
@article{Koleini2010,
abstract = {This study represents an innovative automatic method for black and white films colorization using texture features and a multilayer perceptron artificial neural network. In the proposed method, efforts are made to remove human interference in the process of colorization and replace it with an artificial neural network (ANN) which is trained using the features of the reference frame. Later, this network is employed for automatic colorization of the remained black and white frames. The reference frames of the black and white film are manually colored. Using a Gabor filter bank, texture features of all the pixels of the reference frame are extracted and used as the input feature vector of the ANN, while the output will be the color vector of the corresponding pixel. Finally, the next frames' feature vectors are fed respectively to the trained neural network, and color vectors of those frames are the output. Applying AVI videos and using various color spaces, a series of experiments are conducted to evaluate the proposed colorization process. This method needs considerable time to provide a reasonable output, given rapidly changing scenes. Fortunately however, due to the high correlation between consecutive frames in typical video footage, the overall performance is promising regarding both visual appearance and the calculated MSE error. Apart from the application, we also aim to show the importance of the low level features in a mainly high level process, and the mapping ability of a neural network.},
author = {Koleini, Mina and Mobadjemi, S A and Moallem, Payman},
doi = {10.1080/02533839.2010.9671693},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Koleini, Mobadjemi, Moallem - 2010 - Automatic Black and White Film Colorization Using Texture Features and Artificial Neural Networks.pdf:pdf},
isbn = {0253-3839},
issn = {02533839},
journal = {Journal of the Chinese Institute of Engineers},
keywords = {artificial neural networks,colorization,gabor filters,images,texture features},
mendeley-groups = {deep colorization},
number = {7},
pages = {1049--1057},
publisher = {Taylor {\&} Francis Group},
title = {{Automatic Black and White Film Colorization Using Texture Features and Artificial Neural Networks}},
volume = {33},
year = {2010}
}
@misc{Wu2017,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {Wu, Songtao and Zhong, Shenghua and Liu, Yan},
booktitle = {Multimedia Tools and Applications},
doi = {10.1007/s11042-017-4440-4},
eprint = {1512.03385},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/He et al. - Unknown - Deep Residual Learning for Image Recognition.pdf:pdf},
isbn = {978-1-4673-6964-0},
issn = {15737721},
keywords = {Convolutional neural networks,Image steganalysis,Residual learning},
mendeley-groups = {deep colorization},
pages = {1--17},
pmid = {23554596},
title = {{Deep residual learning for image steganalysis}},
url = {https://arxiv.org/pdf/1512.03385.pdf},
year = {2017}
}
@article{ILSVRC15,
author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C and Fei-Fei, Li},
doi = {10.1007/s11263-015-0816-y},
journal = {International Journal of Computer Vision (IJCV)},
mendeley-groups = {deep colorization},
number = {3},
pages = {211--252},
title = {{ImageNet Large Scale Visual Recognition Challenge}},
volume = {115},
year = {2015}
}
@misc{Deshpande2015,
abstract = {We describe an automated method for image colorization that learns to colorize from examples. Our method exploits a LEARCH framework to train a quadratic objective function in the chromaticity maps, comparable to a Gaussian random field. The coefficients of the objective function are conditioned on image features, using a random forest. The objective function admits correlations on long spatial scales, and can control spatial error in the colorization of the image. Images are then colorized by minimizing this objective function. We demonstrate that our method strongly outperforms a natural baseline on large-scale experiments with images of real scenes using a demanding loss function. We demonstrate that learning a model that is conditioned on scene produces improved results. We show how to incorporate a desired color histogram into the objective function, and that doing so can lead to further improvements in results.},
author = {Deshpande, Aditya and Rock, Jason and Forsyth, David},
booktitle = {2015 IEEE International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2015.72},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Deshpande, Rock, Forsyth - 2015 - Learning Large-Scale Automatic Image Colorization.pdf:pdf},
isbn = {VO -},
keywords = {Color,Histograms,Image color analysis,LEARCH framework,Linear programming,Optimization,Regression tree analysis,Standards,chromaticity maps,color histogram,image processing,large-scale automatic image colorization learning,quadratic objective function,random forest,spatial error control},
mendeley-groups = {deep colorization},
pages = {567--575},
title = {{Learning Large-Scale Automatic Image Colorization}},
year = {2015}
}
@article{Kingma2014,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P. and Ba, Jimmy},
eprint = {1412.6980},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kingma, Ba - 2014 - Adam A Method for Stochastic Optimization.pdf:pdf},
mendeley-groups = {deep colorization},
month = {dec},
title = {{Adam: A Method for Stochastic Optimization}},
url = {http://arxiv.org/abs/1412.6980},
year = {2014}
}
@misc{Wickelmaier2003,
abstract = {1. MDS relation with correlations 2. Classical and nonmetric MDS concept and difference.},
author = {Wickelmaier, Florian},
booktitle = {Sound Quality Research Unit, Aalborg University, Denmark},
doi = {10.1080/13546800903126016},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wickelmaier - 2003 - An Introduction to MDS.pdf:pdf},
isbn = {1379-1176 (Print){\$}\backslash{\$}r1379-1176 (Linking)},
issn = {13791176},
mendeley-groups = {deep colorization},
pages = {26},
pmid = {20491413},
title = {{An introduction to MDS}},
url = {https://homepages.uni-tuebingen.de/florian.wickelmaier/pubs/Wickelmaier2003SQRU.pdf http://steep.inrialpes.fr/{\%}7B{~}{\%}7DArnaud/indexation/mds03.pdf},
year = {2003}
}
@article{Dumoulin2016,
abstract = {We introduce a guide to help deep learning practitioners understand and manipulate convolutional neural network architectures. The guide clarifies the relationship between various properties (input shape, kernel shape, zero padding, strides and output shape) of convolutional, pooling and transposed convolutional layers, as well as the relationship between convolutional and transposed convolutional layers. Relationships are derived for various cases, and are illustrated in order to make them intuitive.},
archivePrefix = {arXiv},
arxivId = {1603.07285},
author = {Dumoulin, Vincent and Visin, Francesco},
eprint = {1603.07285},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dumoulin, Visin - 2016 - A guide to convolution arithmetic for deep learning.pdf:pdf},
mendeley-groups = {deep colorization},
month = {mar},
title = {{A guide to convolution arithmetic for deep learning}},
url = {http://arxiv.org/abs/1603.07285},
year = {2016}
}
@article{Prangnell,
abstract = {—There is a widely held belief in the digital image and video processing community, which is as follows: the Human Visual System (HVS) is more sensitive to luminance (often confused with brightness) than photon energies (often confused with chromaticity and chrominance). Passages similar to the following occur with high frequency in the peer reviewed literature and academic text books: " the HVS is much more sensitive to brightness than colour " or " the HVS is much more sensitive to luma than chroma " . In this discussion paper, a Visible Light-Based Human Visual System (VL-HVS) conceptual model is discussed. The objectives of VL-HVS are as follows: 1. To facilitate a deeper theoretical reflection of the fundamental relationship between visible light, the manifestation of colour perception derived from visible light and the physiology of the perception of colour. That is, in terms of the physics of visible light, photobiology and the human subjective interpretation of visible light, it is appropriate to provide comprehensive background information in relation to the natural interactions between visible light, the retinal photoreceptors and the subsequent cortical processing of such. 2. To provide a more wholesome account with respect to colour information in digital image and video processing applications. 3. To recontextualise colour data in the RGB and YCbCr colour spaces, such that novel techniques in digital image and video processing — including quantisation and artifact reduction techniques — may be developed based on both luma and chroma information (not luma data only). 1.0 Physics of Electromagnetic Radiation (Visible Light Range)},
author = {Prangnell, Lee},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Prangnell - Unknown - Visible Light-Based Human Visual System Conceptual Model.pdf:pdf},
mendeley-groups = {deep colorization},
title = {{Visible Light-Based Human Visual System Conceptual Model}},
url = {https://arxiv.org/pdf/1609.04830.pdf}
}
@article{Hwang,
abstract = {We present a convolutional-neural-network-based sys-tem that faithfully colorizes black and white photographic images without direct human assistance. We explore var-ious network architectures, objectives, color spaces, and problem formulations. The final classification-based model we build generates colorized images that are significantly more aesthetically-pleasing than those created by the base-line regression-based model, demonstrating the viability of our methodology and revealing promising avenues for fu-ture work.},
author = {Hwang, Jeff and Zhou, You},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hwang, Zhou - Unknown - Image Colorization with Deep Convolutional Neural Networks.pdf:pdf},
mendeley-groups = {deep colorization},
title = {{Image Colorization with Deep Convolutional Neural Networks}}
}
