@misc{Karpathy2016,
author = {Karpathy, Andrej},
howpublished = {University Lecture},
institution = {Stanford University},
mendeley-groups = {deep colorization},
title = {{CS231n Convolutional Neural Networks for Visual Recognition}},
year = {2016}
}
@misc{Gibson,
author = {Gibson, Adam and Nicholson, Cris and Patterson, Josh},
mendeley-groups = {deep colorization},
title = {{Introduction to Deep Neural Networks}},
url = {https://deeplearning4j.org/neuralnet-overview}
}
@article{ILSVRC15,
author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C and Fei-Fei, Li},
doi = {10.1007/s11263-015-0816-y},
journal = {International Journal of Computer Vision (IJCV)},
mendeley-groups = {deep colorization},
number = {3},
pages = {211--252},
title = {{ImageNet Large Scale Visual Recognition Challenge}},
volume = {115},
year = {2015}
}
@book{Schwiegerling2004,
abstract = {"SPIE digital library." Visual optics requires an understanding of both biology and optical engineering. This Field Guide assembles the anatomy, physiology, and functioning of the eye, as well as the engineering and design of a wide assortment of tools for measuring, photographing, and characterizing properties of the surfaces and structures of the eye. Also covered are the diagnostic techniques, lenses, and surgical techniques used to correct and improve human vision. Glossary -- Ocular function -- Eyeball -- Cornea -- Retina -- Photoreceptors -- Retinal landmarks -- Properties of ocular components -- Accommodation -- Pupil size and dark adaptation -- Transmission and reflectance -- Axes of the eye -- Stiles-Crawford effect -- Photopic V[lambda] and scotopic V'[lambda] response -- Eye movements -- Vergence -- Paraxial schematic eye -- Arizona eye model -- Aberrations -- Visual acuity -- Visual acuity and eye charts -- Contrast sensitivity function (CSF) -- Emmetropia and ametropia -- Far and near points -- Presbyopia -- Correction of ocular errors -- Spectacles : single vision -- Spectacle lenses -- Lensmeter -- Spherical and cylindrical refractive error -- Prismatic error -- Astigmatic decomposition -- Special ophthalmic lenses -- Variable prisms and lenses -- Contact lenses -- Radiuscope -- Spectacle and contact lens materials -- Surgical correction of refractive error -- Cataract surgery -- Ophthalmic instrumentation and metrology -- Purkinje images -- Fluorescein imaging -- Indocyanine green imaging -- Keratometry -- Corneal topography -- Corneal topography : axial power -- Corneal topography : instantaneous power -- Anterior segment imaging -- Wavefront sensing : Shack-Hartmann sensing -- Wavefront sensing : Tscherning aberrometry -- Wavefront sensing : retinal raytracing -- Wavefront sensing : spatially resolved refractometry -- Wavefront sensing : reconstruction. Zernike polynomials : wavefront sensing standard -- Zernike polynomials : Cartesian coordinates -- Zernike polynomials : useful formulas -- Ophthalmoscopy -- Retinal imaging -- Field of view and perimetry -- Retinoscopy -- Autorefraction -- Badal optometer and Maxwellian view -- Common ophthalmic lasers -- Eye safety : laser sources -- Eye safety : non-laser sources -- Color -- Photometry -- Colorimetry : RBB and CIE XYZ systems -- Colorimetry : chromaticity diagram -- Colorimetry : primaries and gamut -- Colorimetry : CIELUV color space -- Colorimetry : CIELAB color space -- Chromatic adaptation -- L, M, and S cone fundamentals -- Appendices -- Aspheric and astigmatic surfaces -- Differential geometry -- Trigonometric identities -- CIE Photopic V[lambda] and scotopic V'[lambda] response -- 1931 CIE 2° color matching functions -- 1964 CIE 10° color matching functions -- Stockman {\&} Sharpe 2° cone fundamentals -- Incoherent retinal hazard functions -- Zernike polynomials : table in polar coordinates -- Zernike polynomials : table in Cartesian coordinates -- Equation summary -- Bibliography -- Index.},
author = {Schwiegerling, Jim. and {Society of Photo-optical Instrumentation Engineers.}},
isbn = {9780819456298},
mendeley-groups = {deep colorization},
pages = {71},
publisher = {SPIE},
title = {{Field guide to visual and ophthalmic optics}},
url = {https://spie.org/Publications/Book/592975},
year = {2004}
}
@book{Ohta2005,
author = {Ohta, N and Robertson, A R},
doi = {10.1002/0470094745},
isbn = {null},
issn = {1070664X},
mendeley-groups = {deep colorization},
pages = {92--98},
title = {{Colorimetry: Fundamentals and Applications}},
url = {https://books.google.com/books?hl=en{\&}lr={\&}id=U8jeh1uhSHgC{\&}oi=fnd{\&}pg=PR2{\&}dq=Colorimetry:+Fundamentals+and+Applications+{\&}ots=SUbuBOkPfU{\&}sig=FtBn43qgQvrKC0pXi7lAeVB0FGk},
volume = {null},
year = {2005}
}
@article{Prangnell,
abstract = {—There is a widely held belief in the digital image and video processing community, which is as follows: the Human Visual System (HVS) is more sensitive to luminance (often confused with brightness) than photon energies (often confused with chromaticity and chrominance). Passages similar to the following occur with high frequency in the peer reviewed literature and academic text books: " the HVS is much more sensitive to brightness than colour " or " the HVS is much more sensitive to luma than chroma " . In this discussion paper, a Visible Light-Based Human Visual System (VL-HVS) conceptual model is discussed. The objectives of VL-HVS are as follows: 1. To facilitate a deeper theoretical reflection of the fundamental relationship between visible light, the manifestation of colour perception derived from visible light and the physiology of the perception of colour. That is, in terms of the physics of visible light, photobiology and the human subjective interpretation of visible light, it is appropriate to provide comprehensive background information in relation to the natural interactions between visible light, the retinal photoreceptors and the subsequent cortical processing of such. 2. To provide a more wholesome account with respect to colour information in digital image and video processing applications. 3. To recontextualise colour data in the RGB and YCbCr colour spaces, such that novel techniques in digital image and video processing — including quantisation and artifact reduction techniques — may be developed based on both luma and chroma information (not luma data only). 1.0 Physics of Electromagnetic Radiation (Visible Light Range)},
author = {Prangnell, Lee},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Prangnell - Unknown - Visible Light-Based Human Visual System Conceptual Model.pdf:pdf},
mendeley-groups = {deep colorization},
title = {{Visible Light-Based Human Visual System Conceptual Model}},
url = {https://arxiv.org/pdf/1609.04830.pdf}
}
@article{Pm2013,
abstract = {A color model is an abstract mathematical model describing the way colors can be represented as tuples of numbers, typically as three or four values or color components (e.g. RGB and CMYK are color models). However, a color model with no associated mapping function to an absolute color space is a more or less arbitrary color system with no connection to any globally understood system of color interpretation. This paper mainly discusses about various colour spaces and the how they organized and the colour conversion algorithms like CMYK to RGB, RGB to CMYK, HSL to RGB, RGB to HSL , HSV to RGB , RGB To HSV , YUV to RGB And RGB to YUV.},
author = {Pm, Nishad and Chezian, R Manicka},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pm, Chezian - 2013 - VARIOUS COLOUR SPACES AND COLOUR SPACE CONVERSION ALGORITHMS.pdf:pdf},
journal = {Journal of Global Research in Computer Science},
keywords = {CMYK,Colour spaces,HSL,HSV and YUV,RGB,colour conversion algorithms},
mendeley-groups = {deep colorization},
number = {1},
title = {{VARIOUS COLOUR SPACES AND COLOUR SPACE CONVERSION ALGORITHMS}},
url = {http://jgrcs.info/index.php/jgrcs/article/viewFile/587/430},
volume = {4},
year = {2013}
}
@article{Bansal,
abstract = {— This paper proposes an approach for the segmentation of color images using CIELab color space and Ant based clustering. Image segmentation is the process of assigning a label to every pixel in an image such that pixels with the same label share certain visual characteristics. The objective of segmentation is to change the image into meaningful form that is easier to analyze. This paper elaborates the ant based clustering for image segmentation. CMC distance is used to calculate the distance between pixels as this color metric gives good results with CIELab color space. Results show the segmentation using ant based clustering and also verifies that number of clusters for the image with the CMC distance also varies. Clustering quality is evaluated using MSE measure.},
author = {Bansal, Seema and Aggarwal, Deepak},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bansal, Aggarwal - Unknown - Color Image Segmentation Using CIELab Color Space Using Ant Colony Optimization.pdf:pdf},
keywords = {CIELab color space,CMC distance,segmentation,— Ant Clust},
mendeley-groups = {deep colorization},
title = {{Color Image Segmentation Using CIELab Color Space Using Ant Colony Optimization}},
url = {http://www.ijcset.net/docs/Volumes/volume1issue7/ijcset2011010715.pdf}
}
@article{Zhang2016,
author = {Zhang, R and Isola, P and Efros, AA},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Isola, Efros - 2016 - Colorful image colorization.pdf:pdf},
journal = {European Conference on Computer Vision},
mendeley-groups = {deep colorization},
title = {{Colorful image colorization}},
url = {http://link.springer.com/chapter/10.1007/978-3-319-46487-9{\_}40},
year = {2016}
}
@article{Hinton2015,
abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
archivePrefix = {arXiv},
arxivId = {1503.02531},
author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
eprint = {1503.02531},
mendeley-groups = {deep colorization},
month = {mar},
title = {{Distilling the Knowledge in a Neural Network}},
url = {http://arxiv.org/abs/1503.02531},
year = {2015}
}
@article{Chen2016,
abstract = {In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled filters, or 'atrous convolution', as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed "DeepLab" system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7{\%} mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online.},
archivePrefix = {arXiv},
arxivId = {1606.00915},
author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
eprint = {1606.00915},
mendeley-groups = {deep colorization},
month = {jun},
title = {{DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs}},
url = {http://arxiv.org/abs/1606.00915},
year = {2016}
}
@article{Donahue2016,
abstract = {The ability of the Generative Adversarial Networks (GANs) framework to learn generative models mapping from simple latent distributions to arbitrarily complex data distributions has been demonstrated empirically, with compelling results showing that the latent space of such generators captures semantic variation in the data distribution. Intuitively, models trained to predict these semantic latent representations given data may serve as useful feature representations for auxiliary problems where semantics are relevant. However, in their existing form, GANs have no means of learning the inverse mapping -- projecting data back into the latent space. We propose Bidirectional Generative Adversarial Networks (BiGANs) as a means of learning this inverse mapping, and demonstrate that the resulting learned feature representation is useful for auxiliary supervised discrimination tasks, competitive with contemporary approaches to unsupervised and self-supervised feature learning.},
archivePrefix = {arXiv},
arxivId = {1605.09782},
author = {Donahue, Jeff and Kr{\"{a}}henb{\"{u}}hl, Philipp and Darrell, Trevor},
eprint = {1605.09782},
mendeley-groups = {deep colorization},
month = {may},
title = {{Adversarial Feature Learning}},
url = {http://arxiv.org/abs/1605.09782},
year = {2016}
}
@article{Lotter2016,
abstract = {While great strides have been made in using deep learning algorithms to solve supervised learning tasks, the problem of unsupervised learning - leveraging unlabeled examples to learn about the structure of a domain - remains a difficult unsolved challenge. Here, we explore prediction of future frames in a video sequence as an unsupervised learning rule for learning about the structure of the visual world. We describe a predictive neural network ("PredNet") architecture that is inspired by the concept of "predictive coding" from the neuroscience literature. These networks learn to predict future frames in a video sequence, with each layer in the network making local predictions and only forwarding deviations from those predictions to subsequent network layers. We show that these networks are able to robustly learn to predict the movement of synthetic (rendered) objects, and that in doing so, the networks learn internal representations that are useful for decoding latent object parameters (e.g. pose) that support object recognition with fewer training views. We also show that these networks can scale to complex natural image streams (car-mounted camera videos), capturing key aspects of both egocentric movement and the movement of objects in the visual scene, and the representation learned in this setting is useful for estimating the steering angle. Altogether, these results suggest that prediction represents a powerful framework for unsupervised learning, allowing for implicit learning of object and scene structure.},
archivePrefix = {arXiv},
arxivId = {1605.08104},
author = {Lotter, William and Kreiman, Gabriel and Cox, David},
eprint = {1605.08104},
mendeley-groups = {deep colorization},
month = {may},
title = {{Deep Predictive Coding Networks for Video Prediction and Unsupervised Learning}},
url = {http://arxiv.org/abs/1605.08104},
year = {2016}
}
@article{Simonyan2014,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
eprint = {1409.1556},
mendeley-groups = {deep colorization},
month = {sep},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
url = {http://arxiv.org/abs/1409.1556},
year = {2014}
}
@article{Ioffe2015,
abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9{\%} top-5 validation error (and 4.8{\%} test error), exceeding the accuracy of human raters.},
archivePrefix = {arXiv},
arxivId = {1502.03167},
author = {Ioffe, Sergey and Szegedy, Christian},
eprint = {1502.03167},
mendeley-groups = {deep colorization},
month = {feb},
title = {{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}},
url = {http://arxiv.org/abs/1502.03167},
year = {2015}
}
@misc{Deshpande2015,
abstract = {We describe an automated method for image colorization that learns to colorize from examples. Our method exploits a LEARCH framework to train a quadratic objective function in the chromaticity maps, comparable to a Gaussian random field. The coefficients of the objective function are conditioned on image features, using a random forest. The objective function admits correlations on long spatial scales, and can control spatial error in the colorization of the image. Images are then colorized by minimizing this objective function. We demonstrate that our method strongly outperforms a natural baseline on large-scale experiments with images of real scenes using a demanding loss function. We demonstrate that learning a model that is conditioned on scene produces improved results. We show how to incorporate a desired color histogram into the objective function, and that doing so can lead to further improvements in results.},
author = {Deshpande, Aditya and Rock, Jason and Forsyth, David},
booktitle = {2015 IEEE International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2015.72},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Deshpande, Rock, Forsyth - 2015 - Learning Large-Scale Automatic Image Colorization.pdf:pdf},
isbn = {VO -},
keywords = {Color,Histograms,Image color analysis,LEARCH framework,Linear programming,Optimization,Regression tree analysis,Standards,chromaticity maps,color histogram,image processing,large-scale automatic image colorization learning,quadratic objective function,random forest,spatial error control},
mendeley-groups = {deep colorization},
pages = {567--575},
title = {{Learning Large-Scale Automatic Image Colorization}},
year = {2015}
}
@article{Larsson2016,
abstract = {We develop a fully automatic image colorization system. Our approach leverages recent advances in deep networks, exploiting both low-level and semantic representations during colorization. As many scene elements naturally appear according to multimodal color distributions, we train our model to predict per-pixel color histograms. This intermediate output can be used to automatically generate a color image, or further manipulated prior to image formation; our experiments consider both scenarios. On both fully and partially automatic colorization tasks, our system significantly outperforms all existing methods.},
archivePrefix = {arXiv},
arxivId = {1603.06668},
author = {Larsson, Gustav and Maire, Michael and Shakhnarovich, Gregory},
eprint = {1603.06668},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Larsson, Maire, Shakhnarovich - 2016 - Learning Representations for Automatic Colorization.pdf:pdf},
journal = {arXiv preprint},
keywords = {1,4 and 5,automatic colorization,convolutional neural networks,deep learning,fig,grayscale input,hy-,more examples in figures,our automatic colorization of,percolumns},
mendeley-groups = {deep colorization},
month = {mar},
title = {{Learning Representations for Automatic Colorization}},
url = {http://arxiv.org/abs/1603.06668},
year = {2016}
}
@article{Cheng2015,
abstract = {This paper investigates into the colorization problem which converts a grayscale image to a colorful version. This is a very difficult problem and normally requires manual adjustment to achieve artifact-free quality. For instance, it normally requires human-labelled color scribbles on the grayscale target image or a careful selection of colorful reference images (e.g., capturing the same scene in the grayscale target image). Unlike the previous methods, this paper aims at a high-quality fully-automatic colorization method. With the assumption of a perfect patch matching technique, the use of an extremely large-scale reference database (that contains sufficient color images) is the most reliable solution to the colorization problem. However, patch matching noise will increase with respect to the size of the reference database in practice. Inspired by the recent success in deep learning techniques which provide amazing modeling of large-scale data, this paper re-formulates the colorization problem so that deep learning techniques can be directly employed. To ensure artifact-free quality, a joint bilateral filtering based post-processing step is proposed. We further develop an adaptive image clustering technique to incorporate the global image information. Numerous experiments demonstrate that our method outperforms the state-of-art algorithms both in terms of quality and speed.},
archivePrefix = {arXiv},
arxivId = {1511.04587},
author = {Cheng, Zezhou},
doi = {10.1109/ICCV.2015.55},
eprint = {1511.04587},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cheng - 2015 - Deep Colorization.pdf:pdf},
isbn = {978-1-4673-8391-2},
issn = {0162-8828},
journal = {CVPR},
mendeley-groups = {deep colorization},
pages = {415--423},
title = {{Deep Colorization}},
volume = {1},
year = {2015}
}
@article{Koleini2010,
abstract = {This study represents an innovative automatic method for black and white films colorization using texture features and a multilayer perceptron artificial neural network. In the proposed method, efforts are made to remove human interference in the process of colorization and replace it with an artificial neural network (ANN) which is trained using the features of the reference frame. Later, this network is employed for automatic colorization of the remained black and white frames. The reference frames of the black and white film are manually colored. Using a Gabor filter bank, texture features of all the pixels of the reference frame are extracted and used as the input feature vector of the ANN, while the output will be the color vector of the corresponding pixel. Finally, the next frames' feature vectors are fed respectively to the trained neural network, and color vectors of those frames are the output. Applying AVI videos and using various color spaces, a series of experiments are conducted to evaluate the proposed colorization process. This method needs considerable time to provide a reasonable output, given rapidly changing scenes. Fortunately however, due to the high correlation between consecutive frames in typical video footage, the overall performance is promising regarding both visual appearance and the calculated MSE error. Apart from the application, we also aim to show the importance of the low level features in a mainly high level process, and the mapping ability of a neural network.},
author = {Koleini, Mina and Mobadjemi, S A and Moallem, Payman},
doi = {10.1080/02533839.2010.9671693},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Koleini, Mobadjemi, Moallem - 2010 - Automatic Black and White Film Colorization Using Texture Features and Artificial Neural Networks.pdf:pdf},
isbn = {0253-3839},
issn = {02533839},
journal = {Journal of the Chinese Institute of Engineers},
keywords = {artificial neural networks,colorization,gabor filters,images,texture features},
mendeley-groups = {deep colorization},
number = {7},
pages = {1049--1057},
publisher = {Taylor {\&} Francis Group},
title = {{Automatic Black and White Film Colorization Using Texture Features and Artificial Neural Networks}},
volume = {33},
year = {2010}
}
@misc{Dahl,
author = {Dahl, Ryan},
mendeley-groups = {deep colorization},
title = {{Automatic Colorization}},
url = {http://tinyclouds.org/colorize/}
}
@article{Limmer2016,
abstract = {This paper proposes a method for transferring the RGB color spectrum to near-infrared (NIR) images using deep multi-scale convolutional neural networks. A direct and integrated transfer between NIR and RGB pixels is trained. The trained model does not require any user guidance or a reference image database in the recall phase to produce images with a natural appearance. To preserve the rich details of the NIR image, its high frequency features are transferred to the estimated RGB image. The presented approach is trained and evaluated on a real-world dataset containing a large amount of road scene images in summer. The dataset was captured by a multi-CCD NIR/RGB camera, which ensures a perfect pixel to pixel registration.},
archivePrefix = {arXiv},
arxivId = {1604.02245},
author = {Limmer, Matthias and Lensch, Hendrik P. A.},
eprint = {1604.02245},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Limmer, Lensch - 2016 - Infrared Colorization Using Deep Convolutional Neural Networks.pdf:pdf},
mendeley-groups = {deep colorization},
month = {apr},
pages = {6},
title = {{Infrared Colorization Using Deep Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1604.02245},
year = {2016}
}
@article{Hwang,
abstract = {We present a convolutional-neural-network-based sys-tem that faithfully colorizes black and white photographic images without direct human assistance. We explore var-ious network architectures, objectives, color spaces, and problem formulations. The final classification-based model we build generates colorized images that are significantly more aesthetically-pleasing than those created by the base-line regression-based model, demonstrating the viability of our methodology and revealing promising avenues for fu-ture work.},
author = {Hwang, Jeff and Zhou, You},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hwang, Zhou - Unknown - Image Colorization with Deep Convolutional Neural Networks.pdf:pdf},
mendeley-groups = {deep colorization},
title = {{Image Colorization with Deep Convolutional Neural Networks}}
}
@article{Koo2016,
abstract = {We attempt to use DCGANs (deep convolutional genera-tive adversarial nets) to tackle the automatic colorization of black and white photos to combat the tendency for vanilla neural nets to " average out " the results. We construct a small feed-forward convolutional neural network as a base-line colorization system. We train the baseline model on the CIFAR-10 dataset with a per-pixel Euclidean loss func-tion on the chrominance values and achieve sensible but mediocre results. We propose using the adversarial frame-work as proposed by Goodfellow et al. [5] as an alternative to the loss function—we reformulate the baseline model as a generator model that maps grayscale images and random noise input to the color image space, and construct a dis-criminator model that is trained to predict the probability that a given colorization was sampled from data distribu-tion rather than generated by the generator model, condi-tioned on the grayscale image. We analyze the challenges that stand in the way of training adversarial networks, and suggest future steps to test the viability of the model.},
author = {Koo, Stephen},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Koo - 2016 - Automatic Colorization with Deep Convolutional Generative Adversarial Networks.pdf:pdf},
mendeley-groups = {deep colorization},
title = {{Automatic Colorization with Deep Convolutional Generative Adversarial Networks}},
url = {http://cs231n.stanford.edu/reports2016/224{\_}Report.pdf},
year = {2016}
}
@inproceedings{Iizuka2016,
abstract = {We present a novel technique to automatically colorize grayscale images that combines both global priors and local image features. Based on Convolutional Neural Networks, our deep network fea- tures a fusion layer that allows us to elegantly merge local information dependent on small image patches with global priors computed using the entire image. The entire framework, including the global and local priors as well as the colorization model, is trained in an end-to-end fashion. Furthermore, our architecture can process images of any resolution, unlike most existing approaches based on CNN. We leverage an existing large-scale scene classification data-base to train our model, exploiting the class labels of the dataset to more efficiently and discriminatively learn the global priors. We validate our approach with a user study and compare against the state of the art, where we show significant improvements. Further- more, we demonstrate our method extensively on many different types of images, including black-and-white photography from over a hundred years ago, and show realistic colorizations.},
author = {Iizuka, Satoshi and {Edgar Simo-Serra} and Ishikawa, Hiroshi},
booktitle = {SIGGRAPH '16},
doi = {10.1145/2897824.2925974},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Iizuka, Edgar Simo-Serra, Ishikawa - 2016 - Let there be Color! Joint End-to-end Learning of Global and Local Image Priors for Automatic.pdf:pdf},
isbn = {9781450342797},
issn = {07300301},
keywords = {colorization,computing methodologies,convolutional neural network concepts,image processing,neural net-},
mendeley-groups = {deep colorization},
number = {4},
pages = {110},
publisher = {ACM},
title = {{Let there be Color!: Joint End-to-end Learning of Global and Local Image Priors for Automatic Image Colorization with Simultaneous Classification}},
volume = {35},
year = {2016}
}
