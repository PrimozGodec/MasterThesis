@inproceedings{Cheng2015,
abstract = {This paper investigates into the colorization problem which converts a grayscale image to a colorful version. This is a very difficult problem and normally requires manual adjustment to achieve artifact-free quality. For instance, it normally requires human-labelled color scribbles on the grayscale target image or a careful selection of colorful reference images (e.g., capturing the same scene in the grayscale target image). Unlike the previous methods, this paper aims at a high-quality fully-automatic colorization method. With the assumption of a perfect patch matching technique, the use of an extremely large-scale reference database (that contains sufficient color images) is the most reliable solution to the colorization problem. However, patch matching noise will increase with respect to the size of the reference database in practice. Inspired by the recent success in deep learning techniques which provide amazing modeling of large-scale data, this paper re-formulates the colorization problem so that deep learning techniques can be directly employed. To ensure artifact-free quality, a joint bilateral filtering based post-processing step is proposed. We further develop an adaptive image clustering technique to incorporate the global image information. Numerous experiments demonstrate that our method outperforms the state-of-art algorithms both in terms of quality and speed.},
author = {Cheng, Zezhou},
booktitle = {CVPR},
isbn = {978-1-4673-8391-2},
issn = {0162-8828},
pages = {415--423},
title = {{Deep Colorization}},
volume = {1},
year = {2015}
}
@article{Koleini2010,
abstract = {This study represents an innovative automatic method for black and white films colorization using texture features and a multilayer perceptron artificial neural network. In the proposed method, efforts are made to remove human interference in the process of colorization and replace it with an artificial neural network (ANN) which is trained using the features of the reference frame. Later, this network is employed for automatic colorization of the remained black and white frames. The reference frames of the black and white film are manually colored. Using a Gabor filter bank, texture features of all the pixels of the reference frame are extracted and used as the input feature vector of the ANN, while the output will be the color vector of the corresponding pixel. Finally, the next frames' feature vectors are fed respectively to the trained neural network, and color vectors of those frames are the output. Applying AVI videos and using various color spaces, a series of experiments are conducted to evaluate the proposed colorization process. This method needs considerable time to provide a reasonable output, given rapidly changing scenes. Fortunately however, due to the high correlation between consecutive frames in typical video footage, the overall performance is promising regarding both visual appearance and the calculated MSE error. Apart from the application, we also aim to show the importance of the low level features in a mainly high level process, and the mapping ability of a neural network.},
author = {Koleini, Mina and Mobadjemi, S A and Moallem, Payman},
isbn = {0253-3839},
issn = {02533839},
journal = {Journal of the Chinese Institute of Engineers},
keywords = {artificial neural networks,colorization,gabor filters,images,texture features},
number = {7},
pages = {1049--1057},
publisher = {Taylor {\{}{\&}{\}} Francis Group},
title = {{Automatic Black and White Film Colorization Using Texture Features and Artificial Neural Networks}},
volume = {33},
year = {2010}
}
@article{Koo2016,
abstract = {We attempt to use DCGANs (deep convolutional genera-tive adversarial nets) to tackle the automatic colorization of black and white photos to combat the tendency for vanilla neural nets to " average out " the results. We construct a small feed-forward convolutional neural network as a base-line colorization system. We train the baseline model on the CIFAR-10 dataset with a per-pixel Euclidean loss func-tion on the chrominance values and achieve sensible but mediocre results. We propose using the adversarial frame-work as proposed by Goodfellow et al. [5] as an alternative to the loss function—we reformulate the baseline model as a generator model that maps grayscale images and random noise input to the color image space, and construct a dis-criminator model that is trained to predict the probability that a given colorization was sampled from data distribu-tion rather than generated by the generator model, condi-tioned on the grayscale image. We analyze the challenges that stand in the way of training adversarial networks, and suggest future steps to test the viability of the model.},
author = {Koo, Stephen},
title = {{Automatic Colorization with Deep Convolutional Generative Adversarial Networks}},
url = {http://cs231n.stanford.edu/reports2016/224{\%}7B{\_}{\%}7DReport.pdf},
year = {2016}
}
@inproceedings{levin2004colorization,
author = {Levin, Anat and Lischinski, Dani and Weiss, Yair},
booktitle = {ACM Transactions on Graphics (TOG)},
number = {3},
organization = {ACM},
pages = {689--694},
title = {{Colorization using optimization}},
volume = {23},
year = {2004}
}
@inproceedings{huang2005adaptive,
author = {Huang, Yi-Chin and Tung, Yi-Shin and Chen, Jun-Cheng and Wang, Sung-Wen and Wu, Ja-Ling},
booktitle = {Proceedings of the 13th annual ACM international conference on Multimedia},
organization = {ACM},
pages = {351--354},
title = {{An adaptive edge detection based colorization algorithm and its applications}},
year = {2005}
}
@article{shirley2001color,
author = {Shirley, Peter},
journal = {IEEE Corn},
pages = {34--41},
title = {{Color transfer between images}},
volume = {21},
year = {2001}
}
@inproceedings{tai2005local,
author = {Tai, Yu-Wing and Jia, Jiaya and Tang, Chi-Keung},
booktitle = {2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)},
organization = {IEEE},
pages = {747--754},
title = {{Local color transfer via probabilistic segmentation by expectation-maximization}},
volume = {1},
year = {2005}
}
@article{zhang2016colorful,
author = {Zhang, Richard and Isola, Phillip and Efros, Alexei A},
journal = {ECCV, 2016},
title = {{Colorful Image Colorization}},
year = {2016}
}
@article{larsson2016learning,
author = {Larsson, Gustav and Maire, Michael and Shakhnarovich, Gregory},
journal = {arXiv preprint arXiv:1603.06668},
title = {{Learning Representations for Automatic Colorization}},
year = {2016}
}
@article{limmer2016infrared,
author = {Limmer, Matthias and Lensch, Hendrik},
journal = {arXiv preprint arXiv:1604.02245},
title = {{Infrared Colorization Using Deep Convolutional Neural Networks}},
year = {2016}
}
@article{ILSVRC15,
author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C and Fei-Fei, Li},
doi = {10.1007/s11263-015-0816-y},
journal = {International Journal of Computer Vision (IJCV)},
mendeley-groups = {deep colorization},
number = {3},
pages = {211--252},
title = {{ImageNet Large Scale Visual Recognition Challenge}},
volume = {115},
year = {2015}
}
@misc{Gibson,
author = {Gibson, Adam and Nicholson, Cris and Patterson, Josh},
mendeley-groups = {deep colorization},
title = {{Introduction to Deep Neural Networks}},
url = {https://deeplearning4j.org/neuralnet-overview}
}
@misc{Karpathy2016,
author = {Karpathy, Andrej},
howpublished = {University Lecture},
institution = {Stanford University},
mendeley-groups = {deep colorization},
title = {{CS231n Convolutional Neural Networks for Visual Recognition}},
year = {2016}
}
@book{Schwiegerling2004,
abstract = {"SPIE digital library." Visual optics requires an understanding of both biology and optical engineering. This Field Guide assembles the anatomy, physiology, and functioning of the eye, as well as the engineering and design of a wide assortment of tools for measuring, photographing, and characterizing properties of the surfaces and structures of the eye. Also covered are the diagnostic techniques, lenses, and surgical techniques used to correct and improve human vision. Glossary -- Ocular function -- Eyeball -- Cornea -- Retina -- Photoreceptors -- Retinal landmarks -- Properties of ocular components -- Accommodation -- Pupil size and dark adaptation -- Transmission and reflectance -- Axes of the eye -- Stiles-Crawford effect -- Photopic V[lambda] and scotopic V'[lambda] response -- Eye movements -- Vergence -- Paraxial schematic eye -- Arizona eye model -- Aberrations -- Visual acuity -- Visual acuity and eye charts -- Contrast sensitivity function (CSF) -- Emmetropia and ametropia -- Far and near points -- Presbyopia -- Correction of ocular errors -- Spectacles : single vision -- Spectacle lenses -- Lensmeter -- Spherical and cylindrical refractive error -- Prismatic error -- Astigmatic decomposition -- Special ophthalmic lenses -- Variable prisms and lenses -- Contact lenses -- Radiuscope -- Spectacle and contact lens materials -- Surgical correction of refractive error -- Cataract surgery -- Ophthalmic instrumentation and metrology -- Purkinje images -- Fluorescein imaging -- Indocyanine green imaging -- Keratometry -- Corneal topography -- Corneal topography : axial power -- Corneal topography : instantaneous power -- Anterior segment imaging -- Wavefront sensing : Shack-Hartmann sensing -- Wavefront sensing : Tscherning aberrometry -- Wavefront sensing : retinal raytracing -- Wavefront sensing : spatially resolved refractometry -- Wavefront sensing : reconstruction. Zernike polynomials : wavefront sensing standard -- Zernike polynomials : Cartesian coordinates -- Zernike polynomials : useful formulas -- Ophthalmoscopy -- Retinal imaging -- Field of view and perimetry -- Retinoscopy -- Autorefraction -- Badal optometer and Maxwellian view -- Common ophthalmic lasers -- Eye safety : laser sources -- Eye safety : non-laser sources -- Color -- Photometry -- Colorimetry : RBB and CIE XYZ systems -- Colorimetry : chromaticity diagram -- Colorimetry : primaries and gamut -- Colorimetry : CIELUV color space -- Colorimetry : CIELAB color space -- Chromatic adaptation -- L, M, and S cone fundamentals -- Appendices -- Aspheric and astigmatic surfaces -- Differential geometry -- Trigonometric identities -- CIE Photopic V[lambda] and scotopic V'[lambda] response -- 1931 CIE 2° color matching functions -- 1964 CIE 10° color matching functions -- Stockman {\&} Sharpe 2° cone fundamentals -- Incoherent retinal hazard functions -- Zernike polynomials : table in polar coordinates -- Zernike polynomials : table in Cartesian coordinates -- Equation summary -- Bibliography -- Index.},
author = {Schwiegerling, Jim. and {Society of Photo-optical Instrumentation Engineers.}},
isbn = {9780819456298},
mendeley-groups = {deep colorization},
pages = {71},
publisher = {SPIE},
title = {{Field guide to visual and ophthalmic optics}},
url = {https://spie.org/Publications/Book/592975},
year = {2004}
}
@book{Ohta2005,
author = {Ohta, N and Robertson, A R},
doi = {10.1002/0470094745},
isbn = {null},
issn = {1070664X},
mendeley-groups = {deep colorization},
pages = {92--98},
title = {{Colorimetry: Fundamentals and Applications}},
url = {https://books.google.com/books?hl=en{\&}lr={\&}id=U8jeh1uhSHgC{\&}oi=fnd{\&}pg=PR2{\&}dq=Colorimetry:+Fundamentals+and+Applications+{\&}ots=SUbuBOkPfU{\&}sig=FtBn43qgQvrKC0pXi7lAeVB0FGk},
volume = {null},
year = {2005}
}
@article{Prangnell,
abstract = {—There is a widely held belief in the digital image and video processing community, which is as follows: the Human Visual System (HVS) is more sensitive to luminance (often confused with brightness) than photon energies (often confused with chromaticity and chrominance). Passages similar to the following occur with high frequency in the peer reviewed literature and academic text books: " the HVS is much more sensitive to brightness than colour " or " the HVS is much more sensitive to luma than chroma " . In this discussion paper, a Visible Light-Based Human Visual System (VL-HVS) conceptual model is discussed. The objectives of VL-HVS are as follows: 1. To facilitate a deeper theoretical reflection of the fundamental relationship between visible light, the manifestation of colour perception derived from visible light and the physiology of the perception of colour. That is, in terms of the physics of visible light, photobiology and the human subjective interpretation of visible light, it is appropriate to provide comprehensive background information in relation to the natural interactions between visible light, the retinal photoreceptors and the subsequent cortical processing of such. 2. To provide a more wholesome account with respect to colour information in digital image and video processing applications. 3. To recontextualise colour data in the RGB and YCbCr colour spaces, such that novel techniques in digital image and video processing — including quantisation and artifact reduction techniques — may be developed based on both luma and chroma information (not luma data only). 1.0 Physics of Electromagnetic Radiation (Visible Light Range)},
author = {Prangnell, Lee},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Prangnell - Unknown - Visible Light-Based Human Visual System Conceptual Model.pdf:pdf},
mendeley-groups = {deep colorization},
title = {{Visible Light-Based Human Visual System Conceptual Model}},
url = {https://arxiv.org/pdf/1609.04830.pdf}
}
@article{Pm2013,
abstract = {A color model is an abstract mathematical model describing the way colors can be represented as tuples of numbers, typically as three or four values or color components (e.g. RGB and CMYK are color models). However, a color model with no associated mapping function to an absolute color space is a more or less arbitrary color system with no connection to any globally understood system of color interpretation. This paper mainly discusses about various colour spaces and the how they organized and the colour conversion algorithms like CMYK to RGB, RGB to CMYK, HSL to RGB, RGB to HSL , HSV to RGB , RGB To HSV , YUV to RGB And RGB to YUV.},
author = {Pm, Nishad and Chezian, R Manicka},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pm, Chezian - 2013 - VARIOUS COLOUR SPACES AND COLOUR SPACE CONVERSION ALGORITHMS.pdf:pdf},
journal = {Journal of Global Research in Computer Science},
keywords = {CMYK,Colour spaces,HSL,HSV and YUV,RGB,colour conversion algorithms},
mendeley-groups = {deep colorization},
number = {1},
title = {{VARIOUS COLOUR SPACES AND COLOUR SPACE CONVERSION ALGORITHMS}},
url = {http://jgrcs.info/index.php/jgrcs/article/viewFile/587/430},
volume = {4},
year = {2013}
}
@article{Bansal,
abstract = {— This paper proposes an approach for the segmentation of color images using CIELab color space and Ant based clustering. Image segmentation is the process of assigning a label to every pixel in an image such that pixels with the same label share certain visual characteristics. The objective of segmentation is to change the image into meaningful form that is easier to analyze. This paper elaborates the ant based clustering for image segmentation. CMC distance is used to calculate the distance between pixels as this color metric gives good results with CIELab color space. Results show the segmentation using ant based clustering and also verifies that number of clusters for the image with the CMC distance also varies. Clustering quality is evaluated using MSE measure.},
author = {Bansal, Seema and Aggarwal, Deepak},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bansal, Aggarwal - Unknown - Color Image Segmentation Using CIELab Color Space Using Ant Colony Optimization.pdf:pdf},
keywords = {CIELab color space,CMC distance,segmentation,— Ant Clust},
mendeley-groups = {deep colorization},
title = {{Color Image Segmentation Using CIELab Color Space Using Ant Colony Optimization}},
url = {http://www.ijcset.net/docs/Volumes/volume1issue7/ijcset2011010715.pdf}
}
@article{Zhang2016,
author = {Zhang, R and Isola, P and Efros, AA},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Isola, Efros - 2016 - Colorful image colorization.pdf:pdf},
journal = {European Conference on Computer Vision},
mendeley-groups = {deep colorization},
title = {{Colorful image colorization}},
url = {http://link.springer.com/chapter/10.1007/978-3-319-46487-9{\_}40},
year = {2016}
}
@article{Hinton2015,
abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
archivePrefix = {arXiv},
arxivId = {1503.02531},
author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
eprint = {1503.02531},
mendeley-groups = {deep colorization},
month = {mar},
title = {{Distilling the Knowledge in a Neural Network}},
url = {http://arxiv.org/abs/1503.02531},
year = {2015}
}
@article{Chen2016,
abstract = {In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled filters, or 'atrous convolution', as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed "DeepLab" system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7{\%} mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online.},
archivePrefix = {arXiv},
arxivId = {1606.00915},
author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
eprint = {1606.00915},
mendeley-groups = {deep colorization},
month = {jun},
title = {{DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs}},
url = {http://arxiv.org/abs/1606.00915},
year = {2016}
}
@article{Donahue2016,
abstract = {The ability of the Generative Adversarial Networks (GANs) framework to learn generative models mapping from simple latent distributions to arbitrarily complex data distributions has been demonstrated empirically, with compelling results showing that the latent space of such generators captures semantic variation in the data distribution. Intuitively, models trained to predict these semantic latent representations given data may serve as useful feature representations for auxiliary problems where semantics are relevant. However, in their existing form, GANs have no means of learning the inverse mapping -- projecting data back into the latent space. We propose Bidirectional Generative Adversarial Networks (BiGANs) as a means of learning this inverse mapping, and demonstrate that the resulting learned feature representation is useful for auxiliary supervised discrimination tasks, competitive with contemporary approaches to unsupervised and self-supervised feature learning.},
archivePrefix = {arXiv},
arxivId = {1605.09782},
author = {Donahue, Jeff and Kr{\"{a}}henb{\"{u}}hl, Philipp and Darrell, Trevor},
eprint = {1605.09782},
mendeley-groups = {deep colorization},
month = {may},
title = {{Adversarial Feature Learning}},
url = {http://arxiv.org/abs/1605.09782},
year = {2016}
}
@article{Lotter2016,
abstract = {While great strides have been made in using deep learning algorithms to solve supervised learning tasks, the problem of unsupervised learning - leveraging unlabeled examples to learn about the structure of a domain - remains a difficult unsolved challenge. Here, we explore prediction of future frames in a video sequence as an unsupervised learning rule for learning about the structure of the visual world. We describe a predictive neural network ("PredNet") architecture that is inspired by the concept of "predictive coding" from the neuroscience literature. These networks learn to predict future frames in a video sequence, with each layer in the network making local predictions and only forwarding deviations from those predictions to subsequent network layers. We show that these networks are able to robustly learn to predict the movement of synthetic (rendered) objects, and that in doing so, the networks learn internal representations that are useful for decoding latent object parameters (e.g. pose) that support object recognition with fewer training views. We also show that these networks can scale to complex natural image streams (car-mounted camera videos), capturing key aspects of both egocentric movement and the movement of objects in the visual scene, and the representation learned in this setting is useful for estimating the steering angle. Altogether, these results suggest that prediction represents a powerful framework for unsupervised learning, allowing for implicit learning of object and scene structure.},
archivePrefix = {arXiv},
arxivId = {1605.08104},
author = {Lotter, William and Kreiman, Gabriel and Cox, David},
eprint = {1605.08104},
mendeley-groups = {deep colorization},
month = {may},
title = {{Deep Predictive Coding Networks for Video Prediction and Unsupervised Learning}},
url = {http://arxiv.org/abs/1605.08104},
year = {2016}
}
@article{Simonyan2014,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
eprint = {1409.1556},
mendeley-groups = {deep colorization},
month = {sep},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
url = {http://arxiv.org/abs/1409.1556},
year = {2014}
}
@article{Ioffe2015,
abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9{\%} top-5 validation error (and 4.8{\%} test error), exceeding the accuracy of human raters.},
archivePrefix = {arXiv},
arxivId = {1502.03167},
author = {Ioffe, Sergey and Szegedy, Christian},
eprint = {1502.03167},
mendeley-groups = {deep colorization},
month = {feb},
title = {{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}},
url = {http://arxiv.org/abs/1502.03167},
year = {2015}
}
@inproceedings{Pesek2014,
abstract = {{\textcopyright} 2014 IEEE.This study presents an evaluation of two interfaces for gathering user feedback in online surveys. We evaluated the intuitiveness, usability and time complexity of the proposed interfaces in comparison to the more standard approaches. Over 900 users first participated in an online survey exploring the influence of mood on their emotional responses to music and colors. We included several new interfaces in this survey, so after it was completed, users were asked to complete a second survey where they evaluated various aspects of the interfaces. Our analysis shows reduced time complexity and increased intuitiveness of the new interfaces, compared to standard approaches, resulting in lower mental difficulty and frustration of participants.},
author = {Pesek, M. and Godec, P. and Poredo{\v{s}}, M. and Strle, G. and Guna, J. and Stojmenova, E. and Poga{\v{c}}nik, M. and Marolt, M.},
booktitle = {2014 IEEE International Conference on Multimedia and Expo Workshops, ICMEW 2014},
doi = {10.1109/ICMEW.2014.6890678},
isbn = {9781479947171},
keywords = {color perception,mood estimation,music information retrieval,questionnaire evaluation},
title = {{Capturing the mood: Evaluation of the moodstripe and moodgraph interfaces}},
year = {2014}
}
@inproceedings{Pesek2014a,
abstract = {We present an innovative dynamic visual interface, the Mood-Stripe, which provides a continuous-scale, multi-parameter drag-and-drop alternative to the standard n-degree (Likert) scale widgets, commonly used in online evaluation processes. We elaborate on the motivation for the development of the new user input interfaces, and present the results of cross evaluation of the GMail product by using the SUS questionnaire with the standard and the proposed MoodStripe interfaces. The overall goal is to design a more intuitive interface, by reducing the noise and task load inherent in traditional interfaces for standardized user-feedback gathering tests. The results show the MoodStripe interface outperforms the standard scale approach both in terms of intuitiveness and functionality. Additionally, the cross-evaluation of the both approaches shows comparable SUS scores.},
author = {Pesek, M. and Godec, P. and Poredo{\v{s}}, M. and Strle, G. and Guna, J. and Stojmenova, E. and Poga{\v{c}}nik, M. and Marolt, M.},
booktitle = {International series on information systems and management in creative eMedia},
isbn = {9781510800168},
issn = {23415576},
keywords = {Gathering feedback efficiency,Human computer interaction,System usability score,User evaluation study,User interface},
title = {{The moodstripe - An evaluation of a novel visual interface as an alternative for online response gathering}},
year = {2014}
}
@inproceedings{Pesek2014b,
abstract = {The paper presents a new dataset that captures the effect of mood on visual and auditory perception of music. With an online survey, we have collected a dataset of over 6600 responses capturing users' mood, emotions evoked and expressed by music and the perception of color with regard to emotions and music. We describe the methodology of gathering the responses and present two new models for capturing users' emotional states: the MoodGraph and MoodStripe. Also, general research questions and goals, as well as possible future applications of the collected dataset, are being discussed.},
author = {Pesek, M. and Godec, P. and Poredo{\v{s}}, M. and Strle, G. and Guna, J. and Stojmenova, E. and Poga{\v{c}}nik, M. and Marolt, M.},
booktitle = {CEUR Workshop Proceedings},
issn = {16130073},
keywords = {Color perception,Human computer interaction,Mood estimation,Music information retrieval},
title = {{Gathering a dataset of multi-modal mood-dependent perceptual responses to music}},
volume = {1181},
year = {2014}
}
@article{Elesini2015,
abstract = {{\textcopyright} 2015, University of Ljubljana. All rights reserved.Competing in the market means constant development throughout all areas, also in information about business processes, the development of which has significantly increased over last sixty years. Results of a research dealt with how the Slovenian textile, clothing and leather production (TOUP) industries have followed this development are presented in the article. The research was further directed towards a new age state. Based on the data collected from the literature, eight hypotheses were set up, which were examined through interviews and questionnaires. 111 (25.5 percent) of companies responded to the study. The results were analysed separately for large, medium, small-sized and micro companies, as the preliminary research showed that their views (and actual states) regarding business information systems are quite different, so any generalisation of the results wouldn't provide realistic treatment of the set hypotheses. Among the gathered data appropriate correlation was searched for using the Pearson $\chi$2-test. All large and medium-sized TOUP companies are equipped with information systems and 80 percent of small-sized and 26.3 percent of micro companies. More than half of the companies (64.4 percent) prefer the information systems of domestic suppliers. Only 20 percent of large-sized companies and a smaller percentage of micro companies have developed their own business information systems. Medium-sized companies use purchased/licensed systems. Less than half of the large and medium-sized companies use two or more interconnected information systems at the same time. Business information systems support economic and commercial functions in 60.4 percent of companies, while in the other companies the production, controlling, CRM, investing etc. functions are also present. Business information systems in cloud are present in less than 15 percent of Slovenian TOUP companies. The business information systems in large and medium-sized companies are eight years old on average. During last year (2014), 40 percent of companies upgraded their business information systems. Investments into systems are small with the exceptions of some large-sized companies, where investments are reasonably bigger because of the systems' complexities.},
author = {Elesini, U.S. and Zakraj{\v{s}}ek, {\v{S}}. and Cerar, E. and Marolt, M. and Godec, P. and Urbas, R.},
doi = {10.14502/Tekstilec2015.58.281-300},
issn = {03513386},
journal = {Tekstilec},
keywords = {Business information systems,History review,Production of clothes,Production of leather and related products,Production of textiles,TOUP},
number = {4},
title = {{Information regarding Slovenian textile, clothing and leather production companies}},
volume = {58},
year = {2015}
}
@misc{Deshpande2015,
abstract = {We describe an automated method for image colorization that learns to colorize from examples. Our method exploits a LEARCH framework to train a quadratic objective function in the chromaticity maps, comparable to a Gaussian random field. The coefficients of the objective function are conditioned on image features, using a random forest. The objective function admits correlations on long spatial scales, and can control spatial error in the colorization of the image. Images are then colorized by minimizing this objective function. We demonstrate that our method strongly outperforms a natural baseline on large-scale experiments with images of real scenes using a demanding loss function. We demonstrate that learning a model that is conditioned on scene produces improved results. We show how to incorporate a desired color histogram into the objective function, and that doing so can lead to further improvements in results.},
author = {Deshpande, Aditya and Rock, Jason and Forsyth, David},
booktitle = {2015 IEEE International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2015.72},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Deshpande, Rock, Forsyth - 2015 - Learning Large-Scale Automatic Image Colorization.pdf:pdf},
isbn = {VO -},
keywords = {Color,Histograms,Image color analysis,LEARCH framework,Linear programming,Optimization,Regression tree analysis,Standards,chromaticity maps,color histogram,image processing,large-scale automatic image colorization learning,quadratic objective function,random forest,spatial error control},
mendeley-groups = {deep colorization},
pages = {567--575},
title = {{Learning Large-Scale Automatic Image Colorization}},
year = {2015}
}
@article{Larsson2016,
abstract = {We develop a fully automatic image colorization system. Our approach leverages recent advances in deep networks, exploiting both low-level and semantic representations during colorization. As many scene elements naturally appear according to multimodal color distributions, we train our model to predict per-pixel color histograms. This intermediate output can be used to automatically generate a color image, or further manipulated prior to image formation; our experiments consider both scenarios. On both fully and partially automatic colorization tasks, our system significantly outperforms all existing methods.},
archivePrefix = {arXiv},
arxivId = {1603.06668},
author = {Larsson, Gustav and Maire, Michael and Shakhnarovich, Gregory},
eprint = {1603.06668},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Larsson, Maire, Shakhnarovich - 2016 - Learning Representations for Automatic Colorization.pdf:pdf},
journal = {arXiv preprint},
keywords = {1,4 and 5,automatic colorization,convolutional neural networks,deep learning,fig,grayscale input,hy-,more examples in figures,our automatic colorization of,percolumns},
mendeley-groups = {deep colorization},
month = {mar},
title = {{Learning Representations for Automatic Colorization}},
url = {http://arxiv.org/abs/1603.06668},
year = {2016}
}
@article{Larsson2016,
abstract = {We develop a fully automatic image colorization system. Our approach leverages recent advances in deep networks, exploiting both low-level and semantic representations. As many scene elements naturally appear according to multimodal color distributions, we train our model to predict per-pixel color histograms. This intermediate output can be used to automatically generate a color image, or further manipulated prior to image formation. On both fully and partially automatic colorization tasks, we outperform existing methods. We also explore colorization as a vehicle for self-supervised visual representation learning.},
archivePrefix = {arXiv},
arxivId = {1603.06668},
author = {Larsson, Gustav and Maire, Michael and Shakhnarovich, Gregory},
eprint = {1603.06668},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Larsson, Maire, Shakhnarovich - 2016 - Learning Representations for Automatic Colorization.pdf:pdf},
month = {mar},
title = {{Learning Representations for Automatic Colorization}},
url = {http://arxiv.org/abs/1603.06668},
year = {2016}
}
@article{Cheng2015,
abstract = {This paper investigates into the colorization problem which converts a grayscale image to a colorful version. This is a very difficult problem and normally requires manual adjustment to achieve artifact-free quality. For instance, it normally requires human-labelled color scribbles on the grayscale target image or a careful selection of colorful reference images (e.g., capturing the same scene in the grayscale target image). Unlike the previous methods, this paper aims at a high-quality fully-automatic colorization method. With the assumption of a perfect patch matching technique, the use of an extremely large-scale reference database (that contains sufficient color images) is the most reliable solution to the colorization problem. However, patch matching noise will increase with respect to the size of the reference database in practice. Inspired by the recent success in deep learning techniques which provide amazing modeling of large-scale data, this paper re-formulates the colorization problem so that deep learning techniques can be directly employed. To ensure artifact-free quality, a joint bilateral filtering based post-processing step is proposed. We further develop an adaptive image clustering technique to incorporate the global image information. Numerous experiments demonstrate that our method outperforms the state-of-art algorithms both in terms of quality and speed.},
archivePrefix = {arXiv},
arxivId = {1511.04587},
author = {Cheng, Zezhou},
doi = {10.1109/ICCV.2015.55},
eprint = {1511.04587},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cheng - 2015 - Deep Colorization.pdf:pdf},
isbn = {978-1-4673-8391-2},
issn = {0162-8828},
journal = {CVPR},
mendeley-groups = {deep colorization},
pages = {415--423},
title = {{Deep Colorization}},
volume = {1},
year = {2015}
}
@article{Koleini2010,
abstract = {This study represents an innovative automatic method for black and white films colorization using texture features and a multilayer perceptron artificial neural network. In the proposed method, efforts are made to remove human interference in the process of colorization and replace it with an artificial neural network (ANN) which is trained using the features of the reference frame. Later, this network is employed for automatic colorization of the remained black and white frames. The reference frames of the black and white film are manually colored. Using a Gabor filter bank, texture features of all the pixels of the reference frame are extracted and used as the input feature vector of the ANN, while the output will be the color vector of the corresponding pixel. Finally, the next frames' feature vectors are fed respectively to the trained neural network, and color vectors of those frames are the output. Applying AVI videos and using various color spaces, a series of experiments are conducted to evaluate the proposed colorization process. This method needs considerable time to provide a reasonable output, given rapidly changing scenes. Fortunately however, due to the high correlation between consecutive frames in typical video footage, the overall performance is promising regarding both visual appearance and the calculated MSE error. Apart from the application, we also aim to show the importance of the low level features in a mainly high level process, and the mapping ability of a neural network.},
author = {Koleini, Mina and Mobadjemi, S A and Moallem, Payman},
doi = {10.1080/02533839.2010.9671693},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Koleini, Mobadjemi, Moallem - 2010 - Automatic Black and White Film Colorization Using Texture Features and Artificial Neural Networks.pdf:pdf},
isbn = {0253-3839},
issn = {02533839},
journal = {Journal of the Chinese Institute of Engineers},
keywords = {artificial neural networks,colorization,gabor filters,images,texture features},
mendeley-groups = {deep colorization},
number = {7},
pages = {1049--1057},
publisher = {Taylor {\&} Francis Group},
title = {{Automatic Black and White Film Colorization Using Texture Features and Artificial Neural Networks}},
volume = {33},
year = {2010}
}
@misc{Dahl,
author = {Dahl, Ryan},
mendeley-groups = {deep colorization},
title = {{Automatic Colorization}},
url = {http://tinyclouds.org/colorize/}
}
@article{Limmer2016,
abstract = {This paper proposes a method for transferring the RGB color spectrum to near-infrared (NIR) images using deep multi-scale convolutional neural networks. A direct and integrated transfer between NIR and RGB pixels is trained. The trained model does not require any user guidance or a reference image database in the recall phase to produce images with a natural appearance. To preserve the rich details of the NIR image, its high frequency features are transferred to the estimated RGB image. The presented approach is trained and evaluated on a real-world dataset containing a large amount of road scene images in summer. The dataset was captured by a multi-CCD NIR/RGB camera, which ensures a perfect pixel to pixel registration.},
archivePrefix = {arXiv},
arxivId = {1604.02245},
author = {Limmer, Matthias and Lensch, Hendrik P. A.},
eprint = {1604.02245},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Limmer, Lensch - 2016 - Infrared Colorization Using Deep Convolutional Neural Networks.pdf:pdf},
mendeley-groups = {deep colorization},
month = {apr},
pages = {6},
title = {{Infrared Colorization Using Deep Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1604.02245},
year = {2016}
}
@article{Hwang,
abstract = {We present a convolutional-neural-network-based sys-tem that faithfully colorizes black and white photographic images without direct human assistance. We explore var-ious network architectures, objectives, color spaces, and problem formulations. The final classification-based model we build generates colorized images that are significantly more aesthetically-pleasing than those created by the base-line regression-based model, demonstrating the viability of our methodology and revealing promising avenues for fu-ture work.},
author = {Hwang, Jeff and Zhou, You},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hwang, Zhou - Unknown - Image Colorization with Deep Convolutional Neural Networks.pdf:pdf},
mendeley-groups = {deep colorization},
title = {{Image Colorization with Deep Convolutional Neural Networks}}
}
@article{Koo2016,
abstract = {We attempt to use DCGANs (deep convolutional genera-tive adversarial nets) to tackle the automatic colorization of black and white photos to combat the tendency for vanilla neural nets to " average out " the results. We construct a small feed-forward convolutional neural network as a base-line colorization system. We train the baseline model on the CIFAR-10 dataset with a per-pixel Euclidean loss func-tion on the chrominance values and achieve sensible but mediocre results. We propose using the adversarial frame-work as proposed by Goodfellow et al. [5] as an alternative to the loss function—we reformulate the baseline model as a generator model that maps grayscale images and random noise input to the color image space, and construct a dis-criminator model that is trained to predict the probability that a given colorization was sampled from data distribu-tion rather than generated by the generator model, condi-tioned on the grayscale image. We analyze the challenges that stand in the way of training adversarial networks, and suggest future steps to test the viability of the model.},
author = {Koo, Stephen},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Koo - 2016 - Automatic Colorization with Deep Convolutional Generative Adversarial Networks.pdf:pdf},
mendeley-groups = {deep colorization},
title = {{Automatic Colorization with Deep Convolutional Generative Adversarial Networks}},
url = {http://cs231n.stanford.edu/reports2016/224{\_}Report.pdf},
year = {2016}
}
@article{iizuka2016let,
author = {Iizuka, Satoshi and Simo-Serra, Edgar and Ishikawa, Hiroshi},
journal = {ACM Transactions on Graphics (TOG)},
mendeley-groups = {deep colorization},
number = {4},
pages = {110},
publisher = {ACM},
title = {{Let there be color!: joint end-to-end learning of global and local image priors for automatic image colorization with simultaneous classification}},
volume = {35},
year = {2016}
}
@inproceedings{Iizuka2016,
abstract = {We present a novel technique to automatically colorize grayscale images that combines both global priors and local image features. Based on Convolutional Neural Networks, our deep network fea- tures a fusion layer that allows us to elegantly merge local information dependent on small image patches with global priors computed using the entire image. The entire framework, including the global and local priors as well as the colorization model, is trained in an end-to-end fashion. Furthermore, our architecture can process images of any resolution, unlike most existing approaches based on CNN. We leverage an existing large-scale scene classification data-base to train our model, exploiting the class labels of the dataset to more efficiently and discriminatively learn the global priors. We validate our approach with a user study and compare against the state of the art, where we show significant improvements. Further- more, we demonstrate our method extensively on many different types of images, including black-and-white photography from over a hundred years ago, and show realistic colorizations.},
author = {Iizuka, Satoshi and {Edgar Simo-Serra} and Ishikawa, Hiroshi},
booktitle = {SIGGRAPH '16},
doi = {10.1145/2897824.2925974},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Iizuka, Edgar Simo-Serra, Ishikawa - 2016 - Let there be Color! Joint End-to-end Learning of Global and Local Image Priors for Automatic.pdf:pdf},
isbn = {9781450342797},
issn = {07300301},
keywords = {colorization,computing methodologies,convolutional neural network concepts,image processing,neural net-},
mendeley-groups = {deep colorization},
number = {4},
pages = {110},
publisher = {ACM},
title = {{Let there be Color!: Joint End-to-end Learning of Global and Local Image Priors for Automatic Image Colorization with Simultaneous Classification}},
volume = {35},
year = {2016}
}
@book{T.H.Cormen2009,
address = {Cambridge, Massachusetts London},
author = {{T. H. Cormen} and {C. E. Leiserson} and {R. L. Rivest} and {C. Stein}},
edition = {3},
publisher = {The MIT Press},
title = {{Introduction to Algorithms}},
year = {2009}
}
@inproceedings{bu2010music,
author = {Bu, Jiajun and Tan, Shulong and Chen, Chun and Wang, Can and Wu, Hao and Zhang, Lijun and He, Xiaofei},
booktitle = {Proceedings of the international conference on Multimedia},
mendeley-groups = {mood},
organization = {ACM},
pages = {391--400},
title = {{Music recommendation by unified hypergraph: combining social media information and music content}},
year = {2010}
}
@book{lawrence2008fundamentals,
author = {Lawrence, Rabiner},
publisher = {Pearson Education India},
title = {{Fundamentals of speech Recognition}},
year = {2008}
}
@inproceedings{julier1997consistent,
author = {Julier, Simon J and Uhlmann, Jeffrey K},
booktitle = {AeroSense'97},
mendeley-groups = {mood},
organization = {International Society for Optics and Photonics},
pages = {110--121},
title = {{Consistent debiased method for converting between polar and Cartesian coordinate systems}},
year = {1997}
}
@inproceedings{susstrunk1999standard,
author = {S{\"{u}}sstrunk, Sabine and Buckley, Robert and Swen, Steve},
booktitle = {Color and Imaging Conference},
number = {1},
organization = {Society for Imaging Science and Technology},
pages = {127--134},
title = {{Standard RGB color spaces}},
volume = {1999},
year = {1999}
}
@inproceedings{Sural2002,
author = {Sural, S. and Pramanik, S.},
booktitle = {Proceedings. International Conference on Image Processing},
doi = {10.1109/ICIP.2002.1040019},
isbn = {0-7803-7622-6},
issn = {1522-4880},
keywords = {Content based retrieval,Feature extraction,HSV color space,Histograms,Image analysis,Image color analysis,Image retrieval,Image segmentation,Pixel,RGB color space,Smoothing methods,Visual perception,content based image retrieval,content-based retrieval,feature extraction,histogram generation,hue value,image colour analysis,image pixel,image retrieval,image segmentation,intensity value,object identification,pixel features extraction,saturation value,smoothing methods,statistical analysis,uniform color transition,visual databases,visual perception,window-based smoothing},
language = {English},
pages = {II--589--II--592},
publisher = {IEEE},
title = {{Segmentation and histogram generation using the HSV color space for image retrieval}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=1040019},
volume = {2},
year = {2002}
}
@inproceedings{chen2001music,
author = {Chen, Hung-Chen and Chen, Arbee L P},
booktitle = {Proceedings of the tenth international conference on Information and knowledge management},
organization = {ACM},
pages = {231--238},
title = {{A music recommendation system based on music data grouping and user interests}},
year = {2001}
}
@incollection{JaeSikLee2006,
address = {Berlin, Heidelberg},
author = {{Jae Sik Lee}, Jin Chun Lee},
booktitle = {Smart Sensing and Context},
doi = {10.1007/11907503},
editor = {Havinga, Paul and Lijding, Maria and Meratnia, Nirvana and Wegdam, Maarten},
isbn = {978-3-540-47842-3},
pages = {190--203},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Music for My Mood: A Music Recommendation System Based on Context Reasoning}},
url = {http://www.springerlink.com/index/10.1007/11907503},
volume = {4272},
year = {2006}
}
@article{Li2007,
author = {Li, Yipeng and Wang, DeLiang},
doi = {10.1109/TASL.2006.889789},
issn = {1558-7916},
journal = {IEEE Transactions on Audio, Speech and Language Processing},
keywords = {Auditory system,Automatic speech recognition,Cognitive science,Computer science,Humans,Instruments,Laboratories,Music information retrieval,Predominant pitch detection,Speech recognition,Time frequency analysis,audio recording,lyrics recognition,monaural recordings,music,music accompaniment,music information retrieval,predominant pitch detection,singer identification,singing voice detection,singing voice separation,sound separation,speech processing,speech recognition,time-frequency segments,track seperation},
language = {English},
mendeley-groups = {mood},
mendeley-tags = {track seperation},
month = {may},
number = {4},
pages = {1475--1487},
publisher = {IEEE},
title = {{Separation of Singing Voice From Music Accompaniment for Monaural Recordings}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=4156205},
volume = {15},
year = {2007}
}
@article{Gillet2008,
author = {Gillet, O. and Richard, G.},
doi = {10.1109/TASL.2007.914120},
issn = {1558-7916},
journal = {IEEE Transactions on Audio, Speech, and Language Processing},
keywords = {Drum signals,Wiener filtering,Wiener filtering-based separation method,Wiener filters,audio signal processing,drum signals separation,drum signals transcription,drum track extraction,feature selection,filtering theory,fusion strategies,harmonic-noise decomposition,harmonic/noise decomposition,music,music transcription,musical instruments,polyphonic music,source separation,support vector machine (SVM),time-frequency analysis,time-frequency masking,track seperation},
language = {English},
mendeley-groups = {mood},
mendeley-tags = {track seperation},
month = {mar},
number = {3},
pages = {529--540},
publisher = {IEEE},
title = {{Transcription and Separation of Drum Signals From Polyphonic Music}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=4443887},
volume = {16},
year = {2008}
}
@inproceedings{bogdanov2013essentia,
author = {Bogdanov, Dmitry and Wack, Nicolas and G{\'{o}}mez, Emilia and Gulati, Sankalp and Herrera, Perfecto and Mayor, Oscar and Roma, Gerard and Salamon, Justin and Zapata, Jos{\'{e}} R and Serra, Xavier},
booktitle = {Proceedings of the International Conference on Music Information Retrieval (ISMIR)},
mendeley-groups = {mood},
organization = {Citeseer},
pages = {493--498},
title = {{Essentia: An Audio Analysis Library for Music Information Retrieval.}},
year = {2013}
}
@inproceedings{saari2013role,
author = {Saari, Pasi and Eerola, Tuomas and Fazekas, Gy{\"{o}}rgy and Barthet, Mathieu and Lartillot, Olivier and Sandler, Mark B},
booktitle = {Proceedings of the International Conference on Music Information Retrieval (ISMIR)},
mendeley-groups = {mood},
pages = {201--206},
title = {{The Role of Audio and Tags in Music Mood Prediction: A Study Using Semantic Layer Projection.}},
year = {2013}
}
@inproceedings{watson2012modeling,
author = {Watson, Diane and Mandryk, Regan L},
booktitle = {Proceedings of the International Conference on Music Information Retrieval (ISMIR)},
mendeley-groups = {mood},
pages = {31--36},
title = {{Modeling Musical Mood From Audio Features and Listening Context on an In-Situ Data Set.}},
year = {2012}
}
@inproceedings{chu2010lamp,
author = {Chu, Wei-rong and Tsai, RT-H and Wu, Ying-Sian and Wu, Hui-Hsin and Chen, Hung-Yi and Hsu, JY-J},
booktitle = {Technologies and Applications of Artificial Intelligence (TAAI), 2010 International Conference on},
mendeley-groups = {mood},
organization = {IEEE},
pages = {53--59},
title = {{LAMP, a lyrics and audio mandopop dataset for music mood estimation: Dataset compilation, system construction, and testing}},
year = {2010}
}
@phdthesis{pesek2012prepoznavanje,
author = {Pesek, Matev{\v{z}}},
mendeley-groups = {mood},
school = {M. Pesek},
title = {{Prepoznavanje akordov s hierarhi{\{}{\v{c}}{\}}nim kompozicionalnim modelom: diplomsko delo}},
year = {2012}
}
@article{la2001harmonic,
author = {{La Rue}, Jan},
journal = {The Journal of Musicology},
mendeley-groups = {mood},
number = {2},
pages = {221--248},
publisher = {JSTOR},
title = {{Harmonic Rhythm in the Beethoven symphonies}},
volume = {18},
year = {2001}
}
@article{terhardt1974pitch,
author = {Terhardt, Ernst},
journal = {The Journal of the Acoustical Society of America},
mendeley-groups = {mood},
number = {5},
pages = {1061--1069},
publisher = {Acoustical Society of America},
title = {{Pitch, consonance, and harmony}},
volume = {55},
year = {1974}
}
@inproceedings{zhu2005music,
author = {Zhu, Yongwei and Kankanhalli, Mohan S and Gao, Sheng},
booktitle = {Multimedia Modelling Conference, 2005. MMM 2005. Proceedings of the 11th International},
mendeley-groups = {mood},
organization = {IEEE},
pages = {30--37},
title = {{Music key detection for musical audio}},
year = {2005}
}
@inproceedings{gouyon2000classifying,
author = {Gouyon, F and Delerue, O and Pachet, F},
booktitle = {Proceedings of the COST G-6 Conference on Digital Audio Effects},
mendeley-groups = {mood},
title = {{Classifying percussive sounds: a matter of zero-crossing rate?}},
year = {2000}
}
@inproceedings{brossier2004real,
author = {Brossier, Paul and Bello, Juan Pablo and Plumbley, Mark D},
booktitle = {Proceedings of the ICMC},
mendeley-groups = {mood},
title = {{Real-time temporal segmentation of note objects in music signals}},
year = {2004}
}
@inproceedings{gouyon2001exploration,
author = {Gouyon, Fabien and Herrera, Perfecto},
booktitle = {Proceedings of MOSART: Workshop on Current Directions in Computer Music},
mendeley-groups = {mood},
title = {{Exploration of techniques for automatic labeling of audio drum tracks instruments}},
year = {2001}
}
@article{gunderson2007musical,
author = {Gunderson, Steinar Heimdal},
mendeley-groups = {mood},
publisher = {Institutt for elektronikk og telekommunikasjon},
title = {{Musical descriptors: An assessment of psychoacoustical models in the presence of lossy compression}},
year = {2007}
}
@phdthesis{bogdanov2013form,
address = {Barcelona, Spain},
author = {Bogdanov, D},
keywords = {music discovery,music information retrieval,music recommendation,music similarity,personalization,preference elicitation,recommender systems,user modeling,visualization},
mendeley-groups = {mood},
pages = {227},
school = {Universitat Pompeu Fabra},
title = {{From music similarity to music recommendation: Computational approaches based on audio features and metadata}},
year = {2013}
}
@techreport{lenko2009pomen,
author = {Lenko, Mira and Kogov{\v{s}}ek, Tina and Stankovi{\'{c}}, Peter},
mendeley-groups = {mood},
publisher = {M. Lenko},
title = {{Pomen glasbe v o{\v{c}}eh mladih: diplomsko delo}},
year = {2009}
}
@incollection{Krause2012,
address = {London},
author = {Krause, Bernie},
booktitle = {The Great Animal Orchestra: Finding the Origins of Music in the World's Wild Places},
chapter = {Echonest o},
mendeley-groups = {mood},
pages = {5--10},
publisher = {Hachette Digital, Inc.},
title = {{Echonest of the past}},
year = {2012}
}
@incollection{Wallin2001,
author = {Wallin, Nils Lenart and {Merker Bjorn} and Brown, Steven},
booktitle = {The origins of music},
mendeley-groups = {mood},
publisher = {MIT Press},
title = {{The origins of music}},
year = {2001}
}
@book{,
abstract = {What biological and cognitive forces have shaped humankind's musical behavior and the rich global repertoire of musical structures? What is music for, and why does every human culture have it? What are the universal features of music and musical behavior across cultures? In this groundbreaking book, musicologists, biologists, anthropologists, archaeologists, psychologists, neuroscientists, ethologists, and linguists come together for the first time to examine these and related issues. The book can be viewed as representing the birth of evolutionary biomusicology -- the study of which will contribute greatly to our understanding of the evolutionary precursors of human music, the evolution of the hominid vocal tract, localization of brain function, the structure of acoustic-communication signals, symbolic gesture, emotional manipulation through sound, self-expression, creativity, the human affinity for the spiritual, and the human attachment to music itself.Contributors: Simha Arom, Derek Bickerton, Steven Brown, Ellen Dissanayake, Dean Falk, David W. Frayer, Walter Freeman, Thomas Geissmann, Marc D. Hauser, Michel Imberty, Harry Jerison, Drago Kunej, Francois-Bernard Mache, Peter Marler, Bjorn Merker, Geoffrey Miller, Jean Molino, Bruno Nettl, Chris Nicolay, Katharine Payne, Bruce Richman, Peter J.B. Slater, Peter Todd, Sandra Trehub, Ivan Turk, Maria Ujhelyi, Nils L. Wallin, Carol Whaling."},
isbn = {0262731436},
title = {{The Origins of Music}},
url = {http://www.google.si/books?hl=sl{\&}lr={\&}id=vYQEakqM4I0C{\&}pgis=1},
year = {2001}
}
@article{Moorer1977,
author = {Moorer, James A},
journal = {Computer music journal},
mendeley-groups = {mood},
number = {4},
pages = {32--38},
title = {{On the Transcription of Musical Sound by Computer}},
volume = {1},
year = {1977}
}
@article{Piszczalski1986,
author = {Piszczalski, Martin},
mendeley-groups = {mood},
publisher = {University of Michigan},
title = {{A computational model of music transcription}},
url = {http://dl.acm.org/citation.cfm?id=15202},
year = {1986}
}
@book{Rokach2007,
author = {Rokach, Lior and Maimon, Oded Z},
mendeley-groups = {mood},
pages = {244},
publisher = {World Scientific Publishing},
title = {{Data Mining with Decision Trees: Theory and Applications}},
year = {2007}
}
@article{Papadopoulos2007,
author = {Papadopoulos, Helene and Peeters, Geoffroy},
journal = {Content-Based Multimedia Indexing},
mendeley-groups = {mood},
title = {{Large-case Study of Chord Estimation Algorithms Based on Chroma Representation and HMM}},
volume = {53-60},
year = {2007}
}
@misc{Smith1983,
author = {Smith, Dave and Wood, Chet},
mendeley-groups = {mood},
title = {{MIDI Musical Instrument Digital Interface Specification 1.0}},
year = {1983}
}
@article{Werbos1990,
abstract = {Basic backpropagation, which is a simple method now being widely used in areas like pattern recognition and fault diagnosis, is reviewed. The basic equations for backpropagation through time, and applications to areas like pattern recognition involving dynamic systems, systems identification, and control are discussed. Further extensions of this method, to deal with systems other than neural networks, systems involving simultaneous equations, or true recurrent networks, and other practical issues arising with the method are described. Pseudocode is provided to clarify the algorithms. The chain rule for ordered derivatives-the theorem which underlies backpropagation-is briefly discussed. The focus is on designing a simpler version of backpropagation which can be translated into computer code and applied directly by neutral network users},
author = {Werbos, P J},
doi = {10.1109/5.58337},
issn = {00189219},
journal = {Proceedings of the IEEE},
mendeley-groups = {mood},
number = {10},
pages = {1550--1560},
shorttitle = {Proceedings of the IEEE},
title = {{Backpropagation through time: what it does and how to do it}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=58337},
volume = {78},
year = {1990}
}
@book{Holzinger2008,
address = {Berlin, Heidelberg},
doi = {10.1007/978-3-540-89350-9},
editor = {Holzinger, Andreas},
isbn = {978-3-540-89349-3},
issn = {0302-9743},
mendeley-groups = {mood},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{HCI and Usability for Education and Work}},
url = {http://www.springerlink.com/index/10.1007/978-3-540-89350-9},
volume = {5298},
year = {2008}
}
@article{Brown1991,
author = {Brown, Judith},
journal = {Journal of the Acoustical Society of America},
mendeley-groups = {mood},
number = {1},
pages = {425--434},
title = {{Calculation of a constant Q spectral transform}},
volume = {89},
year = {1991}
}
@article{Conklin2010,
author = {Conklin, Darrell},
issn = {1088-467X},
journal = {Intelligent Data Analysis},
keywords = {Pattern discovery,anticorpus,chord sequences,distinctive pattern,folk songs,subsumption},
mendeley-groups = {mood},
number = {5},
pages = {547--554},
publisher = {IOS Press},
title = {{Discovery of distinctive patterns in music}},
url = {http://dl.acm.org/citation.cfm?id=1859240.1859243},
volume = {14},
year = {2010}
}
@article{Cooper2006,
author = {Cooper, Matthew and Foote, Jonathan and Pampalk, Elias and Tzanetakis, George},
journal = {Computer music journal},
mendeley-groups = {mood},
number = {2},
pages = {42--62},
title = {{Visualization in Audio-Based Music Information Retrieval}},
volume = {30},
year = {2006}
}
@article{Roebel2010,
author = {Roebel, Axel and Rodet, Xavier},
doi = {10.1109/TASL.2009.2030006},
issn = {1558-7916},
journal = {IEEE Transactions on Audio, Speech, and Language Processing},
mendeley-groups = {mood},
number = {6},
pages = {1116--1126},
publisher = {IEEE},
title = {{Multiple Fundamental Frequency Estimation and Polyphony Inference of Polyphonic Music Signals}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=5200519},
volume = {18},
year = {2010}
}
@techreport{Gerhard,
address = {Regina},
author = {Gerhard, David},
institution = {University of Regina, Saskatchewan, Canada},
keywords = {Technical Report TR-CS},
mendeley-groups = {mood},
mendeley-tags = {Technical Report TR-CS},
title = {{Pitch Extraction and Fundamental Frequency: History and Current Techniques}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.58.834},
year = {2003}
}
@book{Temperley2007,
author = {Temperley, David},
mendeley-groups = {mood},
pages = {244},
publisher = {MIT Press},
title = {{Music and probability}},
year = {2007}
}
@inproceedings{Kavcic2013,
address = {Bled},
author = {Kav{\v{c}}i{\v{c}}, Alenka and Pesek, Matev{\v{z}} and Bohak, Ciril and Marolt, Matija},
booktitle = {Proceedings of the 1st Workshop on Defining a European Research Agenda on Information Systems and Management in eMedia Industries},
mendeley-groups = {mood},
pages = {14--16},
title = {{Edoo : online match-making portal for educational content production}},
year = {2013}
}
@inproceedings{Battenberg2012,
author = {Battenberg, Eric and Wessel, David},
booktitle = {Proceedings of the International Conference on Music Information Retrieval (ISMIR)},
mendeley-groups = {mood},
pages = {37--42},
title = {{Analyzing Drum Patterns using Conditional Deep Belief Networks}},
year = {2012}
}
@article{Sapp2005,
author = {Sapp, Craig Stuart},
journal = {Computers and Intertainment},
mendeley-groups = {mood},
number = {4},
pages = {1--19},
title = {{Visual hierarchical key analysis}},
volume = {3},
year = {2005}
}
@phdthesis{Moorer1975,
author = {Moorer, James A},
mendeley-groups = {mood},
pages = {171},
title = {{On the Segmentation and Analysis of Continuous Musical Sound by Digital Computer}},
url = {https://ccrma.stanford.edu/papers/segmentation-and-analysis-of-continuous-musical-sound-digital-computer},
year = {1975}
}
@article{Salamon2014,
abstract = {Melody extraction algorithms aim to produce a sequence of frequency values corresponding to the pitch of the dominant melody from a musical recording. Over the past decade, melody extraction has emerged as an active research topic, comprising a large variety of proposed algorithms spanning a wide range of techniques. This article provides an overview of these techniques, the applications for which melody extraction is useful, and the challenges that remain. We start with a discussion of ?melody? from both musical and signal processing perspectives and provide a case study that interprets the output of a melody extraction algorithm for specific excerpts. We then provide a comprehensive comparative analysis of melody extraction algorithms based on the results of an international evaluation campaign. We discuss issues of algorithm design, evaluation, and applications that build upon melody extraction. Finally, we discuss some of the remaining challenges in melody extraction research in terms of algorithmic performance, development, and evaluation methodology.},
author = {Salamon, Justin and Gomez, Emilia and Ellis, Daniel P W and Richard, Gael},
doi = {10.1109/MSP.2013.2271648},
issn = {1053-5888},
journal = {IEEE Signal Processing Magazine},
mendeley-groups = {mood},
number = {2},
pages = {118--134},
shorttitle = {Signal Processing Magazine, IEEE},
title = {{Melody Extraction from Polyphonic Music Signals: Approaches, applications, and challenges}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6739213},
volume = {31},
year = {2014}
}
@incollection{Scherer2001,
address = {New York},
author = {Scherer, K R and Zentner, M R},
booktitle = {Music and emotion},
editor = {Juslin, P N and Sloboda, J A},
mendeley-groups = {mood},
publisher = {Oxford University Press},
title = {{Emotional effects of music: production rules}},
year = {2001}
}
@inproceedings{mauch:adt:2013,
author = {Mauch, Matthias and Ewert, Sebastian},
booktitle = {Proceedings of the 14th International Society for Music Information Retrieval Conference (ISMIR 2013)},
mendeley-groups = {mood},
pages = {83--88},
title = {{The Audio Degradation Toolbox and its Application to Robustness Evaluation}},
year = {2013}
}
@article{Emiya2010,
abstract = {A new method for the estimation of multiple concurrent pitches in piano recordings is presented. It addresses the issue of overlapping overtones by modeling the spectral envelope of the overtones of each note with a smooth autoregressive model. For the background noise, a moving-average model is used and the combination of both tends to eliminate harmonic and sub-harmonic erroneous pitch estimations. This leads to a complete generative spectral model for simultaneous piano notes, which also explicitly includes the typical deviation from exact harmonicity in a piano overtone series. The pitch set which maximizes an approximate likelihood is selected from among a restricted number of possible pitch combinations as the one. Tests have been conducted on a large homemade database called MAPS, composed of piano recordings from a real upright piano and from high-quality samples.},
author = {Emiya, Valentin and Badeau, Roland and David, Bertrand},
doi = {10.1109/TASL.2009.2038819},
issn = {1558-7916},
journal = {IEEE Transactions on Audio, Speech, and Language Processing},
keywords = {Acoustic signal analysis,MAPS,acoustic signal processing,audio processing,autoregressive processes,homemade database,moving-average model,multipitch estimation,multipitch estimation (MPE),musical instruments,overlapping overtones,piano,piano overtone series,piano recordings,piano sounds,probabilistic spectral smoothness principle,probability,smooth autoregressive model,smoothing methods,spectral analysis,spectral envelope modeling,spectral smoothness,transcription},
mendeley-groups = {mood},
number = {6},
pages = {1643--1654},
shorttitle = {Audio, Speech, and Language Processing, IEEE Trans},
title = {{Multipitch Estimation of Piano Sounds Using a New Probabilistic Spectral Smoothness Principle}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5356234},
volume = {18},
year = {2010}
}
@inproceedings{logan2000mel,
author = {Logan, Beth and Others},
booktitle = {ISMIR},
mendeley-groups = {mood},
title = {{Mel Frequency Cepstral Coefficients for Music Modeling.}},
year = {2000}
}
@article{Au2003,
abstract = {In bats and technological sonars, the gain of the receiver is progressively increased with time after the transmission of a signal to compensate for acoustic propagation loss. The current understanding of dolphin echolocation indicates that automatic gain control is not a part of their sonar system. In order to test this understanding, we have performed field measurements of free-ranging echolocating dolphins. Here we show that dolphins do possess an automatic gain control mechanism, but that it is implemented in the transmission phase rather than the receiving phase of a sonar cycle. We find that the amplitude of the dolphins' echolocation signals are highly range dependent; this amplitude increases with increasing target range, R, in a 20 log(R) fashion to compensate for propagation loss. If the echolocation target is a fish school with many sound scatterers, the echoes from the school will remain nearly constant with range as the dolphin closes in on it. This characteristic has the same effect as time-varying gain in bats and technological sonar when considered from a sonar system perspective.},
author = {Au, Whitlow W L and Benoit-Bird, Kelly J},
doi = {10.1038/nature01727},
issn = {0028-0836},
journal = {Nature},
keywords = {Acoustics,Animals,Chiroptera,Chiroptera: physiology,Dolphins,Dolphins: classification,Dolphins: physiology,Echolocation,Echolocation: physiology,Hearing,Hearing: physiology,Sound,Transportation,Transportation: instrumentation,Whales,Whales: physiology},
mendeley-groups = {mood},
number = {6942},
pages = {861--863},
pmid = {12815429},
shorttitle = {Nature},
title = {{Automatic gain control in the echolocation system of dolphins.}},
url = {http://dx.doi.org/10.1038/nature01727},
volume = {423},
year = {2003}
}
@book{Albert2013,
abstract = {Measuring the User Experience was the first book that focused on how to quantify the user experience. Now in the second edition, the authors include new material on how recent technologies have made it easier and more effective to collect a broader range of data about the user experience. As more UX and web professionals need to justify their design decisions with solid, reliable data, Measuring the User Experience provides the quantitative analysis training that these professionals need. The second edition presents new metrics such as emotional engagement, personas, keystroke analysis, and net promoter score. It also examines how new technologies coming from neuro-marketing and online market research can refine user experience measurement, helping usability and user experience practitioners make business cases to stakeholders. The book also contains new research and updated examples, including tips on writing online survey questions, six new case studies, and examples using the most recent version of Excel.Learn which metrics to select for every case, including behavioral, physiological, emotional, aesthetic, gestural, verbal, and physical, as well as more specialized metrics such as eye-tracking and clickstream data.Find a vendor-neutral examination of how to measure the user experience with web sites, digital products, and virtually any other type of product or system. Discover in-depth global case studies showing how organizations have successfully used metrics and the information they revealed.Companion site, www.measuringux.com, includes articles, tools, spreadsheets, presentations, and other resources to help you effectively measure the user experience},
author = {Albert, William and Tullis, Thomas},
isbn = {0124157920},
mendeley-groups = {mood},
pages = {320},
publisher = {Newnes},
title = {{Measuring the User Experience: Collecting, Analyzing, and Presenting Usability Metrics (Google eBook)}},
url = {http://books.google.com/books?id=bPhLeMBLEkAC{\&}pgis=1},
year = {2013}
}
@inproceedings{ShigekiSagayama2004,
address = {Jeju, Korea},
author = {Sagayama, Shigeki and Takahashi, Keigo},
booktitle = {ISCA Tutorial and Research Workshop on Statistical and Perceptual Audio Processing},
mendeley-groups = {mood},
title = {{Specmurt anasylis: A piano-roll-visualization of polyphonic music signal by deconvolution of log-frequency spectrum}},
year = {2004}
}
@inproceedings{Mauch2008,
address = {Philadelphia},
author = {Mauch, Mathias and Dixon, Simon},
booktitle = {Proceedings of the International Conference on Music Information Retrieval (ISMIR)},
mendeley-groups = {mood},
pages = {45--50},
title = {{A Discrete Mixture Model for Chord Labelling}},
volume = {1},
year = {2008}
}
@article{Foote1999,
author = {Foote, Jonathan},
journal = {Multimedia Systems},
mendeley-groups = {mood},
number = {1},
pages = {2--10},
title = {{An overview of audio information retrieval}},
url = {http://link.springer.com/article/10.1007/s005300050106?LI=true{\#}page-1},
volume = {7},
year = {1999}
}
@article{laurier2007audio,
author = {Laurier, Cyril and Herrera, Perfecto and Mandel, M and Ellis, D},
journal = {Music Information Retrieval Evaluation eXchange (MIREX) extended abstract},
mendeley-groups = {mood},
publisher = {Citeseer},
title = {{Audio music mood classification using support vector machine}},
year = {2007}
}
@article{McDermott2008,
author = {McDermott, Josh H and Oxenham, Andrew J},
journal = {Current opinion in Neurobiology},
mendeley-groups = {mood},
number = {18},
pages = {452--463},
title = {{Music perception, pitch and the auditory system}},
year = {2008}
}
@article{Lee2008,
author = {Lee, Kyogu and Stanley, Malcom},
journal = {IEEE Transactions on Audio, Speech, and Language Processing},
mendeley-groups = {mood},
number = {2},
pages = {291--301},
title = {{Acoustic Chord Transcription and Key Extraction From Audio Using Key-Dependent HMMs Trained on Synthesized Audio}},
volume = {16},
year = {2008}
}
@article{Beeli2007,
author = {Beeli, Gian and Esslen, Michaela and Jancke, Lutz},
journal = {Psychological Science},
mendeley-groups = {mood},
number = {9},
pages = {788--792},
title = {{Frequency Correlates in Grapheme-Color Synaesthesia}},
volume = {18},
year = {2007}
}
@incollection{Rumelhart1986,
abstract = {What makes people smarter than computers? The work described in these two volumes suggests that the answer lies in the massively parallel architecture of the human mind. It is some of the most exciting work in cognitive science, unifying neural and cognitive processes in a highly computational framework, with links to artificial intelligence. Although thought and problem solving have a sequential character when viewed over a time frame of minutes or hours, the authors argue that each step in the sequence is the result of the simultaneous activity of a large number of simple computational elements, each influencing others and being influenced by them. "Parallel Distributed Processing" describes their work in developing a theoretical framework for describing this parallel distributed processing activity and in applying the framework to the development of models of aspects of perception, memory, language, and thought. Volume 1 lays the theoretical foundations of parallel distributed processing. It introduces the approach and the reasons why the authors feel it is a fruitful one, describes several models of basic mechanisms with wide applicability to different problems, and presents a number of specific technical analyses of different aspects of parallel distributed models.},
author = {Rumelhart, David E},
booktitle = {Parallel Distributed Processing: Foundations},
chapter = {8},
editor = {McClelland, James L and Rumelhart, David E},
isbn = {026268053X},
mendeley-groups = {mood},
pages = {318--362},
publisher = {MIT Press},
title = {{Learning intemal representations by error propagation}},
url = {http://books.google.si/books/about/Parallel{\_}Distributed{\_}Processing{\_}Foundati.html?id=eFPqqMBK-p8C{\&}pgis=1},
year = {1986}
}
@inproceedings{Scholz2009,
abstract = {The modeling of music as a language is a core issue for a wide range of applications such as polyphonic music retrieval, automatic style identification, audio to symbolic music transcription and computer-assisted composition. In this paper, we focus on the modeling of chord sequences by probabilistic N-grams. Previous studies using these models have achieved limited success, due to overfitting and to the use of a single chord labeling scheme. We investigate these issues using model smoothing and selection techniques initially designed for spoken language modeling. This approach is evaluated over a set of songs by The Beatles, considering several chord labeling schemes. Initial results show that the accuracy of N-grams is increased but that additional improvements may still be achieved in the future using more advanced, possibly music-specific, smoothing techniques.},
author = {Scholz, Ricardo and Vincent, Emmanuel and Bimbot, Frederic},
booktitle = {Proceedings of International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
doi = {10.1109/ICASSP.2009.4959518},
isbn = {978-1-4244-2353-8},
issn = {1520-6149},
keywords = {Dictionaries,Hidden Markov models,History,Labeling,Music,Music information retrieval,N-grams,Natural languages,Robustness,Smoothing methods,Testing,Training data,audio-to-symbolic music transcription,automatic style identification,computer-assisted composition,information retrieval,model selection,model smoothing,model smoothing techniques,musical chord sequences,polyphonic music retrieval,probabilistic N-grams,probabilistic modeling,probability,single chord labeling scheme},
mendeley-groups = {mood},
month = {apr},
pages = {53--56},
publisher = {IEEE},
shorttitle = {Acoustics, Speech and Signal Processing, 2009. ICA},
title = {{Robust modeling of musical chord sequences using probabilistic N-grams}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4959518},
year = {2009}
}
@inproceedings{Smith2011,
address = {Miami},
author = {Smith, Jordan B L and Burgoyne, J Ashley and Fujinaga, Ichiro and {De Roure}, David and Downie, J Stephen},
booktitle = {Proceedings of the International Conference on Music Information Retrieval (ISMIR)},
mendeley-groups = {mood},
number = {Ismir},
pages = {555--560},
title = {{Design and Creation of a Large-scale Database of Structural Annotations}},
year = {2011}
}
@inproceedings{Smith2011,
address = {Miami},
author = {Smith, Jordan B L and Burgoyne, J Ashley and Fujinaga, Ichiro and {De Roure}, David and Downie, J Stephen},
booktitle = {Proceedings of the International Conference on Music Information Retrieval (ISMIR)},
mendeley-groups = {mood},
number = {Ismir},
pages = {555--560},
title = {{Design and Creation of a Large-scale Database of Structural Annotations}},
year = {2011}
}
@inproceedings{Boulanger-Lewandowski2011,
author = {Boulanger-Lewandowski, N and Vincent, P and Bengio, Yoshua},
booktitle = {Snowbird Learning workshop},
mendeley-groups = {mood},
title = {{Energy-based Recurrent Neural Network for Multiple Fundamental Frequency Estimation}},
year = {2011}
}
@inproceedings{Pesek2013a,
author = {Pesek, Matev{\v{z}} and Miheli{\v{c}}, France},
booktitle = {Zbornik dvaindvajsete mednarodne Elektrotehni{\v{s}}ke in ra{\v{c}}unalni{\v{s}}ke konference},
mendeley-groups = {mood},
pages = {145--148},
title = {{Hidden Markov model for chord estimation using compositional hierarchical model features}},
year = {2013}
}
@incollection{Fidler2009,
author = {Fidler, Sanja and Boben, Marko and Leonardis, Ale{\v{s}}},
booktitle = {Object Categorization: Computer and Human Vision Perspectives},
mendeley-groups = {mood},
pages = {196--215},
publisher = {Cambridge University Press},
title = {{Learning Hierarchical Compositional Representations of Object Structure}},
year = {2009}
}
@inproceedings{Dressler2011,
author = {Dressler, Karin},
mendeley-groups = {mood},
pages = {19--24},
title = {{An Auditory Streaming Approach for Melody Extraction from Polyphonic Music.}},
year = {2011}
}
@article{Bengio2013,
abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.},
author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
doi = {10.1109/TPAMI.2013.50},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {Algorithms,Artificial Intelligence,Artificial Intelligence: trends,Humans,Neural Networks (Computer)},
mendeley-groups = {mood},
number = {8},
pages = {1798--1828},
pmid = {23787338},
shorttitle = {Pattern Analysis and Machine Intelligence, IEEE Tr},
title = {{Representation learning: a review and new perspectives.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23787338},
volume = {35},
year = {2013}
}
@article{Leonardis2007,
author = {Leonardis, Ale{\v{s}} and Fidler, Sanja},
journal = {Computer Vision and Pattern Recognition, IEEE},
mendeley-groups = {mood},
pages = {1--8},
title = {{Towards scalable representations of object categories: Learning a hierarchy of parts}},
year = {2007}
}
@inproceedings{Donaldson2009,
author = {Donaldson, Justin and Lamere, Paul},
booktitle = {Proceedings of the International Conference on Music Information Retrieval (ISMIR)},
mendeley-groups = {mood},
pages = {Tutorial},
title = {{Using Visualizations for Music Discovery}},
year = {2009}
}
@inproceedings{Peeters2013,
author = {Peeters, Geoffroy and Pauwels, Johan},
booktitle = {Proceedings of International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
mendeley-groups = {mood},
pages = {749--753},
title = {{Evaluating Automatically Estimated Chord Sequences}},
year = {2013}
}
@inproceedings{Mauch2008a,
address = {Sapporo},
author = {Mauch, Matthias and M{\"{u}}llensiefen, Daniel and Dixon, Simon and Wiggins, Geraint},
booktitle = {Proceedings of International Conference of Music Perception and Cognition},
mendeley-groups = {mood},
title = {{Can Statistical Language Models be Used for the Analysis of Harmonic Progressions?}},
year = {2008}
}
@article{Mauch2010a,
author = {Mauch, Matthias and Dixon, Simon},
journal = {IEEE Transactions on Audio, Speech, and Language Processing},
mendeley-groups = {mood},
number = {6},
pages = {1280--1289},
title = {{Simultaneous Estimation of Chords and Musical Context From Audio}},
volume = {18},
year = {2010}
}
@inproceedings{Mauch2007,
address = {Vienna},
author = {Mauch, Matthias and Dixon, Simon and Harte, Christopher},
booktitle = {Proceedings of the International Conference on Music Information Retrieval (ISMIR)},
mendeley-groups = {mood},
title = {{Discovering Chord Idioms Through Beatles and Real Book Songs}},
year = {2007}
}
@inproceedings{barthet2013design,
author = {Barthet, Mathieu and Marston, David and Baume, Chris and Fazekas, Gy{\"{o}}rgy and Sandler, Mark},
booktitle = {Proc. International Society for Music Information Retrieval Conference},
mendeley-groups = {mood},
title = {{Design and Evaluation of Semantic Mood Models for Music Recommendation}},
year = {2013}
}
@article{Lewis2013,
author = {Lewis, James R},
journal = {Interacting with computers},
mendeley-groups = {mood},
number = {4},
pages = {320--324},
title = {{Critical Review of ‘The Usability Metric for User Experience'}},
volume = {25},
year = {2013}
}
@article{Ryynanen2008,
author = {Ryyn{\"{a}}nen, Matti P and Klapuri, Anssi P},
doi = {10.1162/comj.2008.32.3.72},
issn = {0148-9267},
journal = {Computer Music Journal},
mendeley-groups = {mood},
month = {sep},
number = {3},
pages = {72--86},
publisher = {MIT Press 238 Main St., Suite 500, Cambridge, MA 02142-1046USA journals-info@mit.edu},
title = {{Automatic Transcription of Melody, Bass Line, and Chords in Polyphonic Music}},
url = {http://www.mitpressjournals.org/doi/abs/10.1162/comj.2008.32.3.72?journalCode=comj},
volume = {32},
year = {2008}
}
@incollection{dalgleish1999handbook,
author = {Dalgleish, Tim and Power, Michael J},
booktitle = {Handbook of cognition and emotion},
mendeley-groups = {mood},
publisher = {Wiley Online Library},
title = {{Basic Emotions}},
year = {1999}
}
@article{Maher1990,
author = {Maher, Robert C},
journal = {Journal of the Audio Engineering Society},
mendeley-groups = {mood},
month = {dec},
number = {12},
pages = {956--979},
publisher = {Audio Engineering Society},
title = {{Evaluation of a Method for Separating Digitized Duet Signals}},
url = {http://www.aes.org/e-lib/browse.cfm?elib=6001},
volume = {38},
year = {1990}
}
@article{schuller2010mister,
author = {Schuller, Bj{\"{o}}rn and Hage, Clemens and Schuller, Dagmar and Rigoll, Gerhard},
journal = {Journal of New Music Research},
mendeley-groups = {mood},
number = {1},
pages = {13--34},
publisher = {Taylor {\&} Francis},
title = {{‘Mister DJ, Cheer Me Up!': Musical and textual features for automatic mood classification}},
volume = {39},
year = {2010}
}
@inproceedings{Humphrey2012,
address = {Porto},
author = {Humphrey, Eric J and Bello, Juan P and LeCun, Yann},
booktitle = {Proceedings of the International Conference on Music Information Retrieval (ISMIR)},
mendeley-groups = {mood},
title = {{Moving beyond feature design: deep architectures and automatic feature learning in music informatics}},
year = {2012}
}
@inproceedings{Pikrakis2013,
author = {Pikrakis, Aggelos},
booktitle = {6th International Workshop on Machine Learning and Music, held in conjunction with the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases, ECML/PKDD 2013},
mendeley-groups = {mood},
pages = {1--4},
title = {{A Deep Learning Approach to Rhythm Modelling with Applications}},
year = {2013}
}
@book{juslin2001music,
author = {Juslin, Patrik N and Sloboda, John A},
mendeley-groups = {mood},
publisher = {Oxford University Press},
title = {{Music and emotion: Theory and research.}},
year = {2001}
}
@article{Levitin2005,
author = {Levitin, Daniel J and Rogers, Susan E},
journal = {TRENDS in Cognitive Sciences},
mendeley-groups = {mood},
number = {1},
pages = {26--33},
title = {{Absolute pitch: perception, coding and controversies}},
volume = {9},
year = {2005}
}
@inproceedings{Schmidt2011,
abstract = {The medium of music has evolved specifically for the expression of emotions, and it is natural for us to organize music in terms of its emotional associations. But while such organization is a natural process for humans, quantifying it empirically proves to be a very difficult task, and as such no dominant feature representation for music emotion recognition has yet emerged. Much of the difficulty in developing emotion-based features is the ambiguity of the ground-truth. Even using the smallest time window, opinions on the emotion are bound to vary and reflect some disagreement between listeners. In previous work, we have modeled human response labels to music in the arousal-valence (A-V) representation of affect as a time-varying, stochastic distribution. Current methods for automatic detection of emotion in music seek performance increases by combining several feature domains (e.g. loudness, timbre, harmony, rhythm). Such work has focused largely in dimensionality reduction for minor classification performance gains, but has provided little insight into the relationship between audio and emotional associations. In this new work we seek to employ regression-based deep belief networks to learn features directly from magnitude spectra. While the system is applied to the specific problem of music emotion recognition, it could be easily applied to any regression-based audio feature learning problem.},
author = {Schmidt, Erik M and Kim, Youngmoo E},
booktitle = {2011 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)},
doi = {10.1109/ASPAA.2011.6082328},
isbn = {978-1-4577-0693-6},
issn = {1931-1168},
mendeley-groups = {mood},
pages = {65--68},
publisher = {IEEE},
shorttitle = {Applications of Signal Processing to Audio and Aco},
title = {{Learning emotion-based acoustic features with deep belief networks}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6082328},
year = {2011}
}
@techreport{Rosenblatt1957,
author = {Rosenblatt, Frank},
mendeley-groups = {mood},
pages = {Report 85----460----1},
title = {{The Perceptron - a perceiving and recognizing automaton}},
year = {1957}
}
@article{Laurier2009,
author = {Laurier, Cyril and Meyers, Owen and Serr{\`{a}}, Joan and Blech, Martin and Herrera, Perfecto and Serra, Xavier},
doi = {10.1007/s11042-009-0360-2},
issn = {1380-7501},
journal = {Multimedia Tools and Applications},
mendeley-groups = {mood},
number = {1},
pages = {161--184},
title = {{Indexing music by mood: design and integration of an automatic content-based annotator}},
url = {http://link.springer.com/10.1007/s11042-009-0360-2},
volume = {48},
year = {2009}
}
@article{Temperley2013,
abstract = {AbstractView full textDownload full textOriginal ArticleRelated articlesView all related articles


















var addthis{\_}config = {\{}
ui{\_}cobrand: "Taylor {\&} Francis Online",
services{\_}compact: "citeulike,netvibes,twitter,technorati,delicious,linkedin,facebook,stumbleupon,digg,google,more",
pubid: "ra-4dff56cd6bb1830b"
{\}};




More Sharing Services

var addthis{\_}config = {\{}"data{\_}track{\_}addressbar":true,"ui{\_}click":true{\}};



Add to shortlist









Link





Permalink



















http://dx.doi.org/10.1080/09298215.2013.839525














Download Citation






Recommend to:














A friend},
author = {Temperley, David and de Clercq, Trevor},
doi = {10.1080/09298215.2013.839525},
issn = {0929-8215},
journal = {Journal of New Music Research},
mendeley-groups = {mood},
month = {sep},
pages = {1},
publisher = {Routledge},
title = {{Statistical Analysis of Harmony and Melody in Rock Music}},
url = {http://www.tandfonline.com/doi/abs/10.1080/09298215.2013.839525},
year = {2013}
}
@article{Pertusa2012,
author = {Pertusa, Antonio and I{\~{n}}esta, Jos{\'{e}} M},
doi = {10.1186/1687-6180-2012-27},
issn = {1687-6180},
journal = {EURASIP Journal on Advances in Signal Processing},
mendeley-groups = {mood},
number = {1},
pages = {27},
title = {{Efficient methods for joint estimation of multiple fundamental frequencies in music signals}},
url = {http://asp.eurasipjournals.com/content/2012/1/27},
volume = {2012},
year = {2012}
}
@article{Grindlay2011,
abstract = {This paper presents a general probabilistic model for transcribing single-channel music recordings containing multiple polyphonic instrument sources. The system requires no prior knowledge of the instruments present in the mixture (other than the number), although it can benefit from information about instrument type if available. In contrast to many existing polyphonic transcription systems, our approach explicitly models the individual instruments and is thereby able to assign detected notes to their respective sources. We use training instruments to learn a set of linear manifolds in model parameter space which are then used during transcription to constrain the properties of models fit to the target mixture. This leads to a hierarchical mixture-of-subspaces design which makes it possible to supply the system with prior knowledge at different levels of abstraction. The proposed technique is evaluated on both recorded and synthesized mixtures containing two, three, four, and five instruments each. We compare our approach in terms of transcription with (i.e., detected pitches must be associated with the correct instrument) and without source-assignment to another multi-instrument transcription system as well as a baseline non-negative matrix factorization (NMF) algorithm. For two-instrument mixtures evaluated with source-assignment, we obtain average frame-level F-measures of up to 0.52 in the completely blind transcription setting (i.e., no prior knowledge of the instruments in the mixture) and up to 0.67 if we assume knowledge of the basic instrument types. For transcription without source assignment, these numbers rise to 0.76 and 0.83, respectively.},
author = {Grindlay, Graham and Ellis, Daniel P W},
doi = {10.1109/JSTSP.2011.2162395},
issn = {1932-4553},
journal = {IEEE Journal of Selected Topics in Signal Processing},
keywords = {Aerospace electronics,Eigeninstruments,Hidden Markov models,Instruments,Matrix decomposition,Probabilistic logic,Signal processing algorithms,Training,audio signal processing,eigenvalues and eigenfunctions,frame-level F-measures,hierarchical eigeninstruments,hierarchical mixture-of-subspaces design,multiinstrument polyphonic music,multiple polyphonic instrument sources,music,non-negative matrix factorization (NMF),nonnegative matrix factorization algorithm,polyphonic transcription,probabilistic model,probability,single-channel music recordings,subspace,transcription},
mendeley-groups = {mood},
number = {6},
pages = {1159--1169},
shorttitle = {Selected Topics in Signal Processing, IEEE Journal},
title = {{Transcribing Multi-Instrument Polyphonic Music With Hierarchical Eigeninstruments}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5957256},
volume = {5},
year = {2011}
}
@inproceedings{Sheh2003a,
address = {Baltimore},
author = {Sheh, Alexander and Ellis, Daniel},
booktitle = {Proceedings of the International Conference on Music Information Retrieval (ISMIR)},
mendeley-groups = {mood},
pages = {183--189},
title = {{Chord segmentation and recognition using em-trained HMM}},
year = {2003}
}
@inproceedings{Pesek2014b,
author = {Pesek, Matev{\v{z}} and Godec, Primo{\v{z}} and Poredo{\v{s}}, Mojca and Strle, Gregor and Guna, Jo{\v{z}}e and Stojmenova, Emilija and Poga{\v{c}}nik, Matev{\v{z}} and Marolt, Matija},
booktitle = {Management Information Systems in Multimedia Art, Education, Entertainment, and Culture (MIS-MEDIA), IEEE Internation Conference on Multimedia {\&} Expo (ICME)},
mendeley-groups = {mood},
pages = {1--4},
title = {{Capturing the Mood: Evaluation of the MoodStripe and MoodGraph Interfaces}},
year = {2014}
}
@article{Holzapfel2012,
abstract = {In this paper, we propose a method that can identify challenging music samples for beat tracking without ground truth. Our method, motivated by the machine learning method “selective sampling,” is based on the measurement of mutual agreement between beat sequences. In calculating this mutual agreement we show the critical influence of different evaluation measures. Using our approach we demonstrate how to compile a new evaluation dataset comprised of difficult excerpts for beat tracking and examine this difficulty in the context of perceptual and musical properties. Based on tag analysis we indicate the musical properties where future advances in beat tracking research would be most profitable and where beat tracking is too difficult to be attempted. Finally, we demonstrate how our mutual agreement method can be used to improve beat tracking accuracy on large music collections.},
author = {Holzapfel, A and Davies, M E P and Zapata, J R and Oliveira, J L and Gouyon, F},
doi = {10.1109/TASL.2012.2205244},
issn = {1558-7916},
journal = {IEEE Transactions on Audio, Speech, and Language Processing},
keywords = {Accuracy,Beat tracking,Correlation,Electronic mail,Estimation,Europe,Histograms,Humans,beat sequences,beat tracking evaluation,evaluation,ground truth annotation,learning (artificial intelligence),machine learning method,music,music collection,music sample,musical properties,mutual agreement,selective sampling,signal sampling,tag analysis},
mendeley-groups = {mood},
month = {nov},
number = {9},
pages = {2539--2548},
shorttitle = {Audio, Speech, and Language Processing, IEEE Trans},
title = {{Selective Sampling for Beat Tracking Evaluation}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6220849},
volume = {20},
year = {2012}
}
@article{Ekman1992,
author = {Ekman, Paul},
journal = {Cognition and Emotion},
mendeley-groups = {mood},
pages = {169--200},
title = {{An argument for basic emotions}},
volume = {6},
year = {1992}
}
@inproceedings{Woolhouse2006,
address = {Bologna},
author = {Woolhouse, Matthew and Cross, Ian and Horton, Timothy},
booktitle = {Proceedings of International Conference on Music Perception and Cognition},
mendeley-groups = {mood},
title = {{The perception of non-adjecent harmonic relations}},
year = {2006}
}
@book{Rosenblatt1962,
author = {Rosenblatt, Frank},
mendeley-groups = {mood},
pages = {616},
publisher = {Spartan Books},
title = {{Principles of neurodynamics: perceptrons and the theory of brain mechanisms}},
url = {http://books.google.ca/books/about/Principles{\_}of{\_}neurodynamics.html?id=7FhRAAAAMAAJ{\&}pgis=1},
year = {1962}
}
@article{Tzanetakis2002,
abstract = {Musical genres are categorical labels created by humans to characterize pieces of music. A musical genre is characterized by the common characteristics shared by its members. These characteristics typically are related to the instrumentation, rhythmic structure, and harmonic content of the music. Genre hierarchies are commonly used to structure the large collections of music available on the Web. Currently musical genre annotation is performed manually. Automatic musical genre classification can assist or replace the human user in this process and would be a valuable addition to music information retrieval systems. In addition, automatic musical genre classification provides a framework for developing and evaluating features for any type of content-based analysis of musical signals. In this paper, the automatic classification of audio signals into an hierarchy of musical genres is explored. More specifically, three feature sets for representing timbral texture, rhythmic content and pitch content are proposed. The performance and relative importance of the proposed features is investigated by training statistical pattern recognition classifiers using real-world audio collections. Both whole file and real-time frame-based classification schemes are described. Using the proposed feature sets, classification of 61{\%} for ten musical genres is achieved. This result is comparable to results reported for human musical genre classification.},
author = {Tzanetakis, G and Cook, P},
doi = {10.1109/TSA.2002.800560},
issn = {1063-6676},
journal = {IEEE Transactions on Speech and Audio Processing},
keywords = {Computer science,Cultural differences,Feature extraction,Humans,Instruments,Multiple signal classification,Music information retrieval,Pattern recognition,Signal analysis,Wavelet analysis,World Wide Web,audio signal processing,audio signals,automatic musical genre classification,content-based analysis,feature sets,genre hierarchies,harmonic content,human musical genre classification,information retrieval,instrumentation,music,music information retrieval systems,musical genre annotation,musical signals,pitch content,real-time frame-based classification,rhythmic content,rhythmic structure,signal classification,statistical analysis,statistical pattern recognition classifiers traini,timbral texture,whole file classification},
mendeley-groups = {mood},
number = {5},
pages = {293--302},
shorttitle = {Speech and Audio Processing, IEEE Transactions on},
title = {{Musical genre classification of audio signals}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1021072},
volume = {10},
year = {2002}
}
@inproceedings{mauch:adt:2013,
author = {Mauch, Matthias and Ewert, Sebastian},
booktitle = {Proceedings of the 14th International Society for Music Information Retrieval Conference (ISMIR 2013)},
mendeley-groups = {mood},
pages = {83--88},
title = {{The {\{}A{\}}udio {\{}D{\}}egradation {\{}T{\}}oolbox and its Application to Robustness Evaluation}},
year = {2013}
}
@inproceedings{Woelfer2012,
address = {Porto},
author = {Woelfer, Jill Palzkill},
booktitle = {Proceedings of the International Conference on Music Information Retrieval (ISMIR)},
mendeley-groups = {mood},
pages = {367--372},
title = {{The role of music in the lives of homeless young people}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary;jsessionid=DEC4A9DBC031BFBCD2AF7B1FD2550839?doi=10.1.1.294.892},
year = {2012}
}
@article{McCulloch1943,
author = {McCulloch, Warren S and Pitts, Walter},
doi = {10.1007/BF02478259},
issn = {0007-4985},
journal = {The Bulletin of Mathematical Biophysics},
mendeley-groups = {mood},
month = {dec},
number = {4},
pages = {115--133},
title = {{A logical calculus of the ideas immanent in nervous activity}},
url = {http://link.springer.com/10.1007/BF02478259},
volume = {5},
year = {1943}
}
@article{Manfreda2006,
author = {Manfreda, Katja Lozar and Batagelj, Zenel and Vehovar, Vasja},
doi = {10.1111/j.1083-6101.2002.tb00149.x},
issn = {10836101},
journal = {Journal of Computer-Mediated Communication},
mendeley-groups = {mood},
number = {3},
title = {{Design of Web Survey Questionnaires: Three Basic Experiments}},
url = {http://doi.wiley.com/10.1111/j.1083-6101.2002.tb00149.x},
volume = {7},
year = {2006}
}
@inproceedings{Schluter2013,
author = {Schluter, Jan and Bock, Sebastian},
booktitle = {6th International Workshop on Machine Learning and Music, held in conjunction with the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases, ECML/PKDD 2013},
mendeley-groups = {mood},
title = {{Musical Onset Detection with Convolutional Neural Networks}},
year = {2013}
}
@article{Balaguer-Ballester2009,
author = {Balaguer-Ballester, Emili and Clark, Nicolas R and Coath, Martin and Krumbholz, Katrin and Denham, Susan L},
journal = {PLoS Computational Biology},
mendeley-groups = {mood},
number = {3},
pages = {1--15},
title = {{Understanding Pitch Perception as a Hierarchical Process with Top-Down Modulation}},
volume = {4},
year = {2009}
}
@article{Schedl2013,
author = {Schedl, Markus and Flexer, Arthur and Urbano, Juli{\'{a}}n},
doi = {10.1007/s10844-013-0247-6},
issn = {0925-9902},
journal = {Journal of Intelligent Information Systems},
mendeley-groups = {mood},
number = {3},
pages = {523--539},
title = {{The neglected user in music information retrieval research}},
url = {http://link.springer.com/10.1007/s10844-013-0247-6},
volume = {41},
year = {2013}
}
@book{Klapuri2006,
address = {New York},
editor = {Klapuri, Anssi and Davy, Manuel},
keywords = {Image and Speech Processing,Pattern Recognition,Signal,Signal Processing Methods for Music Transcription},
mendeley-groups = {mood},
pages = {440},
publisher = {Springer},
title = {{Signal Processing Methods for Music Transcription}},
url = {http://www.springer.com/engineering/signals/book/978-0-387-30667-4},
year = {2006}
}
@inproceedings{Tkalcic2003,
abstract = {In this paper, we present and overview of colour spaces used in electrical engineering and image processing. We stress the importance of the perceptual, historical and applicational background that led to a colour space. The colour spaces presented are: RGB, opponent-colour spaces, phenomenal colour spaces, CMY, CMYK, TV colour spaces (YUV and YIQ), PhotoYCC, CIE XYZ, Lab and Luv colour spaces.},
author = {Tkalcic, M and Tasic, J F},
booktitle = {The IEEE Region 8 EUROCON 2003. Computer as a Tool.},
doi = {10.1109/EURCON.2003.1248032},
isbn = {0-7803-7763-X},
mendeley-groups = {mood},
pages = {304--308},
publisher = {IEEE},
shorttitle = {EUROCON 2003. Computer as a Tool. The IEEE Region},
title = {{Colour spaces: perceptual, historical and applicational background}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1248032},
volume = {1},
year = {2003}
}
@article{SJ2009,
author = {Morrison, Stephen J and Demorest, Stephen M},
journal = {Progress in brain research},
mendeley-groups = {mood},
number = {178},
pages = {67--77},
title = {{Cultural constraints on music perception and cognition}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19874962},
year = {2009}
}
@inproceedings{Wu2013,
author = {Wu, B and Wun, S and Lee, C and Horner, A},
booktitle = {Proceedings of the International Conference on Music Information Retrieval (ISMIR)},
mendeley-groups = {mood},
pages = {415--421},
title = {{Spectral correlates in emotion labeling of sustained musical instrument tones}},
year = {2013}
}
@inproceedings{AndrewJ.2010,
address = {Seattle},
author = {Milne, Andrew J},
booktitle = {Proceedings of International Conference of Music Perception and Cognition},
mendeley-groups = {mood},
title = {{Tonal music theory: A psychoacoustic explanation?}},
year = {2010}
}
@article{McColl2001,
author = {McColl, E and Jacoby, A and Thomas, L and Soutter, J and Bamford, C and Steen, N and Thomas, R and Harvey, E and Garratt, A and Bond, J},
issn = {1366-5278},
journal = {Health technology assessment (Winchester, England)},
keywords = {Benchmarking,Bibliographic,Data Collection,Data Collection: methods,Databases,Great Britain,Health Care Surveys,Health Care Surveys: methods,Health Personnel,Questionnaires,Research Design},
mendeley-groups = {mood},
number = {31},
pages = {1--256},
pmid = {11809125},
title = {{Design and use of questionnaires: a review of best practice applicable to surveys of health service staff and patients.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/11809125},
volume = {5},
year = {2001}
}
@inproceedings{Weninger2013,
address = {Vancouver},
author = {Weninger, Felix and Kirst, Christian and Schuller, Bjorn and Bungartz, Hans-Joachim},
booktitle = {Proceedings of International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
mendeley-groups = {mood},
pages = {6--10},
title = {{A discriminative approach to polyphonic piano note transcription using supervised non-negative matrix factorization}},
year = {2013}
}
@inproceedings{Boulanger-Lewandowski2013,
author = {Boulanger-Lewandowski, Nicolas and Bengio, Yoshua and Vincent, Pascal},
booktitle = {2013 IEEE International Conference on Acoustics, Speech and Signal Processing},
doi = {10.1109/ICASSP.2013.6638244},
isbn = {978-1-4799-0356-6},
issn = {1520-6149},
keywords = {Accuracy,Hidden Markov models,Noise,Recurrent neural networks,Sequence transduction,Smoothing methods,Training,Vectors,acoustic transducers,audio signal processing,error statistics,global distribution mode,high dimensional output sequence,high dimensional sequence transduction,music,musically plausible transcription,polyphonic audio music,polyphonic transcription,probabilistic model,probability,realistic output distribution,recurrent neural nets,recurrent neural network,restricted Boltzmann machine,symbolic notation,test error rate},
mendeley-groups = {mood},
pages = {3178--3182},
publisher = {IEEE},
title = {{High-dimensional sequence transduction}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=6638244},
year = {2013}
}
@article{Clarkson1996,
author = {Clarkson, Marsha G and Martin, Rhonda L and Miciek, Sheridan G},
journal = {Infant behavior and development},
mendeley-groups = {mood},
number = {2},
pages = {191--197},
title = {{Infants' Perception of Pitch: Number of Harmonics}},
volume = {19},
year = {1996}
}
@inproceedings{schmidt2010prediction,
author = {Schmidt, Erik M and Kim, Youngmoo E},
booktitle = {ISMIR},
mendeley-groups = {mood},
pages = {465--470},
title = {{Prediction of Time-varying Musical Mood Distributions from Audio.}},
year = {2010}
}
@article{Bengio2009,
author = {Bengio, Yoshua},
journal = {Foundations and Trends{\textregistered} in Machine Learning},
mendeley-groups = {mood},
number = {1},
pages = {1--127},
publisher = {Foundations and Trends in Machine Learning},
title = {{Learning Deep Architectures for AI}},
volume = {2},
year = {2009}
}
@inproceedings{Noland2006,
address = {Victoria},
author = {Noland, Katy and Sandler, Mark},
booktitle = {Proceedings of the International Conference on Music Information Retrieval (ISMIR)},
mendeley-groups = {mood},
title = {{Key Estimation Using a Hidden Markov Model}},
year = {2006}
}
@inproceedings{Hamel2010,
author = {Hamel, Philippe and Eck, Douglas},
booktitle = {Proceedings of the International Conference on Music Information Retrieval (ISMIR)},
mendeley-groups = {mood},
pages = {339--344},
title = {{Learning Features from Music Audio with Deep Belief Networks}},
year = {2010}
}
@article{Cortes1995,
author = {Cortes, Corinna and Vapnik, Vladimir},
doi = {10.1023/A:1022627411411},
issn = {1573-0565},
journal = {Machine Learning},
mendeley-groups = {mood},
month = {sep},
number = {3},
pages = {273--297},
publisher = {Kluwer Academic Publishers-Plenum Publishers},
title = {{Support-Vector Networks}},
url = {http://link.springer.com/article/10.1023/A:1022627411411},
volume = {20},
year = {1995}
}
@article{DavidA.2004,
author = {{David A.}, Schwartz and Purves, Dale},
journal = {Hearing Research},
mendeley-groups = {mood},
number = {1-2},
pages = {31--46},
title = {{Pitch is determined by naturally occuring periodic sounds}},
year = {2004}
}
@inproceedings{Hochenbaum2009,
author = {Hochenbaum, Jordan and Vallis, Owen},
booktitle = {Berlin Open Conference},
mendeley-groups = {mood},
title = {{Bricktable: A Musical Tangible Multi-Touch Interface}},
year = {2009}
}
@article{Poliner2007,
author = {Poliner, Graham E and Ellis, Daniel P W and Ehmann, Andreas F and Gomez, Emilia and Streich, Sebastian and Ong, Beesuan},
doi = {10.1109/TASL.2006.889797},
issn = {1558-7916},
journal = {IEEE Transactions on Audio, Speech and Language Processing},
mendeley-groups = {mood},
number = {4},
pages = {1247--1256},
publisher = {IEEE},
title = {{Melody Transcription From Music Audio: Approaches and Evaluation}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=4156215},
volume = {15},
year = {2007}
}
@book{Fastl2007,
author = {Fastl, Hugo and Zwicker, Eberhard},
mendeley-groups = {mood},
pages = {416},
title = {{Psychoacoustics: Facts and models}},
year = {2007}
}
@article{Rabiner1989,
author = {Rabiner, Lawrence R},
journal = {Proceedings of the IEEE},
mendeley-groups = {mood},
number = {2},
pages = {257--286},
title = {{A tutorial on hidden Markov models and selected applicationsin speech recognition}},
volume = {77},
year = {1989}
}
@article{Orio2006,
author = {Orio, Nicola},
journal = {Foundations and Trends{\textregistered} in Information Retrieval},
mendeley-groups = {mood},
number = {1},
pages = {1--90},
title = {{Music Retrieval: A Tutorial and Review}},
volume = {1},
year = {2006}
}
@article{Ellis2006,
author = {Ellis, Daniel P W and Poliner, Graham E},
doi = {10.1007/s10994-006-8373-9},
issn = {0885-6125},
journal = {Machine Learning},
mendeley-groups = {mood},
number = {2-3},
pages = {439--456},
title = {{Classification-based melody transcription}},
url = {http://link.springer.com/10.1007/s10994-006-8373-9},
volume = {65},
year = {2006}
}
@article{Kumar2013,
abstract = {The physiological basis for musical hallucinations (MH) is not understood. One obstacle to understanding has been the lack of a method to manipulate the intensity of hallucination during the course of experiment. Residual inhibition, transient suppression of a phantom percept after the offset of a masking stimulus, has been used in the study of tinnitus. We report here a human subject whose MH were residually inhibited by short periods of music. Magnetoencephalography (MEG) allowed us to examine variation in the underlying oscillatory brain activity in different states. Source-space analysis capable of single-subject inference defined left-lateralised power increases, associated with stronger hallucinations, in the gamma band in left anterior superior temporal gyrus, and in the beta band in motor cortex and posteromedial cortex. The data indicate that these areas form a crucial network in the generation of MH, and are consistent with a model in which MH are generated by persistent reciprocal communication in a predictive coding hierarchy.},
author = {Kumar, Sukhbinder and Sedley, William and Barnes, Gareth R and Teki, Sundeep and Friston, Karl J and Griffiths, Timothy D},
journal = {Cortex},
keywords = {Auditory cortex,Beta oscillations,Gamma oscillations,Magnetoencephalography,Musical hallucinations,Predictive coding},
mendeley-groups = {mood},
title = {{A brain basis for musical hallucinations}},
url = {http://www.sciencedirect.com/science/article/pii/S0010945213003080},
year = {2013}
}
@article{Remmington2000,
abstract = {The circumplex model of affect has been among the most widely studied representations of affect. Despite the considerable evidence cited in support of it, methods typically used to evaluate the model have substantial limitations. In this article, the authors attempt to correct past limitations by using a covariance structure model specifically designed to assess circumplex structure. This model was fit to 47 individual correlation matrices from published data sets. Analyses revealed that model fit was typically acceptable and that opposing affective states usually demonstrated strong negative correlations with one another. However, analyses also indicated substantial variability in both model fit and correlations among opposing affective states and suggested several characteristics of studies that partially accounted for this variability. Detailed examination of the locations of affective states for 10 of the correlation matrices with relatively optimal characteristics provided mixed support for the model.},
author = {Remmington, N A and Fabrigar, L R and Visser, P S},
issn = {0022-3514},
journal = {Journal of personality and social psychology},
keywords = {Affect,Confounding Factors (Epidemiology),Data Interpretation,Humans,Models,Psychological,Statistical},
mendeley-groups = {mood},
number = {2},
pages = {286--300},
pmid = {10948981},
title = {{Reexamining the circumplex model of affect.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10948981},
volume = {79},
year = {2000}
}
@article{Valdez1994,
author = {Valdez, Patricia and Mehrabian, Albert},
journal = {Journal of Experimental Psychology: General},
mendeley-groups = {mood},
number = {4},
pages = {394--409},
title = {{Effects of Color on Emotions}},
volume = {123},
year = {1994}
}
@inproceedings{Bock2012,
address = {Porto},
author = {Bock, Sebastian and Krebs, Florian and Schedl, Markus},
booktitle = {Proceedings of the International Conference on Music Information Retrieval (ISMIR)},
mendeley-groups = {mood},
title = {{Evaluating the online capabilities of onset detection methods}},
year = {2012}
}
@inproceedings{Laurier2009a,
author = {Laurier, Cyril and Lartillot, Olivier and Eerola, Tuomas and Toiviainen, Petri},
booktitle = {Proceedings of the Conference of European Society for the Cognitive Sciences of Music (ESCOM)},
mendeley-groups = {mood},
title = {{Exploring Relationships between Audio Features and Emotion in Music}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.182.728},
year = {2009}
}
@article{Laskowski1979,
author = {Laskowski, Larry},
journal = {Journal of Music Theory},
mendeley-groups = {mood},
number = {2},
pages = {304--307},
title = {{Heinrich Schenker: An Annotated Index to His Analyses of Musical Works}},
volume = {23},
year = {1979}
}
@inproceedings{Pesek2014c,
address = {Taipei},
author = {Pesek, Matev{\v{z}} and Leonardis, Ale{\v{s}} and Marolt, Matija},
booktitle = {Proceedings of the International Conference on Music Information Retrieval (ISMIR)},
mendeley-groups = {mood},
title = {{A compositional hierarchical model for music information retrieval}},
year = {2014}
}
@inproceedings{schmidt2009projection,
author = {Schmidt, Erik M and Kim, Youngmoo E},
booktitle = {10th International Society for Music Information Retrieval Conference. ISMIR},
mendeley-groups = {mood},
title = {{Projection of acoustic features to continuous valence-arousal mood labels via regression}},
year = {2009}
}
@article{Kral1996,
author = {Kral, A and Majernik, V},
journal = {General Physiology and Biophysics},
mendeley-groups = {mood},
number = {15},
pages = {109--127},
title = {{On lateral inhibition in the auditory system}},
volume = {I},
year = {1996}
}
@inproceedings{schmidt2011modeling,
author = {Schmidt, Erik M and Kim, Youngmoo E},
booktitle = {Proceedings of the International Conference on Music Information Retrieval (ISMIR)},
mendeley-groups = {mood},
pages = {777--782},
title = {{Modeling Musical Emotion Dynamics with Conditional Random Fields.}},
year = {2011}
}
@inproceedings{Farbook2010,
address = {Seattle},
author = {Farbood, Morwaread},
booktitle = {Proceedings of International Conference of Music Perception and Cognition},
mendeley-groups = {mood},
title = {{Working memory and the perception of hierarchical tonal structures}},
year = {2010}
}
@incollection{ben2010user,
author = {Ben-Hur, Asa and Weston, Jason},
booktitle = {Data mining techniques for the life sciences},
mendeley-groups = {mood},
pages = {223--239},
publisher = {Springer},
title = {{A user's guide to support vector machines}},
year = {2010}
}
@inproceedings{Bello2005,
address = {London},
author = {Bello, Juan P and Pickens, Jeremy},
booktitle = {Proceedings of the International Conference on Music Information Retrieval (ISMIR)},
mendeley-groups = {mood},
pages = {304--311},
title = {{A robust mid-level representation for harmonic content in music signals}},
year = {2005}
}
@article{Eerola2010,
abstract = {The primary aim of the present study was to systematically compare perceived emotions in music using two different theoretical frameworks: the discrete emotion model, and the dimensional model of affect. A secondary aim was to introduce a new, improved set of stimuli for the study of music-mediated emotions. A large pilot study established a set of 110 film music excerpts, half were moderately and highly representative examples of five discrete emotions (anger, fear, sadness, happiness and tenderness), and the other half moderate and high examples of the six extremes of three bipolar dimensions (valence, energy arousal and tension arousal). These excerpts were rated in a listening experiment by 116 non-musicians. All target emotions of highly representative examples in both conceptual sets were discriminated by self-ratings. Linear mapping techniques between the discrete and dimensional models revealed a high correspondence along two central dimensions that can be labelled as valence and arousal, and the three dimensions could be reduced to two without significantly reducing the goodness of fit. The major difference between the discrete and categorical models concerned the poorer resolution of the discrete model in characterizing emotionally ambiguous examples. The study offers systematically structured and rich stimulus material for exploring emotional processing.},
author = {Eerola, T and Vuoskoski, J K},
doi = {10.1177/0305735610362821},
issn = {0305-7356},
journal = {Psychology of Music},
mendeley-groups = {mood},
number = {1},
pages = {18--49},
title = {{A comparison of the discrete and dimensional models of emotion in music}},
url = {http://pom.sagepub.com/content/39/1/18.abstract},
volume = {39},
year = {2010}
}
@inproceedings{Wu2010,
address = {Barcelona},
author = {Wu, Ho-Hsiang and Bello, Juan P},
booktitle = {Proceedings of the International Conference on Music Information Retrieval (ISMIR)},
mendeley-groups = {mood},
title = {{Audio-based music visualization for music structure analysis}},
year = {2010}
}
@inproceedings{Mauch2010,
address = {Utrecht},
author = {Mauch, Matthias and Dixon, Simon},
booktitle = {Proceedings of the International Conference on Music Information Retrieval (ISMIR)},
mendeley-groups = {mood},
title = {{Approximate Note Transcription For The Improved Identification Of Difficult Chords}},
year = {2010}
}
@inproceedings{XavierGlorot2011,
author = {{Xavier Glorot}, Antoine Bordes Yoshua Bengio},
booktitle = {ICML},
mendeley-groups = {mood},
pages = {513--520},
title = {{Domain adaptation for large-scale sentiment classification: A deep learning approach}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.231.3442},
volume = {513-520},
year = {2011}
}
@article{Guna2014,
abstract = {We present the results of an evaluation of the performance of the Leap Motion Controller with the aid of a professional, high-precision, fast motion tracking system. A set of static and dynamic measurements was performed with different numbers of tracking objects and configurations. For the static measurements, a plastic arm model simulating a human arm was used. A set of 37 reference locations was selected to cover the controller's sensory space. For the dynamic measurements, a special V-shaped tool, consisting of two tracking objects maintaining a constant distance between them, was created to simulate two human fingers. In the static scenario, the standard deviation was less than 0.5 mm. The linear correlation revealed a significant increase in the standard deviation when moving away from the controller. The results of the dynamic scenario revealed the inconsistent performance of the controller, with a significant drop in accuracy for samples taken more than 250 mm above the controller's surface. The Leap Motion Controller undoubtedly represents a revolutionary input device for gesture-based human-computer interaction; however, due to its rather limited sensory space and inconsistent sampling frequency, in its current configuration it cannot currently be used as a professional tracking system.},
author = {Guna, Jo{\v{z}}e and Jakus, Grega and Poga{\v{c}}nik, Matev{\v{z}} and Toma{\v{z}}i{\v{c}}, Sa{\v{s}}o and Sodnik, Jaka},
doi = {10.3390/s140203702},
issn = {1424-8220},
journal = {Sensors (Basel, Switzerland)},
mendeley-groups = {mood},
number = {2},
pages = {3702--3720},
pmid = {24566635},
publisher = {Multidisciplinary Digital Publishing Institute},
title = {{An analysis of the precision and reliability of the leap motion sensor and its suitability for static and dynamic tracking.}},
url = {http://www.mdpi.com/1424-8220/14/2/3702},
volume = {14},
year = {2014}
}
@article{Tolonen2000,
author = {Tolonen, Tero and Karjalainen, Matti},
journal = {IEEE Transactions on Speech and Audio Processing},
mendeley-groups = {mood},
number = {6},
pages = {708--716},
title = {{A computationally Efficient Multipitch Analysis Model}},
volume = {8},
year = {2000}
}
@article{Hassenzahl2003,
author = {Hassenzahl, M and Burmester, M and Koller, F},
journal = {Mensch {\&} Computer},
mendeley-groups = {mood},
title = {{AttrakDiff: A questionnaire to measure perceived hedonic and pragmatic quality}},
url = {http://scholar.google.si/scholar?hl=en{\&}q=attrakdiff{\&}btnG={\&}as{\_}sdt=1,5{\&}as{\_}sdtp={\#}1},
year = {2003}
}
@article{Colibazzi2010,
abstract = {The circumplex model of affect construes all emotions as linear combinations of 2 independent neurophysiological dimensions, valence and arousal. We used functional magnetic resonance imaging to identify the neural networks subserving valence and arousal, and we assessed, in 10 participants, the associations of the BOLD (blood oxygen level-dependent) response, an indirect index of neural activity, with ratings of valence and arousal during the emotional experiences induced by the presentation of evocative sentences. Unpleasant emotional experience was associated with increased BOLD signal intensities in the supplementary motor, anterior midcingulate, right dorsolateral prefrontal, occipito-temporal, inferior parietal, and cerebellar cortices. Highly arousing emotions were associated with increased BOLD signal intensities in the left thalamus, globus pallidus, caudate, parahippocampal gyrus, amygdala, premotor cortex, and cerebellar vermis. Separate analyses using a finite impulse response model confirmed these results and revealed that pleasant emotions engaged an additional network that included the midbrain, ventral striatum, and caudate nucleus, all portions of a reward circuit. These findings suggest the existence of distinct networks subserving the valence and arousal dimensions of emotions, with midline and medial temporal lobe structures mediating arousal and dorsal cortical areas and mesolimbic pathways mediating valence.},
author = {Colibazzi, Tiziano and Posner, Jonathan and Wang, Zhishun and Gorman, Daniel and Gerber, Andrew and Yu, Shan and Zhu, Hongtu and Kangarlu, Alayar and Duan, Yunsuo and Russell, James A and Peterson, Bradley S},
doi = {10.1037/a0018484},
issn = {1931-1516},
journal = {Emotion (Washington, D.C.)},
keywords = {Adult,Amygdala,Amygdala: physiology,Arousal,Arousal: physiology,Brain,Brain: physiology,Caudate Nucleus,Caudate Nucleus: physiology,Cerebellar Cortex,Cerebellar Cortex: physiology,Emotions,Emotions: physiology,Female,Globus Pallidus,Globus Pallidus: physiology,Humans,Magnetic Resonance Imaging,Male,Parahippocampal Gyrus,Parahippocampal Gyrus: physiology,Thalamus,Thalamus: physiology,Young Adult},
mendeley-groups = {mood},
number = {3},
pages = {377--389},
pmid = {20515226},
title = {{Neural systems subserving valence and arousal during the experience of induced emotions.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20515226},
volume = {10},
year = {2010}
}
@inproceedings{Pesek2013b,
address = {Ljubljana},
author = {Pesek, Matev{\v{z}} and Poredo{\v{s}}, Mojca and Guna, Jo{\v{z}}e and Stojmenova, Emilija and Marolt, Matija},
booktitle = {Proceedings of the 4th International Conference World Usability Day Slovenia 2013},
mendeley-groups = {mood},
pages = {53--55},
title = {{Mood-dependent visual representation of audio recordings for music recommendation}},
year = {2013}
}
@article{Smith2006,
author = {Smith, Evan C and Lewicki, Michael S},
journal = {Nature},
mendeley-groups = {mood},
number = {2},
pages = {978--982},
title = {{Efficient Auditory Coding}},
volume = {439},
year = {2006}
}
@article{Meredith2002,
abstract = {In previous approaches to repetition discovery in music, the music to be analysed has been represented using strings. However, there are certain types of interesting musical repetitions that cannot be discovered using string algorithms. We propose a geometric approach to repetition discovery in which the music is represented as a multidimensional dataset. Certain types of interesting musical repetition that cannot be found using string algorithms can efficiently be found using algorithms that process multidimensional datasets. Our approach allows polyphonic music to be analysed as efficiently as monophonic music and it can be used to discover polyphonic repeated patterns ?with gaps? in the timbre, dynamic and rhythmic structure of a passage as well as its pitch structure. We present two new algorithms: SIA and SIATEC. SIA computes all the maximal repeated patterns in a multidimensional dataset and SIATEC computes all the occurrences of all the maximal repeated patterns in a dataset. For a k -dimensional dataset of size n, the worstcase running time of SIA is O (kn 2 log 2 n) and the worst-case running time of SIATEC is O (kn 3). In previous approaches to repetition discovery in music, the music to be analysed has been represented using strings. However, there are certain types of interesting musical repetitions that cannot be discovered using string algorithms. We propose a geometric approach to repetition discovery in which the music is represented as a multidimensional dataset. Certain types of interesting musical repetition that cannot be found using string algorithms can efficiently be found using algorithms that process multidimensional datasets. Our approach allows polyphonic music to be analysed as efficiently as monophonic music and it can be used to discover polyphonic repeated patterns ?with gaps? in the timbre, dynamic and rhythmic structure of a passage as well as its pitch structure. We present two new algorithms: SIA and SIATEC. SIA computes all the maximal repeated patterns in a multidimensional dataset and SIATEC computes all the occurrences of all the maximal repeated patterns in a dataset. For a k -dimensional dataset of size n, the worstcase running time of SIA is O (kn 2 log 2 n) and the worst-case running time of SIATEC is O (kn 3).},
author = {Meredith, David and Lemstrom, Kjell and Wiggins, Geraint A},
doi = {10.1076/jnmr.31.4.321.14162},
issn = {0929-8215},
journal = {Journal of New Music Research},
mendeley-groups = {mood},
month = {dec},
number = {4},
pages = {321--345},
publisher = {Routledge},
title = {{Algorithms for discovering repeated patterns in multidimensional representations of polyphonic music}},
url = {http://www.tandfonline.com/doi/abs/10.1076/jnmr.31.4.321.14162},
volume = {31},
year = {2002}
}
@article{Peretz2003,
author = {Peretz, Isabelle and Coltheart, Max},
journal = {Nature Neuroscience},
mendeley-groups = {mood},
number = {7},
pages = {688--691},
title = {{Modularity of music processing}},
volume = {6},
year = {2003}
}
@article{panda2013multi,
author = {Panda, R and Malheiro, R and Rocha, B and Oliveira, A and Paiva, R P},
journal = {Proc. CMMR},
mendeley-groups = {mood},
title = {{Multi-Modal Music Emotion Recognition: A New Dataset, Methodology and Comparative Analysis}},
year = {2013}
}
@article{DiRusso2001,
abstract = {This study investigated the effect of attention on the contrast response curves of steady-state visual evoked potentials (VEPs) to counter-phased sinusoidal gratings. The 1 cyc/deg gratings were modulated either in luminance or chromaticity (equiluminant red-green). The luminance grating counter-phased at 9 Hz (to favour activation of the magno-cellular system), and the chromatic grating at 2.5 Hz (to favour activation of the parvo-cellular system). Attention was directed towards the gratings (displayed in the left visual field) by requiring subjects to detect and respond to randomly occurring changes in contrast. In the control condition, attention towards the grating was minimised by requiring subjects to detect a target letter amongst distracters briefly flashed in the contra-lateral visual field. Attention increased VEP amplitudes for both luminance and chromatic stimuli, more so at high than at low contrasts, increasing the slope of the contrast amplitude curves (over the non-saturating range of contrasts). The estimates of contrast threshold from extrapolation of amplitudes were unaffected by attention. Attention also changed the VEP phases, but only for luminance gratings, where it acted to reduce the magnitude of phase advance with contrast. Attention had no effect on the average phases for chromatic gratings. The results are consistent with the notion that attention acts on cortical gain control mechanisms, which are known to be different for the magno- and parvo-cellular systems.},
author = {{Di Russo}, Francesco and Spinelli, Donatella and Morrone, M.Concetta},
journal = {Vision Research},
keywords = {Attention,Automatic gain control contrast mechanisms,Visual evoked potentials},
mendeley-groups = {mood},
number = {19},
pages = {2435--2447},
title = {{Automatic gain control contrast mechanisms are modulated by attention in humans: evidence from visual evoked potentials}},
url = {http://www.sciencedirect.com/science/article/pii/S0042698901001341},
volume = {41},
year = {2001}
}
@article{Melara1989,
author = {Melara, Robert D},
journal = {Journal of Experimental Psychology: Human Perception and Performacne},
mendeley-groups = {mood},
number = {1},
pages = {69--79},
title = {{Dimensional Interaction Between Color and Pitch}},
volume = {15},
year = {1989}
}
@article{Barbancho2012,
author = {Barbancho, Ana M and Klapuri, Anssi and Tardon, Lorenzo J and Barbancho, Isabel},
doi = {10.1109/TASL.2011.2174227},
issn = {1558-7916},
journal = {IEEE Transactions on Audio, Speech, and Language Processing},
mendeley-groups = {mood},
number = {3},
pages = {915--921},
publisher = {IEEE},
title = {{Automatic Transcription of Guitar Chords and Fingering From Audio}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=6064873},
volume = {20},
year = {2012}
}
@article{Hart2006,
author = {Hart, S G},
doi = {10.1177/154193120605000909},
issn = {1071-1813},
journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
mendeley-groups = {mood},
number = {9},
pages = {904--908},
publisher = {SAGE Publications},
title = {{Nasa-Task Load Index (NASA-TLX); 20 Years Later}},
url = {http://pro.sagepub.com/content/50/9/904.abstract},
volume = {50},
year = {2006}
}
@inproceedings{Pesek2013,
author = {Pesek, Matev{\v{z}} and Marolt, Matija},
booktitle = {6th International Workshop on Machine Learning and Music, held in conjunction with the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases, ECML/PKDD 2013},
mendeley-groups = {mood},
title = {{Chord estimation using compositional hierarchical model}},
year = {2013}
}
@article{Klapuri2004,
abstract = {The aim of this overview is to describe methods for the automatic transcription of Western polyphonic music. The transcription task is here understood as transforming an acoustic musical signal into a MIDI-like symbolic representation. Only pitched musical instruments are considered: recognizing the sounds of drum instruments is not discussed. The main emphasis is laid on estimating the multiple fundamental frequencies of several concurrent sounds. Various approaches to solve this problem are discussed, including methods that are based on modelling the human auditory periphery, methods that mimic the human auditory scene analysis function, signal model-based Bayesian inference methods, and data-adaptive methods. Another subproblem addressed is the rhythmic parsing of acoustic musical signals. From the transcription point of view, this amounts to the temporal segmentation of music signals at different time scales. The relationship between the two subproblems and the general structure of the transcription problem is discussed.
The aim of this overview is to describe methods for the automatic transcription of Western polyphonic music. The transcription task is here understood as transforming an acoustic musical signal into a MIDI-like symbolic representation. Only pitched musical instruments are considered: recognizing the sounds of drum instruments is not discussed. The main emphasis is laid on estimating the multiple fundamental frequencies of several concurrent sounds. Various approaches to solve this problem are discussed, including methods that are based on modelling the human auditory periphery, methods that mimic the human auditory scene analysis function, signal model-based Bayesian inference methods, and data-adaptive methods. Another subproblem addressed is the rhythmic parsing of acoustic musical signals. From the transcription point of view, this amounts to the temporal segmentation of music signals at different time scales. The relationship between the two subproblems and the general structure of the transcription problem is discussed.},
author = {Klapuri, Anssi P},
doi = {10.1080/0929821042000317840},
issn = {0929-8215},
journal = {Journal of New Music Research},
mendeley-groups = {mood},
month = {sep},
number = {3},
pages = {269--282},
publisher = {Routledge},
title = {{Automatic Music Transcription as We Know it Today}},
url = {http://dx.doi.org/10.1080/0929821042000317840},
volume = {33},
year = {2004}
}
@article{DeCheveigne2002,
author = {de Cheveigne, Alain},
journal = {The Journal of Acoustical Society of America},
mendeley-groups = {mood},
number = {4},
pages = {1917--1930},
title = {{YIN, a fundamental frequency estimator for speech and music}},
volume = {111},
year = {2002}
}
@inproceedings{Pesek2014,
address = {Taipei},
author = {Pesek, Matev{\v{z}} and Godec, Primo{\v{z}}},
booktitle = {Proceedings of the International Conference on Music Information Retrieval (ISMIR)},
mendeley-groups = {mood},
title = {{INTRODUCING A DATASET OF EMOTIONAL AND COLOR RESPONSES TO MUSIC}},
year = {2014}
}
@inproceedings{Wankhammer2009,
address = {Como},
author = {Wankhammer, Alexander and Sciri, Peter and Sontacchi, Alois},
booktitle = {Proceedings of Conference on Digital Audio Effects},
mendeley-groups = {mood},
title = {{Chroma and MFCC based pattern recognition in audio files utilizing hidden Markov models and dynamic programming}},
year = {2009}
}
@inproceedings{Harte2005,
address = {London},
author = {Harte, Christopher and Sandler, Mark and Abdallah, Samer and Gomez, Emilia},
booktitle = {Proceedings of the International Conference on Music Information Retrieval (ISMIR)},
mendeley-groups = {mood},
title = {{Symbolic representation of musical chords: A proposed syntax for text annotations}},
year = {2005}
}
@inproceedings{Sheh2003,
address = {Baltimore},
author = {Sheh, Alexander and Ellis, Daniel},
booktitle = {Proceedings of the International Conference on Music Information Retrieval (ISMIR)},
mendeley-groups = {mood},
pages = {1--7},
title = {{Chord segmentation and recognition using EM-trained hidden Markov models}},
year = {2003}
}
@phdthesis{Maher1989,
author = {Maher, Robert Crawford},
mendeley-groups = {mood},
pages = {150},
title = {{An Approach for the Separation of Voices in Composite Musical Signals}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.68.8532},
year = {1989}
}
@inproceedings{Muller2011,
address = {Miami},
author = {M{\"{u}}ller, Meinard and Ewert, Sebastian},
booktitle = {Proceedings of the International Conference on Music Information Retrieval (ISMIR)},
mendeley-groups = {mood},
pages = {288--295},
title = {{Chroma Toolbox: MATLAB Implementations for Extracting Variants of Chroma-Based Audio Features}},
year = {2011}
}
@article{turnbull2008semantic,
author = {Turnbull, Douglas and Barrington, Luke and Torres, David and Lanckriet, Gert},
journal = {Audio, Speech, and Language Processing, IEEE Transactions on},
mendeley-groups = {mood},
number = {2},
pages = {467--476},
publisher = {IEEE},
title = {{Semantic annotation and retrieval of music and sound effects}},
volume = {16},
year = {2008}
}
@article{Papadopoulos2011,
author = {Papadopoulos, Helene and Peeters, Geoffroy},
journal = {IEEE Transactions on Audio, Speech, and Language Processing},
mendeley-groups = {mood},
number = {1},
pages = {138--152},
title = {{Joint Estimation of Chords and Downbeats From an Audio Signal}},
volume = {19},
year = {2011}
}
@book{Rabiner1975,
author = {Rabiner, Lawrence R and Gold, Bernard},
mendeley-groups = {mood},
pages = {762},
publisher = {Prentice Hall},
title = {{Theory and application of digital signal processing}},
year = {1975}
}
@article{abdi2007method,
author = {Abdi, Herv{\'{e}}},
journal = {Encyclopedia of Measurement and Statistics. CA, USA: Thousand Oaks},
mendeley-groups = {mood},
title = {{The method of least squares}},
year = {2007}
}
@inproceedings{Ni2012,
address = {Porto},
author = {Ni, Yizhao and McVicar, Matt and Santos-Rodriguez, Raul and Bie, Tijl De},
booktitle = {Proceedings of the International Conference on Music Information Retrieval (ISMIR)},
mendeley-groups = {mood},
pages = {109--114},
title = {{Using Hyper-genre Training to Explore Genre Information for Automatic Chord Estimation}},
year = {2012}
}
@inproceedings{Schmidt2013,
author = {Schmidt, Eric M and Kim, Youngmoo E},
booktitle = {Proceedings of the International Conference on Music Information Retrieval (ISMIR)},
mendeley-groups = {mood},
pages = {21--26},
title = {{Learning Rhythm and Melody Features with Deep Belief Networks}},
year = {2013}
}
@article{Gabrielsson2002,
abstract = {A distinction is made between emotion perception, that is, to perceive emotional expression in music without necessarily being affected oneself, and emotion induction, that is, listeners' emotional response to music. This distinction is not always observed, neither in everyday conversation about emotions, nor in scientific papers. Empirical studies of emotion perception are briefly reviewed with regard to listener agreement concerning expressed emotions, followed by a selective review of empirical studies on emotional response to music. Possible relationships between emotion perception and emotional response are discussed and exemplified: Positive relationship, negative relationship, no systematic relationship and no relationship. It is emphasised that both emotion perception and, especially, emotional response are dependent on an interplay between musical, personal, and situational factors. Some methodological questions and suggestions for further research are discussed.},
author = {Gabrielsson, Alf},
doi = {10.1177/10298649020050S105},
journal = {Musicae Scientiae},
mendeley-groups = {mood},
month = {sep},
number = {1{\_}suppl},
pages = {123--147},
title = {{Emotion Perceived and Emotion Felt: Same or Different?}},
url = {http://msx.sagepub.com/content/5/1{\_}suppl/123.short},
volume = {5},
year = {2002}
}
@book{Bregman1990,
author = {Bregman, Albert S},
mendeley-groups = {mood},
pages = {773},
publisher = {MIT Press},
title = {{Auditory scene analysis - The perceptual organization of sound}},
year = {1990}
}
@inproceedings{Song2012,
address = {London},
author = {Song, Y and Dixon, S and Pearce, M},
booktitle = {Proc. 9th Int. Symp. Computer Music Modelling and Retrieval (CMMR)},
mendeley-groups = {mood},
pages = {395--410},
title = {{A survey of music recommendation systems and future perspectives}},
year = {2012}
}
@article{Felleman1991,
author = {Felleman, Daniel J and {Van Essen}, David C},
journal = {Cerebral Cortex},
mendeley-groups = {mood},
number = {1},
pages = {1--47},
title = {{Distributed Hierarchical Processing in the Primate Cerebral Cortex}},
volume = {1},
year = {1991}
}
@inproceedings{Hinton1983,
author = {Hinton, Geoffrey E and Sejnowski, Terrence J},
booktitle = {Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (CVPR)},
mendeley-groups = {mood},
pages = {448--453},
title = {{Optimal Perceptual Inference}},
year = {1983}
}
@inproceedings{Lee2009,
author = {Lee, Honglak and Pham, Peter and Largman, Yan and Ng, Andrew Y},
booktitle = {Advances in Neural Information Processing Systems},
mendeley-groups = {mood},
pages = {1096--1104},
title = {{Unsupervised feature learning for audio classification using convolutional deep belief networks}},
url = {http://papers.nips.cc/paper/3674-unsupervised-feature-learning-for-audio-classification-using-convolutional-deep-belief-networks},
year = {2009}
}
@article{Oudre2011,
author = {Oudre, Laurent and Grenier, Yves and Fevotte, Cedric},
journal = {IEEE Transactions on Audio, Speech, and Language Processing},
mendeley-groups = {mood},
number = {7},
pages = {2222--2233},
title = {{Chord Recognition by Fitting Rescaled Chroma Vectors to Chord Templates}},
volume = {19},
year = {2011}
}
@inproceedings{Kashino1995,
address = {Quebec},
author = {Kashino, Kunio and Nakadai, Kazuhiro and Kinoshita, Tomoyoshi and Tanaka, Hidehiko},
booktitle = {International Joint Conference on Artificial Intelligence},
mendeley-groups = {mood},
pages = {158--164},
title = {{Organization of Hierarchical Perceptual Sounds: Music Scene Analysis with Autonomous Processing Modules and a Quantitative Information Integration Mechanism}},
url = {http://citeseer.uark.edu:8380/citeseerx/showciting;jsessionid=D58CCAED426097BA141A1A6547B06F36?cid=3408864},
year = {1995}
}
@inproceedings{MattiRyynanen2006,
author = {Ryyn{\"{a}}nen, Matti and Klapuri, Anssi},
booktitle = {Proceedings of the International Conference on Music Information Retrieval (ISMIR)},
mendeley-groups = {mood},
pages = {222--227},
title = {{Transcription of the singing melody in polyphonic music}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.79.7724},
year = {2006}
}
@inproceedings{Klapuri,
author = {Klapuri, A P},
booktitle = {IEEE Workshop on Applications of Signal Processing to Audio and Acoustics, 2005.},
doi = {10.1109/ASPAA.2005.1540227},
isbn = {0-7803-9154-3},
keywords = {Acoustic signal processing,Auditory system,Computational modeling,Computer peripherals,Frequency estimation,Humans,Multiple signal classification,Music,Signal analysis,Signal processing,acoustic signal detection,concurrent musical sounds,human auditory periphery,multiple-fundamental frequency estimation method,periodicity analysis mechanism,peripheral hearing model},
mendeley-groups = {mood},
pages = {291--294},
publisher = {IEEE},
title = {{A perceptually motivated multiple-F0 estimation method}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=1540227},
year = {2005}
}
@inproceedings{Pesek,
address = {Dubrovnik},
author = {Pesek, Matevz and Leonardis, Ales and Marolt, Matija},
booktitle = {International Conference on Systems, Signals and Image Processing (IWSSIP), 2014},
issn = {2157-8672},
keywords = {Estimation,Hidden Markov models,Silicon carbide,audio chord estimation,compositional hierarchical model,deep learning,stacking generalization},
mendeley-groups = {mood},
pages = {107--110},
publisher = {IEEE},
title = {{Boosting audio chord estimation using multiple classifiers}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=6837642},
year = {2014}
}
@inproceedings{Kim2010,
address = {Utrecht},
author = {Kim, Y E and Schmidt, E M and Migneco, R and Morton, B G and Richardson, P and Scott, J and Speck, J A and Turnbull, D},
booktitle = {Proceedings of the International Conference on Music Information Retrieval (ISMIR)},
mendeley-groups = {mood},
pages = {255--266},
title = {{Music emotion recognition: A state of the art review}},
year = {2010}
}
@article{Mohamed2010,
author = {Mohamed, Abdel-rahman and Dahl, George E and Hinton, Geoffrey},
journal = {IEEE Transactions on Audio, Speech, and Language Processing},
mendeley-groups = {mood},
number = {1},
pages = {14--22},
title = {{Acoustic Modeling using Deep Belief Networks}},
volume = {20},
year = {2010}
}
@phdthesis{Marolt2002,
author = {Marolt, Matija},
mendeley-groups = {mood},
title = {{Automatic Transcription of Piano Music with Neural Networks}},
year = {2002}
}
@article{Tirovolas2011,
author = {Tirovolas, Anna K and Levitin, Daniel J},
journal = {Music Perception: An Interdisciplinary Journal},
mendeley-groups = {mood},
number = {1},
pages = {23--36},
title = {{music perception and cognition research from 1983 to 2010: a categorical and bibliometric analysis of empirical articles in Music Perception}},
volume = {29},
year = {2011}
}
@inproceedings{LardeurERHS09_ClassAudioProduction_ICASSP,
author = {Lardeur, M and Essid, S and Richard, G and Haller, M and Sikora, T},
booktitle = {Acoustics, Speech and Signal Processing, 2009. ICASSP 2009. IEEE International Conference on},
doi = {10.1109/ICASSP.2009.4959918},
issn = {1520-6149},
mendeley-groups = {mood},
month = {apr},
pages = {1653--1656},
title = {{Incorporating prior knowledge on the digital media creation process into audio classifiers}},
year = {2009}
}
@book{Gelfand2004,
author = {Gelfand, Stanley A},
mendeley-groups = {mood},
pages = {312},
title = {{Hearing: An introduction to psychological and physiological acoustics}},
year = {2004}
}
@inproceedings{Henaff2011,
author = {Henaff, Mikael and Jarrett, Kevin and Kavukcouglu, Koray and LeCun, Yann},
booktitle = {Proceedings of the International Conference on Music Information Retrieval (ISMIR)},
mendeley-groups = {mood},
pages = {681--686},
title = {{Unsupervised Learning of Sparse Features for Scalable Audio Classification}},
year = {2011}
}
@article{Amitay2006,
author = {Amitay, Sygal and Irwin, Amy and Moore, David R},
journal = {Nature Neuroscience},
mendeley-groups = {mood},
number = {11},
pages = {1446--1448},
title = {{Discrimination learning induced by training with identical stimuli}},
volume = {9},
year = {2006}
}
@inproceedings{Zhang2005,
address = {Vancouver},
author = {Zhang, Yun-Gang and Zhang, Chang-Shui},
booktitle = {Advances in Neural Information Processing Systems},
mendeley-groups = {mood},
title = {{Separation of Music Signals by Harmonic Structure Modeling}},
year = {2005}
}
@book{Werner2012,
address = {New York},
author = {Werner, Lynne A and Abdala, Carolina and Keefe, Douglas H and Eggermont, Jos J and Moore, Jean K and Buss, Emily and Hall, Joseph W I I I and Grose, John H and Leibold, Lori J and Litovsky, Ruth Y and Panneton, Robin and Newman, Rochelle and Trainor, Laurel J and Unrau, Andrea and Eisenberg, Laurie S and Johnson, Karen C and Ambrose, Sophie E},
editor = {Jones, Mari Riess and Fay, Richard R and Popper, Arthur N},
mendeley-groups = {mood},
pages = {284},
publisher = {Springer},
title = {{Human Auditory Development}},
year = {2012}
}
@article{Downie2008,
author = {Downie, J Stephen},
journal = {Acoustical Science and Technology},
mendeley-groups = {mood},
number = {4},
pages = {247--255},
title = {{The music information retrieval evaluation exchange (2005–2007): A window into music information retrieval research}},
url = {https://www.jstage.jst.go.jp/article/ast/29/4/29{\_}4{\_}247/{\_}article},
volume = {29},
year = {2008}
}
@inproceedings{Dessein2010,
author = {Dessein, A and Cont, A and Lemaitre, G},
booktitle = {Proceedings of the International Conference on Music Information Retrieval (ISMIR)},
mendeley-groups = {mood},
pages = {489--494},
title = {{Real-time polyphonic music transcription with non-negative matrix factorization and beta-divergence}},
url = {http://hal.upmc.fr/hal-00708682},
year = {2010}
}
@article{Braun1999,
author = {Braun, Martin},
journal = {Hearing Research},
mendeley-groups = {mood},
pages = {71--82},
title = {{Audtiroy midbrain laminar structure appears adapted to f0 extraction: further evidence and implications of the double critical bandwidth}},
volume = {129},
year = {1999}
}
@inproceedings{Pesek2013c,
author = {Pesek, Matev{\v{z}} and Guna, Jo{\v{z}}e and Leonardis, Ale{\v{s}} and Marolt, Matija},
booktitle = {Proceedings of the 4th International Conference World Usability Day Slovenia 2013},
mendeley-groups = {mood},
pages = {56--59},
title = {{Visualization of a deep architecture using the compositional hierarchical model}},
year = {2013}
}
@inproceedings{Humphrey2012a,
address = {New York},
author = {Humphrey, Eric J and Cho, Taemin and Bello, Juan P},
booktitle = {Acoustics, Speech and Signal Processing (ICASSP)},
mendeley-groups = {mood},
pages = {453--456},
title = {{Learning a Robust Tonnetz-Space Transform for Automatic Chord recognition}},
year = {2012}
}
@incollection{Downie2010,
address = {Berlin},
author = {Downie, J Stephen and Ehmann, Andreas F and Bay, Mert and Jones, M Cameron},
booktitle = {Advances in Music Information Retrieval},
editor = {A.A., Wieczorkowska and Z.W., Ras},
mendeley-groups = {mood},
pages = {93--115},
publisher = {Springer-Verlag},
title = {{The Music Information Retrieval Evaluation eXchange: Some Observations and Insights}},
year = {2010}
}
@inproceedings{Bengio2007,
author = {Bengio, Yoshua and Lamblin, Pascal and Popovici, Dan and Larochelle, Hugo},
booktitle = {Advances in Neural Information Processing Systems},
mendeley-groups = {mood},
pages = {153--160},
publisher = {MIT Press},
title = {{Greedy Layer-Wise Training of Deep Networks}},
year = {2007}
}
@article{Tkalcic2013,
abstract = {Affective labeling of multimedia content has proved to be useful in recommender systems. In this paper we present a methodology for the implicit acquisition of affective labels for images. It is based on an emotion detection technique that takes as input the video sequences of the users' facial expressions. It extracts Gabor low level features from the video frames and employs a k nearest neighbors machine learning technique to generate affective labels in the valence-arousal-dominance space. We performed a comparative study of the performance of a content-based recommender (CBR) system for images that uses three types of metadata to model the users and the items: (i) generic metadata, (ii) explicitly acquired affective labels and (iii) implicitly acquired affective labels with the proposed methodology. The results show that the CBR performs best when explicit labels are used. However, implicitly acquired labels yield a significantly better performance of the CBR than generic metadata while being an unobtrusive feedback tool.},
author = {Tkalcic, Marko and Odic, Ante and Kosir, Andrej and Tasic, Jurij},
doi = {10.1109/TMM.2012.2229970},
issn = {1520-9210},
journal = {IEEE Transactions on Multimedia},
mendeley-groups = {mood},
number = {2},
pages = {391--400},
shorttitle = {Multimedia, IEEE Transactions on},
title = {{Affective Labeling in a Content-Based Recommender System for Images}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6362231},
volume = {15},
year = {2013}
}
@article{Yu2011,
abstract = {The purpose of this article is to introduce the readers to the emerging technologies enabled by deep learning and to review the research work conducted in this area that is of direct relevance to signal processing. We also point out, in our view, the future research directions that may attract interests of and require efforts from more signal processing researchers and practitioners in this emerging area for advancing signal and information processing technology and applications.},
author = {Yu, Dong and Deng, Li},
doi = {10.1109/MSP.2010.939038},
issn = {1053-5888},
journal = {IEEE Signal Processing Magazine},
mendeley-groups = {mood},
number = {1},
pages = {145--154},
shorttitle = {Signal Processing Magazine, IEEE},
title = {{Deep Learning and Its Applications to Signal and Information Processing [Exploratory DSP}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5670617},
volume = {28},
year = {2011}
}
@inproceedings{Smaragdis2003,
abstract = {We present a methodology for analyzing polyphonic musical passages comprised of notes that exhibit a harmonically fixed spectral profile (such as piano notes). Taking advantage of this unique note structure, we can model the audio content of the musical passage by a linear basis transform and use non-negative matrix decomposition methods to estimate the spectral profile and the temporal information of every note. This approach results in a very simple and compact system that is not knowledge-based, but rather learns notes by observation.},
author = {Smaragdis, P and Brown, J C},
booktitle = {2003 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (IEEE Cat. No.03TH8684)},
doi = {10.1109/ASPAA.2003.1285860},
isbn = {0-7803-7850-4},
keywords = {Cost function,Educational institutions,Harmonic analysis,Image analysis,Independent component analysis,Matrix decomposition,Multiple signal classification,Physics,Principal component analysis,Redundancy,audio content modeling,audio signal processing,harmonically fixed spectral profile,linear basis transform,music,nonnegative matrix decomposition,nonnegative matrix factorization,parameter estimation,polyphonic music transcription,spectral analysis,spectral profile estimation,temporal information estimation},
mendeley-groups = {mood},
pages = {177--180},
publisher = {IEEE},
shorttitle = {Applications of Signal Processing to Audio and Aco},
title = {{Non-negative matrix factorization for polyphonic music transcription}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1285860},
year = {2003}
}
@inproceedings{Goodfellow2009,
author = {Goodfellow, Ian J and Le, Quoc V and Saxe, Andrew M and Lee, Honglak and Ng, Andrew Y},
booktitle = {Proceedings of Neural Information Processing Systems},
mendeley-groups = {mood},
title = {{Measuring invariances in deep networks}},
year = {2009}
}
@inproceedings{Gomez2004,
address = {Barcelona},
author = {Gomez, Emilia and Herrera, Perfecto},
booktitle = {Proceedings of the International Conference on Music Information Retrieval (ISMIR)},
mendeley-groups = {mood},
pages = {92--95},
title = {{Estimating the Tonality of Polyphonic Audio Files: Cognitive versus Machine Learning Modelling Strategies}},
year = {2004}
}
@inproceedings{Pesek2014a,
author = {Pesek, Matev{\v{z}} and Godec, Primo{\v{z}} and Poredo{\v{s}}, Mojca and Strle, Gregor and Guna, Jo{\v{z}}e and Stojmenova, Emilija and Poga{\v{c}}nik, Matev{\v{z}} and Marolt, Matija},
booktitle = {Proceedings of the EMPIRE workshop},
mendeley-groups = {mood},
title = {{Gathering a dataset of multi-modal mood-dependent perceptual responses to music}},
year = {2014}
}
@article{Yu1983,
author = {Yu, Julie and Cooper, Harris},
journal = {Journal of Marketing Research},
mendeley-groups = {mood},
number = {1},
pages = {36--44},
title = {{A Quantitative Review of Research Design Effects on Response Rates to Questionnaires}},
volume = {20},
year = {1983}
}
@article{Bangor2009,
author = {Bangor, Aaron and Kortum, Philip and Miller, James},
journal = {Journal of Usability Studies},
mendeley-groups = {mood},
number = {3},
title = {{Determining What Individual SUS Scores Mean: Adding an Adjective Rating Scale}},
volume = {4},
year = {2009}
}
@article{Paraskevopoulos2010,
author = {Paraskevopoulos, Evangelos and Tsapkini, Kyrana and Peretz, Isabelle},
journal = {Journal of the International Neuropsychological Society},
mendeley-groups = {mood},
number = {4},
pages = {1--10},
title = {{Cultural aspects of music perception: Validation of a Greek version of the Montreal Battery of Evaluation of Amusias}},
volume = {16},
year = {2010}
}
@book{Palm1986,
address = {Berlin, Heidelberg},
doi = {10.1007/978-3-642-70911-1},
editor = {Palm, G{\"{u}}nther and Aertsen, Ad},
isbn = {978-3-642-70913-5},
mendeley-groups = {mood},
publisher = {Springer Berlin Heidelberg},
title = {{Brain Theory}},
url = {http://www.springerlink.com/index/10.1007/978-3-642-70911-1},
year = {1986}
}
@book{Lerdahl1983,
author = {Lerdahl, Fred and Jackendoff, Ray},
mendeley-groups = {mood},
publisher = {Cambridge: MIT Press},
title = {{A generative theory of tonal music}},
year = {1983}
}
@article{Liu2006,
author = {Liu, D.},
doi = {10.1109/TSA.2005.860344},
issn = {1558-7916},
journal = {IEEE Transactions on Audio, Speech and Language Processing},
keywords = {Acoustic signal detection,Affective computing,Computer vision,Data mining,Mood,Multiple signal classification,Music information retrieval,Psychology,Rhythm,Testing,Timbre,acoustic signal detection,audio signal processing,automatic music mood detection,feature extraction,hierarchical framework,intensity feature set,mood detection,mood tracking,music,music audio signal tracking,music clip emotional expression,music emotion,music information retrieval,music mood,music retrieval,music understanding,rhythm feature set,rhythm regularity,rhythm strength,spectral contrast features,spectral shape features,tempo,timbre feature set},
language = {English},
mendeley-groups = {mood},
month = {jan},
number = {1},
pages = {5--18},
publisher = {IEEE},
title = {{Automatic mood detection and tracking of music audio signals}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=1561259},
volume = {14},
year = {2006}
}
@article{Mandryk2012,
author = {Mandryk, Regan L},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mandryk - 2012 - MODELING MUSICAL MOOD FROM AUDIO FEATURES AND LISTENING CONTEXT ON AN IN-SITU DATA SET Diane Watson.pdf:pdf},
number = {Ismir},
pages = {31--36},
title = {{MODELING MUSICAL MOOD FROM AUDIO FEATURES AND LISTENING CONTEXT ON AN IN-SITU DATA SET Diane Watson}},
year = {2012}
}
@article{Panda,
author = {Panda, R and Malheiro, R and Rocha, B and Oliveira, A and Paiva, R P},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Panda et al. - Unknown - Multi-Modal Music Emotion Recognition A New Dataset , Methodology and Comparative Analysis.pdf:pdf},
keywords = {machine learning,multi-modal,music emotion recognition},
pages = {1--13},
title = {{Multi-Modal Music Emotion Recognition : A New Dataset , Methodology and Comparative Analysis}}
}
@article{Laurier,
author = {Laurier, Cyril and Herrera, Perfecto and Mandel, Michael and Ellis, Dan},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Laurier et al. - Unknown - AUDIO MUSIC MOOD CLASSIFICATION USING SUPPORT VECTOR MACHINE.pdf:pdf},
pages = {2--4},
title = {{AUDIO MUSIC MOOD CLASSIFICATION USING SUPPORT VECTOR MACHINE}}
}
@article{Schmidt2011,
author = {Schmidt, Erik M and Kim, Youngmoo E},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schmidt, Kim - 2011 - MODELING MUSICAL EMOTION DYNAMICS WITH CONDITIONAL RANDOM FIELDS.pdf:pdf},
number = {Ismir},
pages = {777--782},
title = {{MODELING MUSICAL EMOTION DYNAMICS WITH CONDITIONAL RANDOM FIELDS}},
year = {2011}
}
@article{Schmidt2010,
author = {Schmidt, Erik M and Kim, Youngmoo E},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schmidt, Kim - 2010 - PREDICTION OF TIME-VARYING MUSICAL MOOD DISTRIBUTIONS FROM AUDIO.pdf:pdf},
number = {Ismir},
pages = {465--470},
title = {{PREDICTION OF TIME-VARYING MUSICAL MOOD DISTRIBUTIONS FROM AUDIO}},
year = {2010}
}
