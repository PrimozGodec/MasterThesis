@techreport{Saupe2006,
author = {Saupe, D and Hamzaoui, R and Hartenstein, H},
mendeley-groups = {deep colorization},
publisher = {Albert-Ludwigs University at Freiburg},
title = {{Fractal Image Compression - An Introductory Overview}},
year = {1997}
}
@article{Wu2017,
abstract = {Image steganalysis is to discriminate innocent images and those suspected images with hidden messages. This task is very challenging for modern adaptive steganography, since modifications due to message hiding are extremely small. Recent studies show that Convolutional Neural Networks (CNN) have demonstrated superior performances than traditional steganalytic methods. Following this idea, we propose a novel CNN model for image steganalysis based on residual learning. The proposed Deep Residual learning based Network (DRN) shows two attractive properties than existing CNN based methods. First, the model usually contains a large number of network layers, which proves to be effective to capture the complex statistics of digital images. Second, the residual learning in DRN preserves the stego signal coming from secret messages, which is extremely beneficial for the discrimination of cover images and stego images. Comprehensive experiments on standard dataset show that the DRN model can detect the state of arts steganographic algorithms at a high accuracy. It also outperforms the classical rich model method and several recently proposed CNN based methods.},
author = {Wu, Songtao and Zhong, Shenghua and Liu, Yan},
doi = {10.1007/s11042-017-4440-4},
issn = {1573-7721},
journal = {Multimedia Tools and Applications},
mendeley-groups = {deep colorization},
title = {{Deep residual learning for image steganalysis}},
url = {http://dx.doi.org/10.1007/s11042-017-4440-4},
year = {2017}
}
@article{Prangnell,
author = {Prangnell, Lee},
journal = {CoRR},
mendeley-groups = {deep colorization},
title = {{Visible Light-Based Human Visual System Conceptual Model}},
url = {http://arxiv.org/abs/1609.04830},
volume = {abs/1609.0},
year = {2016}
}
@incollection{Krizhevsky2012,
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
booktitle = {Advances in Neural Information Processing Systems 25},
editor = {Pereira, F and Burges, C J C and Bottou, L and Weinberger, K Q},
mendeley-groups = {deep colorization},
pages = {1097--1105},
publisher = {Curran Associates, Inc.},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
year = {2012}
}
@misc{Karpathy2016a,
author = {Karpathy, Andrej},
howpublished = {CS231n Convolutional Neural Networks for Visual Recognition, Stanford University},
institution = {Stanford University},
mendeley-groups = {deep colorization},
publisher = {Stanford University},
title = {{Neural Networks Part 1: Setting up the Architecture}},
year = {2016}
}
@inbook{Zhang2016,
abstract = {Given a grayscale photograph as input, this paper attacks the problem of hallucinating a plausible color version of the photograph. This problem is clearly underconstrained, so previous approaches have either relied on significant user interaction or resulted in desaturated colorizations. We propose a fully automatic approach that produces vibrant and realistic colorizations. We embrace the underlying uncertainty of the problem by posing it as a classification task and use class-rebalancing at training time to increase the diversity of colors in the result. The system is implemented as a feed-forward pass in a CNN at test time and is trained on over a million color images. We evaluate our algorithm using a ``colorization Turing test,'' asking human participants to choose between a generated and ground truth color image. Our method successfully fools humans on 32 {\{}{\%}{\}} of the trials, significantly higher than previous methods. Moreover, we show that colorization can be a powerful pretext task for self-supervised feature learning, acting as a cross-channel encoder. This approach results in state-of-the-art performance on several feature learning benchmarks.},
address = {Cham},
author = {Zhang, Richard and Isola, Phillip and Efros, Alexei A},
booktitle = {Computer Vision -- ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part III},
doi = {10.1007/978-3-319-46487-9_40},
editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
isbn = {978-3-319-46487-9},
mendeley-groups = {deep colorization},
pages = {649--666},
publisher = {Springer International Publishing},
title = {{Colorful Image Colorization}},
url = {http://dx.doi.org/10.1007/978-3-319-46487-9{\_}40},
year = {2016}
}
@inproceedings{Cheng2015,
author = {Cheng, Zezhou and Yang, Qingxiong and Sheng, Bin},
booktitle = {The IEEE International Conference on Computer Vision (ICCV)},
mendeley-groups = {deep colorization},
month = {dec},
title = {{Deep Colorization}},
year = {2015}
}
@article{bay2006surf,
abstract = {Abstract. In this paper, we present a novel scale- and rotation-invariant interest point detector and descriptor, coined SURF (Speeded Up Ro- bust Features). It approximates or even outperforms previously proposed schemes with respect to repeatability, distinctiveness, and robustness, yet can be computed and compared much faster. This is achieved by relying on integral images for image convolutions; by building on the strengths of the leading existing detectors and descriptors (in casu, using a Hessian matrix-based measure for the detector, and a distribution-based descriptor); and by simplifying these methods to the essential. This leads to a combination of novel detection, description, and matching steps. The paper presents experimental results on a standard evaluation set, as well as on imagery obtained in the context of a real-life object recognition application. Both show SURF's strong performance.},
author = {Bay, Herbert and Tuytelaars, Tinne and {Van Gool}, Luc},
doi = {10.1007/11744023_32},
isbn = {3540338322},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {To Read,matching,sift},
mendeley-groups = {deep colorization},
pages = {404--417},
pmid = {16081019},
publisher = {Springer},
title = {{SURF: Speeded up robust features}},
url = {papers2://publication/uuid/32095B14-5C8B-49A1-B157-ED2737F06BE1},
volume = {3951 LNCS},
year = {2006}
}
@inproceedings{dalal2005histograms,
author = {Dalal, Navneet and Triggs, Bill},
booktitle = {Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on},
mendeley-groups = {deep colorization},
organization = {IEEE},
pages = {886--893},
title = {{Histograms of oriented gradients for human detection}},
volume = {1},
year = {2005}
}
@inproceedings{ke2004pca,
author = {Ke, Yan and Sukthankar, Rahul},
booktitle = {Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004.},
mendeley-groups = {deep colorization},
organization = {IEEE},
pages = {II--506--II--513},
title = {{PCA-SIFT: A more distinctive representation for local image descriptors}},
volume = {2},
year = {2004}
}
@article{hauke2011comparison,
author = {Hauke, Jan and Kossowski, Tomasz},
journal = {Quaestiones geographicae},
mendeley-groups = {deep colorization},
number = {2},
pages = {87},
publisher = {De Gruyter Open Sp. z oo},
title = {{Comparison of values of Pearson's and Spearman's correlation coefficients on the same sets of data}},
volume = {30},
year = {2011}
}
@book{welstead1999fractal,
author = {Welstead, Stephen T},
booktitle = {Fractal and wavelet image compression techniques},
mendeley-groups = {deep colorization},
pages = {1--10},
publisher = {Spie Press},
title = {{Introduction}},
volume = {40},
year = {1999}
}
@article{Wickelmaier2003,
abstract = {1. MDS relation with correlations 2. Classical and nonmetric MDS concept and difference.},
author = {Wickelmaier, Florian},
doi = {10.1080/13546800903126016},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wickelmaier - 2003 - An Introduction to MDS.pdf:pdf},
isbn = {1379-1176 (Print){\$}\backslash{\$}r1379-1176 (Linking)},
issn = {13791176},
journal = {Sound Quality Research Unit, Aalborg University, Denmark},
mendeley-groups = {deep colorization},
pmid = {20491413},
title = {{An introduction to MDS}},
url = {https://homepages.uni-tuebingen.de/florian.wickelmaier/pubs/Wickelmaier2003SQRU.pdf http://steep.inrialpes.fr/{\%}7B{~}{\%}7DArnaud/indexation/mds03.pdf},
year = {2003}
}
@article{DBLP:journals/corr/KingmaB14,
author = {Kingma, Diederik P and Ba, Jimmy},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kingma, Ba - 2014 - Adam A Method for Stochastic Optimization.pdf:pdf},
journal = {CoRR},
mendeley-groups = {deep colorization},
title = {{Adam: A Method for Stochastic Optimization}},
url = {http://arxiv.org/abs/1412.6980},
volume = {abs/1412.6},
year = {2014}
}
@incollection{joyce2011kullback,
author = {Joyce, James M},
booktitle = {International Encyclopedia of Statistical Science},
mendeley-groups = {deep colorization},
pages = {720--722},
publisher = {Springer},
title = {{Kullback-leibler divergence}},
year = {2011}
}
@inproceedings{Mannor2005,
address = {New York, New York, USA},
author = {Mannor, Shie and Peleg, Dori and Rubinstein, Reuven},
booktitle = {Proceedings of the 22nd international conference on Machine learning  - ICML '05},
doi = {10.1145/1102351.1102422},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mannor, Peleg, Rubinstein - 2005 - The cross entropy method for classification.pdf:pdf},
isbn = {1595931805},
mendeley-groups = {deep colorization},
pages = {561--568},
publisher = {ACM Press},
title = {{The cross entropy method for classification}},
url = {http://portal.acm.org/citation.cfm?doid=1102351.1102422},
year = {2005}
}
@article{Dumoulin2016,
abstract = {We introduce a guide to help deep learning practitioners understand and manipulate convolutional neural network architectures. The guide clarifies the relationship between various properties (input shape, kernel shape, zero padding, strides and output shape) of convolutional, pooling and transposed convolutional layers, as well as the relationship between convolutional and transposed convolutional layers. Relationships are derived for various cases, and are illustrated in order to make them intuitive.},
archivePrefix = {arXiv},
arxivId = {1603.07285},
author = {Dumoulin, Vincent and Visin, Francesco},
doi = {10.1051/0004-6361/201527329},
eprint = {1603.07285},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dumoulin, Visin - 2016 - A guide to convolution arithmetic for deep learning.pdf:pdf},
isbn = {9783319105895},
issn = {16113349},
journal = {Xiv preprint arXiv:1603.07285},
mendeley-groups = {deep colorization},
month = {mar},
pmid = {26353135},
title = {{A guide to convolution arithmetic for deep learning}},
url = {http://arxiv.org/abs/1603.07285},
year = {2016}
}
@article{mackay1992practical,
author = {MacKay, David J C},
journal = {Neural computation},
mendeley-groups = {deep colorization},
number = {3},
pages = {448--472},
publisher = {MIT Press},
title = {{A practical Bayesian framework for backpropagation networks}},
volume = {4},
year = {1992}
}
@inproceedings{Ramaswamy,
abstract = {In this paper, we propose a novel formulation for distance-based outliers that is based on the distance of a point from its kth nearest neighbor. We rank each point on the basis of its distance to its kth nearest neighbor and declare the top n points in this ranking to be outliers. In addition to developing relatively straightforward solutions to finding such outliers based on the classical nested-loop join and index join algorithms, we develop a highly efficient partition-based algorithm for mining outliers. This algorithm first partitions the input data set into disjoint subsets, and then prunes entire partitions as soon as it is determined that they cannot contain outliers. This results in substantial savings in computation. We present the results of an extensive experimental study on real-life and synthetic data sets. The results from a real-life NBA database highlight and reveal several expected and unexpected aspects of the database. The results from a study on synthetic data sets demonstrate that the partition-based algorithm scales well with respect to both data set size and data set dimensionality.},
author = {Ramaswamy, Sridhar and Rastogi, Rajeev and Shim, Kyuseok},
booktitle = {Proceedings of the 2000 ACM SIGMOD international conference on Management of data - SIGMOD '00},
doi = {10.1145/342009.335437},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ramaswamy, Rastogi, Shim KAIST - Unknown - Efficient Algorithms for Mining Outliers from Large Data Sets.pdf:pdf},
isbn = {1581132174},
issn = {01635808},
mendeley-groups = {deep colorization},
pages = {427--438},
title = {{Efficient algorithms for mining outliers from large data sets}},
url = {ftp://ftp10.us.freebsd.org/users/azhang/disc/disc01/cd1/out/papers/sigmod/efficientalgorisrrak.pdf http://portal.acm.org/citation.cfm?doid=342009.335437},
year = {2000}
}
@inproceedings{collobert2008unified,
author = {Collobert, Ronan and Weston, Jason},
booktitle = {Proceedings of the 25th international conference on Machine learning},
mendeley-groups = {deep colorization},
organization = {ACM},
pages = {160--167},
title = {{A unified architecture for natural language processing: Deep neural networks with multitask learning}},
year = {2008}
}
@article{lecun1995convolutional,
author = {LeCun, Yann and Bengio, Yoshua and Others},
journal = {The handbook of brain theory and neural networks},
mendeley-groups = {deep colorization},
number = {10},
pages = {1995},
title = {{Convolutional networks for images, speech, and time series}},
volume = {3361},
year = {1995}
}
@article{LeCun2015,
author = {LeCun, Y and Bengio, Y and Hinton, G},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/LeCun, Bengio, Hinton - 2015 - Deep learning.pdf:pdf},
journal = {Nature},
mendeley-groups = {deep colorization},
title = {{Deep learning}},
url = {https://www.nature.com/nature/journal/v521/n7553/abs/nature14539.html},
year = {2015}
}
@article{Connolly1997,
author = {Connolly, C. and Fleiss, T.},
doi = {10.1109/83.597279},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Connolly, Fleiss - 1997 - A study of efficiency and accuracy in the transformation from RGB to CIELAB color space.pdf:pdf},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
mendeley-groups = {deep colorization},
month = {jul},
number = {7},
pages = {1046--1048},
title = {{A study of efficiency and accuracy in the transformation from RGB to CIELAB color space}},
url = {http://ieeexplore.ieee.org/document/597279/},
volume = {6},
year = {1997}
}
@article{Weatherall1992,
author = {Weatherall, Ian L and Coombs, Bernard D},
doi = {10.1111/1523-1747.ep12616156},
issn = {0022202X},
journal = {Journal of Investigative Dermatology},
mendeley-groups = {deep colorization},
month = {oct},
number = {4},
pages = {468--473},
title = {{Skin Color Measurements in Terms of CIELAB Color Space Values}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0022202X92903477},
volume = {99},
year = {1992}
}
@inproceedings{tai2005local,
author = {Tai, Yu-Wing and Jia, Jiaya and Tang, Chi-Keung},
booktitle = {2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)},
mendeley-groups = {deep colorization},
organization = {IEEE},
pages = {747--754},
title = {{Local color transfer via probabilistic segmentation by expectation-maximization}},
volume = {1},
year = {2005}
}
@inproceedings{huang2005adaptive,
author = {Huang, Yi-Chin and Tung, Yi-Shin and Chen, Jun-Cheng and Wang, Sung-Wen and Wu, Ja-Ling},
booktitle = {Proceedings of the 13th annual ACM international conference on Multimedia},
mendeley-groups = {deep colorization},
organization = {ACM},
pages = {351--354},
title = {{An adaptive edge detection based colorization algorithm and its applications}},
year = {2005}
}
@article{larsson2016learning,
author = {Larsson, Gustav and Maire, Michael and Shakhnarovich, Gregory},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Larsson, Maire, Shakhnarovich - 2016 - Learning Representations for Automatic Colorization.pdf:pdf},
journal = {arXiv preprint arXiv:1603.06668},
mendeley-groups = {deep colorization},
title = {{Learning Representations for Automatic Colorization}},
year = {2016}
}
@article{shirley2001color,
author = {Shirley, Peter},
journal = {IEEE Corn},
mendeley-groups = {deep colorization},
pages = {34--41},
title = {{Color transfer between images}},
volume = {21},
year = {2001}
}
@inproceedings{levin2004colorization,
author = {Levin, Anat and Lischinski, Dani and Weiss, Yair},
booktitle = {ACM Transactions on Graphics (TOG)},
mendeley-groups = {deep colorization},
number = {3},
organization = {ACM},
pages = {689--694},
title = {{Colorization using optimization}},
volume = {23},
year = {2004}
}
@incollection{Jack2005,
abstract = {This international bestseller and essential reference is the "bible" for digital video engineers and programmers worldwide. This fourth edition is completely updated with all new chapters on MPEG-4, H.264, SDTV/HDTV, ATSC/DVB, and Streaming Video (Video over DSL, Ethernet, etc.), as well as discussions of the latest standards throughout. This is by far the most informative analog and digital video reference available, made even more comprehensive through the author's inclusion of the hottest new trends and cutting-edge developments in the field. Finding another amalgamated source of the huge amount of information in this book is impossible. The author attends DVD and HDTV standards meetings, so the absolute most up-to-date content is assured. The accompanying CD is updated to include a unique set of video test files in the newest formats. This book is a "one stop" reference guide for the various digital video technologies. Professionals in this rapidly changing field need the new edition of this book to keep up with the latest developments and standards in the industry.*This essential reference is the "bible" for digital video engineers and programmers worldwide *Contains all new chapters on MPEG-4, H.264, SDTV/HDTV, ATSC/DVB, and Streaming Video *Completely revised with all the latest and most up-to-date industry standards},
author = {Jack, Keith},
booktitle = {Video Demystified - A Handbook for the Digital Engineer},
isbn = {1-878707-56-6},
mendeley-groups = {deep colorization},
pages = {15--34},
pmid = {14767919},
publisher = {Elsevier},
title = {{Color spaces}},
url = {http://www.sciencedirect.com/science/book/9780750678223},
year = {1989}
}
@incollection{Jack2005,
abstract = {The chapter reviews the common color spaces, how they are mathematically related and when a specific color space is used. A color space is a mathematical representation of a set of colors. All of the color spaces can be derived from the red, green, and blue (RGB) information supplied by devices such as cameras and scanners. The three most popular color models are RGB (used in computer graphics); YIQ, YUV, or YCbCr (used in video systems); and CMYK (used in color printing). Considerations for converting from a non-RGB to a RGB color space and gamma correction are also discussed in the chapter. By “gamma correcting” the video signals before transmission, the intensity output of the display becomes roughly linear and the transmission- induced noise is reduced.},
author = {Jack, Keith and Jack, Keith},
booktitle = {Video Demystified},
doi = {10.1016/B978-075067822-3/50004-X},
isbn = {9780750678223},
mendeley-groups = {deep colorization},
pages = {15--34},
title = {{Chapter 3 – Color Spaces}},
year = {2005}
}
@article{ILSVRC15,
author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C and Fei-Fei, Li},
doi = {10.1007/s11263-015-0816-y},
journal = {International Journal of Computer Vision (IJCV)},
mendeley-groups = {deep colorization},
number = {3},
pages = {211--252},
title = {{ImageNet Large Scale Visual Recognition Challenge}},
volume = {115},
year = {2015}
}
@misc{Karpathy2016,
author = {Karpathy, Andrej},
howpublished = {CS231n Convolutional Neural Networks for Visual Recognition, Stanford University},
institution = {Stanford University},
mendeley-groups = {deep colorization},
publisher = {Stanford University},
title = {{Understanding and Visualizing Convolutional Neural Networks}},
year = {2016}
}
@book{Schwiegerling2004,
abstract = {"SPIE digital library." Visual optics requires an understanding of both biology and optical engineering. This Field Guide assembles the anatomy, physiology, and functioning of the eye, as well as the engineering and design of a wide assortment of tools for measuring, photographing, and characterizing properties of the surfaces and structures of the eye. Also covered are the diagnostic techniques, lenses, and surgical techniques used to correct and improve human vision. Glossary -- Ocular function -- Eyeball -- Cornea -- Retina -- Photoreceptors -- Retinal landmarks -- Properties of ocular components -- Accommodation -- Pupil size and dark adaptation -- Transmission and reflectance -- Axes of the eye -- Stiles-Crawford effect -- Photopic V[lambda] and scotopic V'[lambda] response -- Eye movements -- Vergence -- Paraxial schematic eye -- Arizona eye model -- Aberrations -- Visual acuity -- Visual acuity and eye charts -- Contrast sensitivity function (CSF) -- Emmetropia and ametropia -- Far and near points -- Presbyopia -- Correction of ocular errors -- Spectacles : single vision -- Spectacle lenses -- Lensmeter -- Spherical and cylindrical refractive error -- Prismatic error -- Astigmatic decomposition -- Special ophthalmic lenses -- Variable prisms and lenses -- Contact lenses -- Radiuscope -- Spectacle and contact lens materials -- Surgical correction of refractive error -- Cataract surgery -- Ophthalmic instrumentation and metrology -- Purkinje images -- Fluorescein imaging -- Indocyanine green imaging -- Keratometry -- Corneal topography -- Corneal topography : axial power -- Corneal topography : instantaneous power -- Anterior segment imaging -- Wavefront sensing : Shack-Hartmann sensing -- Wavefront sensing : Tscherning aberrometry -- Wavefront sensing : retinal raytracing -- Wavefront sensing : spatially resolved refractometry -- Wavefront sensing : reconstruction. Zernike polynomials : wavefront sensing standard -- Zernike polynomials : Cartesian coordinates -- Zernike polynomials : useful formulas -- Ophthalmoscopy -- Retinal imaging -- Field of view and perimetry -- Retinoscopy -- Autorefraction -- Badal optometer and Maxwellian view -- Common ophthalmic lasers -- Eye safety : laser sources -- Eye safety : non-laser sources -- Color -- Photometry -- Colorimetry : RBB and CIE XYZ systems -- Colorimetry : chromaticity diagram -- Colorimetry : primaries and gamut -- Colorimetry : CIELUV color space -- Colorimetry : CIELAB color space -- Chromatic adaptation -- L, M, and S cone fundamentals -- Appendices -- Aspheric and astigmatic surfaces -- Differential geometry -- Trigonometric identities -- CIE Photopic V[lambda] and scotopic V'[lambda] response -- 1931 CIE 2° color matching functions -- 1964 CIE 10° color matching functions -- Stockman {\&} Sharpe 2° cone fundamentals -- Incoherent retinal hazard functions -- Zernike polynomials : table in polar coordinates -- Zernike polynomials : table in Cartesian coordinates -- Equation summary -- Bibliography -- Index.},
author = {Schwiegerling, Jim. and {Society of Photo-optical Instrumentation Engineers.}},
isbn = {9780819456298},
mendeley-groups = {deep colorization},
pages = {71},
publisher = {SPIE},
title = {{Field guide to visual and ophthalmic optics}},
url = {https://spie.org/Publications/Book/592975},
year = {2004}
}
@incollection{Ohta2005,
author = {Ohta, N and Robertson, A R},
booktitle = {Colorimetry: Fundamentals and Applications},
doi = {10.1002/0470094745},
isbn = {null},
issn = {1070664X},
mendeley-groups = {deep colorization},
pages = {63--114},
title = {{CIE Standard Colometric System}},
url = {https://books.google.com/books?hl=en{\&}lr={\&}id=U8jeh1uhSHgC{\&}oi=fnd{\&}pg=PR2{\&}dq=Colorimetry:+Fundamentals+and+Applications+{\&}ots=SUbuBOkPfU{\&}sig=FtBn43qgQvrKC0pXi7lAeVB0FGk},
year = {2006}
}
@article{Pm2013,
abstract = {A color model is an abstract mathematical model describing the way colors can be represented as tuples of numbers, typically as three or four values or color components (e.g. RGB and CMYK are color models). However, a color model with no associated mapping function to an absolute color space is a more or less arbitrary color system with no connection to any globally understood system of color interpretation. This paper mainly discusses about various colour spaces and the how they organized and the colour conversion algorithms like CMYK to RGB, RGB to CMYK, HSL to RGB, RGB to HSL , HSV to RGB , RGB To HSV , YUV to RGB And RGB to YUV.},
author = {Nishad, P.M. and {Manicka Chezian}, R.},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pm, Chezian - 2013 - VARIOUS COLOUR SPACES AND COLOUR SPACE CONVERSION ALGORITHMS.pdf:pdf},
isbn = {2229-371X},
journal = {Journal of Global Research in Computer Science},
keywords = {cmyk,colour conversion algorithms,colour spaces,hsl,hsv and yuv,rgb},
mendeley-groups = {deep colorization},
number = {1},
pages = {44--48},
title = {{Various Colour Spaces and Colour Space Conversion}},
url = {http://jgrcs.info/index.php/jgrcs/article/viewFile/587/430},
volume = {4},
year = {2013}
}
@article{Bansal,
abstract = {— This paper proposes an approach for the segmentation of color images using CIELab color space and Ant based clustering. Image segmentation is the process of assigning a label to every pixel in an image such that pixels with the same label share certain visual characteristics. The objective of segmentation is to change the image into meaningful form that is easier to analyze. This paper elaborates the ant based clustering for image segmentation. CMC distance is used to calculate the distance between pixels as this color metric gives good results with CIELab color space. Results show the segmentation using ant based clustering and also verifies that number of clusters for the image with the CMC distance also varies. Clustering quality is evaluated using MSE measure.},
author = {Bansal, Seema and Aggarwal, Deepak},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bansal, Aggarwal - Unknown - Color Image Segmentation Using CIELab Color Space Using Ant Colony Optimization.pdf:pdf},
journal = {IJCSET},
keywords = {CIELab color space,CMC distance,segmentation,— Ant Clust},
mendeley-groups = {deep colorization},
number = {7},
pages = {415--420},
title = {{Color Image Segmentation Using CIELab Color Space Using Ant Colony Optimization}},
url = {http://www.ijcset.net/docs/Volumes/volume1issue7/ijcset2011010715.pdf},
volume = {1},
year = {2011}
}
@article{Hinton2015,
abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
archivePrefix = {arXiv},
arxivId = {1503.02531},
author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
eprint = {1503.02531},
mendeley-groups = {deep colorization},
month = {mar},
title = {{Distilling the Knowledge in a Neural Network}},
url = {http://arxiv.org/abs/1503.02531},
year = {2015}
}
@article{Chen2016,
abstract = {In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled filters, or 'atrous convolution', as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed "DeepLab" system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7{\%} mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online.},
archivePrefix = {arXiv},
arxivId = {1606.00915},
author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
eprint = {1606.00915},
mendeley-groups = {deep colorization},
month = {jun},
title = {{DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs}},
url = {http://arxiv.org/abs/1606.00915},
year = {2016}
}
@article{Donahue2016,
abstract = {The ability of the Generative Adversarial Networks (GANs) framework to learn generative models mapping from simple latent distributions to arbitrarily complex data distributions has been demonstrated empirically, with compelling results showing that the latent space of such generators captures semantic variation in the data distribution. Intuitively, models trained to predict these semantic latent representations given data may serve as useful feature representations for auxiliary problems where semantics are relevant. However, in their existing form, GANs have no means of learning the inverse mapping -- projecting data back into the latent space. We propose Bidirectional Generative Adversarial Networks (BiGANs) as a means of learning this inverse mapping, and demonstrate that the resulting learned feature representation is useful for auxiliary supervised discrimination tasks, competitive with contemporary approaches to unsupervised and self-supervised feature learning.},
archivePrefix = {arXiv},
arxivId = {1605.09782},
author = {Donahue, Jeff and Kr{\"{a}}henb{\"{u}}hl, Philipp and Darrell, Trevor},
eprint = {1605.09782},
mendeley-groups = {deep colorization},
month = {may},
title = {{Adversarial Feature Learning}},
url = {http://arxiv.org/abs/1605.09782},
year = {2016}
}
@article{Simonyan2014,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
eprint = {1409.1556},
journal = {CoPR},
mendeley-groups = {deep colorization},
month = {sep},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
url = {http://arxiv.org/abs/1409.1556},
volume = {abs/1409.1},
year = {2014}
}
@article{Lotter2016,
abstract = {While great strides have been made in using deep learning algorithms to solve supervised learning tasks, the problem of unsupervised learning - leveraging unlabeled examples to learn about the structure of a domain - remains a difficult unsolved challenge. Here, we explore prediction of future frames in a video sequence as an unsupervised learning rule for learning about the structure of the visual world. We describe a predictive neural network ("PredNet") architecture that is inspired by the concept of "predictive coding" from the neuroscience literature. These networks learn to predict future frames in a video sequence, with each layer in the network making local predictions and only forwarding deviations from those predictions to subsequent network layers. We show that these networks are able to robustly learn to predict the movement of synthetic (rendered) objects, and that in doing so, the networks learn internal representations that are useful for decoding latent object parameters (e.g. pose) that support object recognition with fewer training views. We also show that these networks can scale to complex natural image streams (car-mounted camera videos), capturing key aspects of both egocentric movement and the movement of objects in the visual scene, and the representation learned in this setting is useful for estimating the steering angle. Altogether, these results suggest that prediction represents a powerful framework for unsupervised learning, allowing for implicit learning of object and scene structure.},
archivePrefix = {arXiv},
arxivId = {1605.08104},
author = {Lotter, William and Kreiman, Gabriel and Cox, David},
eprint = {1605.08104},
mendeley-groups = {deep colorization},
month = {may},
title = {{Deep Predictive Coding Networks for Video Prediction and Unsupervised Learning}},
url = {http://arxiv.org/abs/1605.08104},
year = {2016}
}
@article{Ioffe2015,
abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9{\%} top-5 validation error (and 4.8{\%} test error), exceeding the accuracy of human raters.},
archivePrefix = {arXiv},
arxivId = {1502.03167},
author = {Ioffe, Sergey and Szegedy, Christian},
eprint = {1502.03167},
mendeley-groups = {deep colorization},
month = {feb},
title = {{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}},
url = {http://arxiv.org/abs/1502.03167},
year = {2015}
}
@inproceedings{Deshpande2015,
abstract = {We describe an automated method for image coloriza-tion that learns to colorize from examples. Our method ex-ploits a LEARCH framework to train a quadratic objective function in the chromaticity maps, comparable to a Gaus-sian random field. The coefficients of the objective function are conditioned on image features, using a random forest. The objective function admits correlations on long spatial scales, and can control spatial error in the colorization of the image. Images are then colorized by minimizing this objective function. We demonstrate that our method strongly outperforms a natural baseline on large-scale experiments with images of real scenes using a demanding loss function. We demon-strate that learning a model that is conditioned on scene produces improved results. We show how to incorporate a desired color histogram into the objective function, and that doing so can lead to further improvements in results.},
author = {Deshpande, Aditya and Rock, Jason and Forsyth, David},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2015.72},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Deshpande, Rock, Forsyth - 2015 - Learning Large-Scale Automatic Image Colorization.pdf:pdf},
isbn = {9781467383912},
issn = {15505499},
keywords = {Color,Histograms,Image color analysis,LEARCH framework,Linear programming,Optimization,Regression tree analysis,Standards,chromaticity maps,color histogram,image processing,large-scale automatic image colorization learning,quadratic objective function,random forest,spatial error control},
mendeley-groups = {deep colorization},
pages = {567--575},
title = {{Learning large-scale automatic image colorization}},
volume = {11-18-Dece},
year = {2016}
}
@article{Koleini2010,
abstract = {This study represents an innovative automatic method for black and white films colorization using texture features and a multilayer perceptron artificial neural network. In the proposed method, efforts are made to remove human interference in the process of colorization and replace it with an artificial neural network (ANN) which is trained using the features of the reference frame. Later, this network is employed for automatic colorization of the remained black and white frames. The reference frames of the black and white film are manually colored. Using a Gabor filter bank, texture features of all the pixels of the reference frame are extracted and used as the input feature vector of the ANN, while the output will be the color vector of the corresponding pixel. Finally, the next frames' feature vectors are fed respectively to the trained neural network, and color vectors of those frames are the output. Applying AVI videos and using various color spaces, a series of experiments are conducted to evaluate the proposed colorization process. This method needs considerable time to provide a reasonable output, given rapidly changing scenes. Fortunately however, due to the high correlation between consecutive frames in typical video footage, the overall performance is promising regarding both visual appearance and the calculated MSE error. Apart from the application, we also aim to show the importance of the low level features in a mainly high level process, and the mapping ability of a neural network.},
author = {Koleini, Mina and Mobadjemi, S A and Moallem, Payman},
doi = {10.1080/02533839.2010.9671693},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Koleini, Mobadjemi, Moallem - 2010 - Automatic Black and White Film Colorization Using Texture Features and Artificial Neural Networks.pdf:pdf},
isbn = {0253-3839},
issn = {02533839},
journal = {Journal of the Chinese Institute of Engineers},
keywords = {artificial neural networks,colorization,gabor filters,images,texture features},
mendeley-groups = {deep colorization},
number = {7},
pages = {1049--1057},
publisher = {Taylor {\&} Francis Group},
title = {{Automatic Black and White Film Colorization Using Texture Features and Artificial Neural Networks}},
volume = {33},
year = {2010}
}
@article{Limmer2016,
abstract = {This paper proposes a method for transferring the RGB color spectrum to near-infrared (NIR) images using deep multi-scale convolutional neural networks. A direct and integrated transfer between NIR and RGB pixels is trained. The trained model does not require any user guidance or a reference image database in the recall phase to produce images with a natural appearance. To preserve the rich details of the NIR image, its high frequency features are transferred to the estimated RGB image. The presented approach is trained and evaluated on a real-world dataset containing a large amount of road scene images in summer. The dataset was captured by a multi-CCD NIR/RGB camera, which ensures a perfect pixel to pixel registration.},
archivePrefix = {arXiv},
arxivId = {1604.02245},
author = {Limmer, Matthias and Lensch, Hendrik P. A.},
eprint = {1604.02245},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Limmer, Lensch - 2016 - Infrared Colorization Using Deep Convolutional Neural Networks.pdf:pdf},
mendeley-groups = {deep colorization},
month = {apr},
pages = {6},
title = {{Infrared Colorization Using Deep Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1604.02245},
year = {2016}
}
@article{Hwang,
abstract = {We present a convolutional-neural-network-based sys-tem that faithfully colorizes black and white photographic images without direct human assistance. We explore var-ious network architectures, objectives, color spaces, and problem formulations. The final classification-based model we build generates colorized images that are significantly more aesthetically-pleasing than those created by the base-line regression-based model, demonstrating the viability of our methodology and revealing promising avenues for fu-ture work.},
author = {Hwang, Jeff and Zhou, You},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hwang, Zhou - Unknown - Image Colorization with Deep Convolutional Neural Networks.pdf:pdf},
mendeley-groups = {deep colorization},
title = {{Image Colorization with Deep Convolutional Neural Networks}}
}
@article{Koo2016,
abstract = {We attempt to use DCGANs (deep convolutional genera-tive adversarial nets) to tackle the automatic colorization of black and white photos to combat the tendency for vanilla neural nets to " average out " the results. We construct a small feed-forward convolutional neural network as a base-line colorization system. We train the baseline model on the CIFAR-10 dataset with a per-pixel Euclidean loss func-tion on the chrominance values and achieve sensible but mediocre results. We propose using the adversarial frame-work as proposed by Goodfellow et al. [5] as an alternative to the loss function—we reformulate the baseline model as a generator model that maps grayscale images and random noise input to the color image space, and construct a dis-criminator model that is trained to predict the probability that a given colorization was sampled from data distribu-tion rather than generated by the generator model, condi-tioned on the grayscale image. We analyze the challenges that stand in the way of training adversarial networks, and suggest future steps to test the viability of the model.},
author = {Koo, Stephen},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Koo - 2016 - Automatic Colorization with Deep Convolutional Generative Adversarial Networks.pdf:pdf},
mendeley-groups = {deep colorization},
title = {{Automatic Colorization with Deep Convolutional Generative Adversarial Networks}},
url = {http://cs231n.stanford.edu/reports2016/224{\_}Report.pdf},
year = {2016}
}
@article{iizuka2016let,
author = {Iizuka, Satoshi and Simo-Serra, Edgar and Ishikawa, Hiroshi},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Iizuka, Edgar Simo-Serra, Ishikawa - 2016 - Let there be Color! Joint End-to-end Learning of Global and Local Image Priors for Automatic.pdf:pdf},
journal = {ACM Transactions on Graphics (TOG)},
mendeley-groups = {deep colorization},
number = {4},
pages = {110},
publisher = {ACM},
title = {{Let there be color!: joint end-to-end learning of global and local image priors for automatic image colorization with simultaneous classification}},
volume = {35},
year = {2016}
}
@inproceedings{Iizuka2016,
abstract = {We present a novel technique to automatically colorize grayscale images that combines both global priors and local image features. Based on Convolutional Neural Networks, our deep network fea- tures a fusion layer that allows us to elegantly merge local information dependent on small image patches with global priors computed using the entire image. The entire framework, including the global and local priors as well as the colorization model, is trained in an end-to-end fashion. Furthermore, our architecture can process images of any resolution, unlike most existing approaches based on CNN. We leverage an existing large-scale scene classification data-base to train our model, exploiting the class labels of the dataset to more efficiently and discriminatively learn the global priors. We validate our approach with a user study and compare against the state of the art, where we show significant improvements. Further- more, we demonstrate our method extensively on many different types of images, including black-and-white photography from over a hundred years ago, and show realistic colorizations.},
author = {Iizuka, Satoshi and {Edgar Simo-Serra} and Ishikawa, Hiroshi},
booktitle = {SIGGRAPH '16},
doi = {10.1145/2897824.2925974},
file = {:home/primoz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Iizuka, Edgar Simo-Serra, Ishikawa - 2016 - Let there be Color! Joint End-to-end Learning of Global and Local Image Priors for Automatic.pdf:pdf},
isbn = {9781450342797},
issn = {07300301},
keywords = {colorization,computing methodologies,convolutional neural network concepts,image processing,neural net-},
mendeley-groups = {deep colorization},
number = {4},
pages = {110},
publisher = {ACM},
title = {{Let there be Color!: Joint End-to-end Learning of Global and Local Image Priors for Automatic Image Colorization with Simultaneous Classification}},
volume = {35},
year = {2016}
}

